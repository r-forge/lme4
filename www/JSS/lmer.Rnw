\documentclass{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{bm,amsmath,thumbpdf,amsfonts}
\author{Douglas Bates\\University of Wisconsin - Madison\And
  Martin M\"achler\\ETH Zurich\And
  Ben Bolker\\McMaster University
}
\Plainauthor{Douglas Bates, Martin M\"achler, Ben Bolker}
\title{Fitting linear mixed-effects models using \pkg{lme4}}
\Plaintitle{Fitting linear mixed-effects models using lme4}
\Shorttitle{Linear mixed models with lme4}
\Abstract{%
  Maximum likelihood or REML estimates of the parameters in linear
  mixed-effects models can be determined using the \code{lmer}
  function in the \pkg{lme4} package for \proglang{R}. As in most
  model-fitting functions, the model is described in an \code{lmer}
  call by a formula, in this case including both fixed-effects terms
  and random-effects terms. The formula and data together determine a
  numerical representation of the model from which the profiled
  deviance or the profiled REML criterion can be evaluated as a
  function of some of the model parameters.  The appropriate criterion
  is optimized, using one of the constrained optimization functions in
  \proglang{R}, to provide the parameter estimates.  We describe the
  structure of the model, the steps in evaluating the profiled
  deviance or REML criterion and the structure of the S4 class
  that represents such a model.  Sufficient detail is
  included to allow specialization of these structures by those who
  wish to write functions to fit specialized linear mixed models, such
  as models incorporating pedigrees or smoothing splines, that aren't
  easily expressible in the formula language used by \code{lmer}.
}
\Keywords{%
  sparse matrix methods,
  linear mixed models,
  penalized least squares,
  Cholesky decomposition}
\Address{
  Douglas Bates\\
  Department of Statistics, University of Wisconsin\\
  1300 University Ave.\\
  Madison, WI 53706, U.S.A.\\
  E-mail: \email{bates@stat.wisc.edu}
  \par\bigskip
  Martin M\"achler\\
  Seminar f\"ur Statistik, HG G~16\\
  ETH Zurich\\
  8092 Zurich, Switzerland\\
  E-mail: \email{maechler@stat.math.ethz.ch}\\
  % URL: \url{http://stat.ethz.ch/people/maechler}
  \par\bigskip
  Benjamin M. Bolker\\
  Departments of Mathematics \& Statistics and Biology \\
  McMaster University \\
  1280 Main Street W \\
  Hamilton, ON L8S 4K1, Canada \\
  E-mail: \email{bolker@mcmaster.ca}
}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\abs}{\operatorname{abs}}
\newcommand{\bLt}{\ensuremath{\bm\Lambda_\theta}}
\newcommand{\mc}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\trans}{\ensuremath{^\prime}}
\newcommand{\yobs}{\ensuremath{\bm y_{\mathrm{obs}}}}
\newcommand*{\eq}[1]{eqn.~\ref{#1}}% or just {(\ref{#1})}
%
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=10,height=6.5,strip.white=all}
\SweaveOpts{prefix=TRUE,prefix.string=figs/lmer,include=TRUE}
\SweaveOpts{keep.source=TRUE}
\setkeys{Gin}{width=\textwidth}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=69,show.signif.stars=FALSE,str=strOptions(strict.width="cut"))
library(lme4Eigen)
@
\begin{document}
\section{Introduction}
\label{sec:intro}

The \code{lme4} package for \proglang{R} provides functions to fit and
analyze linear mixed models (LMMs), generalized linear mixed models
(GLMMs) and nonlinear mixed models (NLMMs).  In each of these names,
the term ``mixed'' or, more fully, ``mixed-effects'', denotes a model
that incorporates both fixed-effects terms and random-effects terms in
a linear predictor expression from which the conditional mean of the
response can be evaluated.  In this paper we describe the formulation
and representation of linear and generalized linear mixed models.  The
techniques used for nonlinear mixed models will be described
separately.

Just as a linear model can be described in terms of the distribution
of $\mc{Y}$, the vector-valued random variable whose observed value is
$\yobs$, the observed response vector, a linear mixed model can be
described by the distribution of two vector-valued random variables:
$\mc{Y}$, the response and $\mc{B}$, the vector of random effects.  In
a linear model the distribution of $\mc Y$ is multivariate normal,
\begin{equation}
  \label{eq:linearmodel}
  \mc Y\sim\mc{N}(\bm X\bm\beta,\sigma^2\bm I_n),
\end{equation}
where $n$ is the dimension of the response vector, $\bm I_n$ is the
identity matrix of size $n$, $\bm\beta$ is a $p$-dimensional
coefficient vector and $\bm X$ is an $n\times p$ model matrix. The
parameters of the model are the coefficients, $\bm\beta$, and the
scale parameter, $\sigma$.

In a linear mixed model it is the \emph{conditional} distribution of
$\mc Y$ given $\mc B=\bm b$ that has such a form,
\begin{equation}
  \label{eq:LMMcondY}
  (\mc Y|\mc B=\bm b)\sim\mc{N}(\bm X\bm\beta+\bm Z\bm b,\sigma^2\bm I_n)
\end{equation}
where $\bm Z$ is the $n\times q$ model matrix for the $q$-dimensional
vector-valued random effects variable, $\mc B$, whose value we are
fixing at $\bm b$.  The unconditional distribution of $\mc B$ is also
multivariate normal with mean zero and a parameterized $q\times q$
variance-covariance matrix, $\bm\Sigma$,
\begin{equation}
  \label{eq:LMMuncondB}
  \mc B\sim\mc N(\bm0,\bm\Sigma) .
\end{equation}
As a variance-covariance matrix, $\bm\Sigma$ must be positive
semidefinite.  It is convenient to express the model in terms of a
\emph{relative covariance factor}, $\bm\Lambda_\theta$, which is a
$q\times q$ matrix, depending on the \emph{variance-component
  parameter}, $\bm\theta$, and generating the symmetric $q\times q$
variance-covariance matrix, $\bm\Sigma$, according to
\begin{equation}
  \label{eq:relcovfac}
  \bm\Sigma_\theta=\sigma^2\bm\Lambda_\theta\bm\Lambda_\theta\trans ,
\end{equation}
where $\sigma$ is the same scale factor as in the conditional
distribution (\ref{eq:LMMcondY}).

Although $q$, the number of columns in $\bm Z$ and the
size of $\bm\Sigma_{\bm\theta}$, can be very large indeed, the
dimension of $\bm\theta$ is small, frequently less than 10.

In calls to the \code{lm} function for fitting linear models the form
of the model matrix $\bm X$ is determined by the \code{formula} and
\code{data} arguments. The right-hand side of the formula consists of
one or more terms that each generate one or more columns in the model
matrix, $\bm X$.  For \code{lmer} the formula language is extended to
allow for random-effects terms that generate the model matrix $\bm Z$
and the mapping from $\bm\theta$ to $\bm\Lambda_{\bm\theta}$.

To understand why the formulation in equations \ref{eq:LMMcondY} and
\ref{eq:LMMuncondB} is particularly useful, we first show that the
profiled deviance (negative twice the log-likelihood) and the profiled
REML criterion can be expressed as a function of $\bm\theta$ only.
Furthermore these criteria can be evaluated quickly and accurately.

\section{Profiling the deviance and the REML criterion}
\label{sec:profdev}

As stated above, $\bm\theta$ determines the $q\times q$ matrix
$\bm\Lambda_\theta$ which, together with a value of $\sigma^2$,
determines
$\Var(\mc B)=\bm\Sigma_\theta=\sigma^2\bm\Lambda_\theta\bm\Lambda_\theta\trans$.
If we define a \emph{spherical}%
\footnote{$\mathcal{N}(\bm\mu,\sigma^2\bm I)$
  distributions are called ``spherical'' because contours of the
  probability density are spheres.}
\emph{random effects} variable, $\mc U$, with distribution
\begin{equation}
  \label{eq:sphericalRE}
  \mc U\sim\mathcal{N}(\bm 0,\sigma^2\bm I_q),
\end{equation}
and set
\begin{equation}
  \label{eq:BU}
  \mc B=\bm\Lambda_\theta\mc U,
\end{equation}
then $\mc B$ will have the desired $\mathcal{N}(\bm
0,\bm\Sigma_\theta)$ distribution.

Although it may seem more natural to define $\mc U$ in terms of $\mc
B$ we must write the relationship as in \eq{eq:BU} because
$\bm\Lambda_\theta$ may be singular.  In fact, it is important to
allow for $\bm\Lambda_\theta$ to be singular because situations
where the parameter estimates, $\widehat{\bm\theta}$, produce a
singular $\bm\Lambda_{\widehat{\theta}}$ do occur in practice.  And
even if the parameter estimates do not correspond to a singular
$\bm\Lambda_\theta$, it may be necessary to evaluate the estimation
criterion at such values during the course of the numerical
optimization of the criterion.

The model can now be defined in terms of
\begin{equation}
  \label{eq:LMMcondYU}
  (\mc Y|\mc U=\bm u)\sim\mc{N}(\bm Z\bm\Lambda_\theta\bm u+\bm X\bm\beta,\sigma^2\bm I_n)
\end{equation}
producing the joint density function
\begin{equation}
  \label{eq:jointDens}
  \begin{aligned}
    f_{\mc Y,\mc U}(\bm y,\bm u)&
    =f_{\mc Y|\mc U}(\bm y|\bm u)\,f_{\mc U}(\bm u)\\
    &=\frac{\exp(-\frac{1}{2\sigma^2}\|\bm y-\bm X\bm\beta-\bm Z\bLt\bm u\|^2)}
    {(2\pi\sigma^2)^{n/2}}\;
    \frac{\exp(-\frac{1}{2\sigma^2}\|\bm u\|^2)}{(2\pi\sigma^2)^{q/2}}\\
    &=\frac{\exp(-
      \left[\|\bm y-\bm X\bm\beta-\bm Z\bLt\bm u\|^2+\|\bm u\|^2\right]/(2\sigma^2))}
    {(2\pi\sigma^2)^{(n+q)/2}} .
  \end{aligned}
\end{equation}

% To obtain an expression for the likelihood it is convenient to
% distinguish between a general argument, $\bm y$, and the particular
% value of the observed response, which we will write as $\yobs$
% for the remainder of this section.
The \emph{likelihood} of the parameters, $\bm\theta$, $\bm\beta$ and
$\sigma^2$, given the observed data is the value of the
marginal density of $\mc Y$, evaluated at $\yobs$.  That is
\begin{equation}
  \label{eq:likelihoodLMM}
  L(\bm\theta,\bm\beta,\sigma^2|\yobs)=\int_{\mathbb{R}^q}f_{\mc Y,\mc
    U}(\yobs,\bm u)\,d\bm u .
\end{equation}
The integrand of \eq{eq:likelihoodLMM} is the \emph{unscaled
  conditional density} of $\mc U$ given $\mc Y=\yobs$.  The
conditional density of $\mc U$ given $\mc Y=\yobs$ is
\begin{equation}
  \label{eq:condDens}
  f_{\mc U|\mc Y}(\bm u|\yobs)=\frac{f_{\mc Y,\mc U}(\yobs,\bm u)}
  {\int f_{\mc Y,\mc U}(\yobs,\bm u)\,d\bm u}
\end{equation}
which is, up to a scale factor, the joint density, $f_{\mc Y,\mc
  U}(\yobs,\bm u)$.  The unscaled conditional density will be, up to a
scale factor, a $q$-dimensional multivariate Gaussian with an integral
that is easily evaluated if we know the mean and variance-covariance
of the conditional density.

The conditional mean, $\bm\mu_{\mc U|\mc Y=\yobs}$, is also the mode
of the conditional distribution.  Because a constant factor in a
function does not affect the location of the optimum, we can determine
the conditional mode, and hence the conditional mean, by maximizing
the unscaled conditional density.  This is in the form of a
\emph{penalized linear least squares} problem,
\begin{equation}
  \label{eq:PLSprob}
  \bm\mu_{\mc U|\mc Y=\yobs}=\arg\min_{\bm u}
  \left(\left\|\yobs-\bm X\bm\beta-\bm Z\bLt\bm u\right\|^2 +
    \left\|\bm u\right\|^2\right) .
\end{equation}

\subsection{Solving the penalized least squares problem}
\label{sec:solvingPLS}

In the so-called ``pseudo-data'' approach to penalized least squares
problems we write the objective as a residual sum of squares for an
extended response vector and model matrix
\begin{equation}
  \label{eq:pseudoData}
  \left\|\yobs-\bm X\bm\beta-\bm Z\bLt\bm u\right\|^2 +
  \left\|\bm u\right\|^2 =
  \left\| \begin{bmatrix}\yobs-\bm X\bm\beta\\\bm 0\end{bmatrix}-
    \begin{bmatrix}\bm Z\bLt\\\bm I_q\end{bmatrix}
    \bm u\right\|^2.
\end{equation}
The contribution to the residual sum of squares from the ``pseudo''
observations appended to $\yobs-\bm X\bm\beta$, is exactly the penalty
term, $\left\|\bm u\right\|^2$.

From \eq{eq:pseudoData} we can see that the conditional mean satisfies
\begin{equation}
  \label{eq:PLSsol}
  \left(\bLt\trans\bm Z\trans\bm Z\bLt+\bm I_q\right)
  \bm\mu_{\mc U|\mc Y=\yobs}=\bLt\trans\bm Z\trans(\yobs-\bm X\bm\beta),
\end{equation}
which would be interesting, but not terribly useful, were it not for
the fact that we can determine the solution to \eq{eq:PLSsol}
quickly and accurately, even when $q$, the size of the system to
solve, is very large indeed.  (We have done so in cases where $q$ is
in the millions.)

The key to solving \eq{eq:PLSsol} is the \emph{sparse Cholesky
  factor}, $\bm L_\theta$, which is a sparse, lower-triangular matrix
such that
\begin{equation}
  \label{eq:sparseChol}
  \bm L_\theta\bm L\trans_\theta=\bm P
  \left(\bLt\trans\bm Z\trans\bm Z\bLt+\bm I_q\right)
  \bm P\trans,
\end{equation}
where $\bm P$ is a permutation matrix representing a fill-reducing
permutation~\citep[Ch.~7]{davis06:csparse_book}.

As for most sparse matrix methods, the sparse Cholesky factorization
can be split into two phases: a symbolic phase in which the positions
of the non-zero elements in the result are determined and a numeric
phase in which the actual numeric values in these positions are
determined.  Determining the fill-reducing permutation represented by
$\bm P$ is part of the symbolic phase, which often takes much longer
than the numeric phase.  During the course of determining the maximum
likelihood or REML estimates of the parameters in a linear
mixed-effects model we may need to evaluate $\bm L_\theta$ for many
different values of $\bm\theta$, but each evaluation after the first
requires only the numeric phase.  Changing $\bm\theta$ can change the
values of the non-zero elements in $\bm L$ but does not change their
positions.  Hence, the symbolic phase must be done only once.

The \code{Cholesky} function in the \pkg{Matrix} package for
\proglang{R} performs both the symbolic and numeric phases of the
factorization to produce $\bm L_\theta$ from $\bLt\trans\bm Z\trans\bm
Z\bLt$.  The resulting object has S4 class \code{"CHMsuper"} or
\code{"CHMsimp"} depending on whether it is in the
supernodal~\citep[\S~4.8]{davis06:csparse_book} or simplicial form.
Both these classes inherit from the virtual class \code{"CHMfactor"}.
Optional arguments to the \code{Cholesky} function control
determination of a fill-reducing permutation and addition of multiple
of the identity to the symmetric matrix before factorization.  Once
the factor has been determined for the initial value, $\bm\theta_0$,
it can be updated for new values of $\bm\theta$ in a single call to
the \code{update} method.

Although the  \code{solve} method for the \code{"CHMfactor"} class has
an option to evaluate $\bm\mu_{\mc U|\mc Y=\yobs}$ directly as the solution to
\begin{equation}
  \label{eq:Cholsol}
  \bm P\trans\bm L_\theta\bm L\trans_\theta\bm P
  \bm\mu_{\mc U|\mc Y=\yobs}=
  \bLt\trans\bm Z\trans(\bm y-\bm X\bm\beta) .
\end{equation}
we will express the solution in two stages:
\begin{enumerate}
\item Solve $\bm L\bm c_{\bm u}=\bm P\bLt\trans\bm Z\trans(\bm y-\bm
  X\bm\beta)$ for $\bm c_{\bm u}$.
\item Solve $\bm L\trans\bm P\bm\mu_{\mc U|\mc Y=\yobs}=\bm c_{\bm u}$ for $\bm
  P\bm\mu_{\mc U|\mc Y=\yobs}$ and then for
  $\bm\mu_{\mc U|\mc Y=\yobs}=\bm P\trans\left(\bm P\bm\mu_{\mc U|\mc Y=\yobs}\right)$.
\end{enumerate}

\subsection{Evaluating the likelihood}
\label{sec:evallike}

After solving for $\bm\mu_{\mc U|\mc Y=\yobs}$ the exponent in $f_{\mc Y,\mc
  U}(\yobs, \bm u)$ can be written
\begin{equation}
  \label{eq:PLS}
  \|\yobs-\bm X\bm\beta-\bm Z\bLt\bm u\|^2+\|\bm u\|^2=
  r^2(\bm\theta,\bm\beta)+
  \|\bm L\trans\bm P(\bm u-\bm\mu_{\mc U|\mc Y=\yobs})\|^2.
\end{equation}
where $r^2(\bm\theta,\bm\beta)=\|\yobs-\bm X\bm\beta-
\bm Z\bm\Lambda_\theta\bm\mu_{\mc U|\mc Y=\yobs}\|^2+
\|\bm\mu_{\mc U|\mc Y=\yobs}\|^2$, is the minimum
penalized residual sum of squares for these values of $\bm\theta$ and
$\bm\beta$.

With expression (\ref{eq:PLS}) and the change of variable $\bm v=\bm
L\trans\bm P(\bm u-\bm\mu_{\mc U|\mc Y=\yobs})$, for which $d\bm
v=\abs(|\bm L||\bm P|)\,d\bm u$, we have
\begin{equation}
  \label{eq:intExpr}
  \int\frac{\exp\left(\frac{-\|\bm L\trans\bm P(\bm u-\bm\mu_{\mc U|\mc Y})\|^2}
      {2\sigma^2}\right)}
  {(2\pi\sigma^2)^{q/2}}\,d\bm u \\
  = \int\frac{\exp\left(\frac{-\|\bm
        v\|^2}{2\sigma^2}\right)}{(2\pi\sigma^2)^{q/2}}\,\frac{d\bm
    v}{\abs(|\bm L||\bm P|)} = \frac{1}{\abs(|\bm L||\bm
    P|)}=\frac{1}{|\bm L|}
\end{equation}
because $\abs|\bm P|=1$ (one property of a permutation matrix is $|\bm
P|=\pm1$) and $|\bm L|$, which, because $\bm L$ is triangular, is the
product of its diagonal elements, all of which are positive, is
positive.

Using this expression we can write the deviance (negative twice the
log-likelihood) as
\begin{equation}
  \label{eq:deviance}
  -2\ell(\bm\theta,\bm\beta,\sigma^2|\yobs)=-2\log L(\bm\theta,\bm\beta,\sigma^2|\yobs)=
  n\log(2\pi\sigma^2)+\frac{r^2(\bm\theta,\bm\beta)}{\sigma^2}+
  \log(|\bm L_\theta|^2)
\end{equation}
Because the dependence of \eq{eq:deviance} on $\sigma^2$ is
straightforward, we can form the conditional estimate
\begin{equation}
  \label{eq:conddev}
  \widehat{\sigma^2}(\bm\theta,\bm\beta)=\frac{r^2(\bm\theta,\bm\beta)}{n} ,
\end{equation}
producing the \emph{profiled deviance}
\begin{equation}
  \label{eq:profdev1}
  -2\tilde{\ell}(\bm\theta,\bm\beta|\yobs)=\log(|\bm L_\theta|^2)+
  n\left[1+\log\left(\frac{2\pi r^2(\bm\theta,\bm\beta)}{n}\right)\right]
\end{equation}

However, observing that \eq{eq:profdev1} depends on $\bm\beta$
only through $r^2(\bm\theta,\bm\beta)$ provides a much greater
simplification because it allows us to ``profile out'' the
fixed-effects parameter, $\bm\beta$, from the evaluation of the
deviance.  The conditional estimate, $\widehat{\bm\beta}_\theta$, is
the value of $\bm\beta$ at the solution of the joint penalized least
squares problem
\begin{equation}
  \label{eq:jointPLS}
  r^2_\theta=\min_{\bm u,\bm\beta}
  \left(\left\|\bm y-\bm X\bm\beta-\bm Z\bLt\bm u\right\|^2 +
    \left\|\bm u\right\|^2\right) ,
\end{equation}
producing the profiled deviance,
\begin{equation}
  \label{eq:profdev2}
  -2\tilde{\ell}(\bm\theta)=\log(|\bm L_\theta|^2)+
  n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right],
\end{equation}
which is a function of $\bm\theta$ only.  Eqn.~\ref{eq:profdev2} is a
remarkably compact expression for the deviance.

\subsection{Solving the joint penalized least squares problem}
\label{sec:jointPLS}
The solutions, $\bm\mu_{\mc U|\mc Y=\yobs}$ and
$\widehat{\bm\beta}_\theta$, of the joint penalized least squares
problem (\ref{eq:jointPLS}) satisfy
\begin{equation}
  \label{eq:jointPLSeqn}
  \begin{bmatrix}
    \bLt\trans\bm Z\trans\bm Z\bLt+\bm I_q & \bm
    \bLt\trans\bm Z\trans\bm X\\
    \bm X\trans\bm Z\bLt & \bm X\trans\bm X
  \end{bmatrix}
  \begin{bmatrix}
    \bm\mu_{\mc U|\mc Y=\yobs}\\\widehat{\bm\beta}_\theta
  \end{bmatrix}=
  \begin{bmatrix}\bLt\trans\bm Z\trans\yobs\\\bm X\trans\yobs .
  \end{bmatrix}
\end{equation}
As before we will use the sparse Cholesky decomposition producing,
$\bm L_\theta$, the sparse Cholesky factor, and $\bm P$, the
permutation matrix, satisfying $\bm L_\theta\bm L_\theta\trans=\bm
P(\bLt\trans\bm Z\trans\bm Z\bLt+\bm I)\bm P\trans$ and $\bm c_{\bm u}$, the
solution to $\bm L_\theta\bm c_{\bm u}=\bm P\bLt\trans\bm Z\trans\yobs$.

We extend the decomposition with the $q\times p$ matrix $\bm
R_{ZX}$, the upper triangular $p\times p$ matrix $\bm R_X$, and
the $p$-vector $\bm c_{\bm\beta}$ satisfying
\begin{align*}
  \bm L\bm R_{ZX}&=\bm P\bLt\trans\bm Z\trans\bm X\\
  \bm R_X\trans\bm R_X&=\bm X\trans\bm X-\bm R_{ZX}\trans\bm R_{ZX}\\
  \bm R_X\trans\bm c_{\bm\beta}&=\bm X\trans\yobs-\bm R_{ZX}\trans\bm c_{\bm u}
\end{align*}
so that
\begin{equation}
  \label{eq:fulldecomp}
  \begin{bmatrix}
    \bm P\trans\bm L& \bm 0\\
    \bm R_{ZX}\trans & \bm R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L\trans\bm P & \bm R_{ZX}\\
    \bm 0            & \bm R_X
  \end{bmatrix}=
  \begin{bmatrix}
    \bLt\trans\bm Z\trans\bm Z\bLt+\bm I & \bLt\trans\bm Z\trans\bm X\\
    \bm X\trans\bm Z\bLt       & \bm X\trans\bm X
  \end{bmatrix} ,
\end{equation}
and the solutions, $\bm\mu_{\mc U|\mc Y=\yobs}$ and
$\widehat{\bm\beta}_\theta$, satisfy
\begin{align}
  \bm R_X\widehat{\bm\beta}_\theta&=\bm c_{\bm\beta}\\
  \bm L\trans\bm P\bm\mu_{\mc U|\mc Y=\yobs}&=\bm c_{\bm u}-\bm
  R_{ZX}\widehat{\bm\beta}_\theta .
\end{align}

\subsection{The profiled REML criterion}
\label{sec:profiledREML}

\citet{laird_ware_1982} show that the criterion to be optimized by the
REML estimates can be expressed as
\begin{equation}
  \label{eq:REMLcrit}
  L_R(\bm\theta,\sigma^2|\yobs)=\int
  L(\bm\theta,\bm\beta,\sigma^2|\yobs)\,d\bm\beta .
\end{equation}

Because the joint solutions, $\bm\mu_{\mc U|\mc Y=\yobs}$ and
$\widehat{\bm\beta}_\theta$, to the penalized least squares problem
allow us to express
\begin{multline}
  \label{eq:PLS2}
  \|\yobs-\bm X\bm\beta-\bm Z\bLt\bm u\|^2+\|\bm u\|^2=\\
  r^2_\theta+
  \left\|\bm L\trans\bm P\left[\bm u-\bm\mu_{\mc U|\mc Y=\yobs}-
  \bm R_{ZX}(\bm\beta - \widehat{\bm\beta}_\theta)\right]\right\|^2+
  \left\|\bm R_X(\bm\beta - \widehat{\bm\beta}_\theta)\right\|^2
\end{multline}
we can use a change of variable, similar to that in
\eq{eq:intExpr}, to evaluate the profiled REML criterion.  On the
deviance scale the criterion can be evaluated as
\begin{equation}
  \label{eq:profiledREML}
  -2\tilde{\ell}_R(\bm\theta)=\log(|\bm L|^2)+\log(|\bm R_X|^2)+
  (n-p)\left[1+\log\left(\frac{2\pi r^2_\theta}{n-p}\right)\right] .
\end{equation}

% \section{Generalized Linear Mixed Models}
% \label{sec:GLMMs}

The structures in \pkg{lme4} for representing mixed-models are
somewhat more general than is required for linear mixed models.  They
are designed to also be used for generalized linear mixed models
(GLMMs) and nonlinear mixed models (NLMMs), which we describe in the
next sections.

\section{Generalized Linear Mixed Models}
\label{sec:GLMMdef}

The generalized linear mixed models (GLMMs) that can be fit by the
\pkg{lme4} package preserve the multivariate Gaussian unconditional
distribution of the random effects, $\mc B$
(eqn.~\ref{eq:LMMuncondB}).  Because most families used for the conditional
distribution, $\mc Y|\mc B=\bm b$, do not incorporate a separate scale
factor, $\sigma$, we remove it from the definition of $\bm\Sigma$ and
from the distribution of the spherical random effects, $\mc U$.  That
is, 
\begin{equation}
  \label{eq:UdistGLMM}
  \mc U\sim\mc N(\bm0,\bm I_q)
\end{equation}
and
\begin{equation}
  \label{eq:GLMMSigma}
  \bm\Sigma_\theta=\bm\Lambda_\theta\bm\Lambda_\theta\trans .
\end{equation}

The conditional distributions, $\mc Y|\mc B=\bm b$ and $\mc Y|\mc
U=\bm u$, preserve the properties that the components of $\mc Y$ are
conditionally independent and that the mean, $\bm\mu_{\mc Y|\mc U=\bm
  u}$, depends on $\bm u$ only through the linear predictor,
\begin{equation}
  \label{eq:GLMMlinpred}
  \bm\eta=\bm Z\bm\Lambda_\theta\bm u+\bm X\bm\beta .
\end{equation}
The mapping from $\bm\mu_{\mc Y|\mc U=\bm u}$ to $\bm\eta$, which is called
the \emph{link function} and written
\begin{equation}
  \label{eq:linkfun}
  \bm Z\bm\Lambda_\theta\bm u+\bm X\bm\beta=\bm\eta=\bm g\left(
    \bm\mu_{\mc Y|\mc U=\bm u }\right) ,
\end{equation}
is a \emph{diagonal mapping} in the sense that there is a scalar
function, $g$, such that the $i$th component of $\bm\eta$ is $g$
applied to the $i$th component of $\bm\mu_{\mc Y|\mc U=\bm u }$.  (The
name ``diagonal'' reflects the fact that the Jacobian matrix,
$\frac{d\eta}{d\mu\trans}$, of such a mapping will be diagonal.)

The scalar link function must be invertible over its range.  The
vector-valued \emph{inverse link} function, $\bm g^{-1}$, will be the
scalar inverse link, $g^{-1}$, applied component-wise to $\bm\eta$.

Common forms of the conditional distribution are Bernoulli, for binary
responses, binomial for binary responses that are recorded as the
number of trials and the number of successes, and Poisson, for count
data.  The combination of a distributional form and a link function is
called a \emph{family}.  For distributional forms in the exponential
family there is a \emph{canonical link}.  For Bernoulli or binomial
forms the canonical link is the \emph{logit} link function
\begin{equation}
  \label{eq:logitLink}
  \eta_i=\log\left(\frac{\mu_i}{1-\mu_i}\right);
\end{equation}
for the Poisson distribution the canonical link is the natural
logarithm.

The form of the distribution determines the conditional variance,
$\Var(\mc Y|\mc U=\bm u)$, as a function of the conditional mean and,
possibly, a separate scale factor. (In most cases the conditional
variance is completely determined by the conditional mean.)

The likelihood of the parameters, given the observed data, is now
\begin{equation}
  \label{eq:GLMMlike}
  L(\bm\beta,\bm\theta|\yobs)=\int_{\mathbb{R}^q}f_{\mc Y,\mc U}(\yobs,\bm u)\,d\bm u
\end{equation}
where, as in the case of linear mixed models, $f_{\mc Y,\mc
  U}(\yobs,\bm u)$ is the unscaled conditional density of $\mc U$
given $\mc Y=\yobs$.  The notation here is a bit blurred because,
although the joint distribution of $\mc Y$ and $\mc U$ is always
continuous with respect to $\mc U$, it can be (and often is) discrete
with respect to $\mc Y$. However, when we condition on the observed
value $\mc Y=\yobs$, the resulting function is continuous with respect
to $\bm u$ so the unscaled conditional density is indeed well-defined
as a density, up to a scale factor.

To evaluate the integrand in (\ref{eq:GLMMlike}) we use the value of
the \code{dev.resids} function in the GLM family.  This vector,
$\bm d(\yobs,\bm u)$, with elements, $d_i(\yobs,\bm u), i=1,\dots,n$,
provides the deviance of a generalized linear model as
\begin{displaymath}
  \sum_{i=1}^n d_i(\yobs,\bm u) .
\end{displaymath}
(We should note that there 
some confusion in \proglang{R} (and in its predecessor,
\proglang{S}) about what exactly the deviance residuals
for a family are.  As indicated above, we will use this name for the
value of the \code{dev.resids} function in the family.  The signed
square root of this vector, using the signs of $\yobs-\mu$, is returned
from \code{residuals} applied to a fitted model
of class \code{"glm"} when \code{type="deviance"}, the
default, is specified.  Both are called ``deviance residuals''
in the documentation but, although they are related, they are not the same.)

The likelihood can now be expressed as
\begin{equation}
  \label{eq:GLMMlike1}
  L(\bm\beta,\bm\theta|\yobs)=
  \int_{\mathbb{R}^q}\exp\left(-\frac{\sum_{i=1}^nd_i(\yobs,\bm u)+\|\bm u\|^2}{2}\right)\,(2\pi)^{-q/2}\,d\bm u
\end{equation}

As for linear mixed models, we simplify evaluation of the integral
(\ref{eq:GLMMlike}) by determining the value, $\tilde{\bm
  u}_{\beta,\theta}$, that maximizes the integrand.
When the conditional density, $\mc U|\mc Y=\yobs$, is multivariate
Gaussian, this conditional mode will also be the conditional mean.
However, for most families used in GLMMs, the mode and the mean need
not coincide so use the more general term and call $\tilde{\bm
  u}_{\beta,\theta}$ the \emph{conditional mode}.  We first describe 
the numerical methods
for determining the conditional mode using the Penalized Iteratively
Reweighted Least Squares (PIRLS) algorithm then return to the question
of evaluating the integral (\ref{eq:GLMMlike}).

\subsection{Determining the conditional mode}
\label{sec:conditionalMode}

The iteratively reweighted least squares (IRLS) algorithm is an
incredibly efficient method of determining the maximum likelihood
estimates of the coefficients in a generalized linear model.  We
extend it to a \emph{penalized iteratively reweighted least squares}
(PIRLS) algorithm for determining the conditional mode, $\tilde{\bm
  u}_{\beta,\theta}$.   This algorithm has the form
\begin{enumerate}
\item Given parameter values, $\bm\beta$ and $\bm\theta$, and starting
  estimates, $\bm u_0$, evaluate the linear predictor, $\bm\eta$, the
  corresponding conditional mean, $\bm\mu_{\mc Y|\mc U=\bm u}$, and
  the conditional variance.  Establish the weights as the inverse of
  the variance.  We write these weights in the form of a diagonal
  weight matrix, $\bm W$, although they are stored and manipulated as
  a vector.
\item Solve the penalized, weighted, nonlinear least squares problem
  \begin{equation}
    \label{eq:weightedNLS}
    \arg\min_{\bm u}\left(\left\|\bm W^{1/2}\left(\yobs-\bm\mu_{\mc
            Y|\mc U=\bm u}\right)\right\|^2+\|\bm u\|^2\right)
  \end{equation}
\item Update the weights, $\bm W$, and check for convergence.  If not
  converged, go to step 2.
\end{enumerate}

We use a Gauss-Newton algorithm with an orthogonality convergence
criterion~\citep[\S 2.2.3]{bateswatts88:_nonlin} to solve the
penalized, weighted, nonlinear least squares problem in step 2.  At
the $i$th iteration we determine an increment, $\bm\delta_i$, as the
solution to the penalized, weighted, linear least squares problem
\begin{equation}
  \label{eq:incr}
  \bm\delta_i=\arg\min_{\bm\delta}\left\|
    \begin{bmatrix}
      \bm W^{1/2}\left(\yobs-\bm\mu_i\right)\\
      \bm u_i
    \end{bmatrix}-
    \begin{bmatrix}
      \bm W^{1/2}\bm M_i\bm Z\bm\Lambda_\theta\\
      \bm I_q
    \end{bmatrix}\bm u\right\|^2
\end{equation}
where $\bm u_i$ is current value of $\bm u$, $\bm\mu_i$ is the
corresponding conditional mean of $\mc Y|\mc U=\bm u_i$ and $\bm M_i$ is
the Jacobian matrix of the vector-valued inverse link, evaluated at
$\bm\mu_i$.  That is
\begin{equation}
  \label{eq:Jacobian}
  \bm M_i=\left.\frac{d\bm\mu}{d\bm\eta\trans}\right|_{\bm\eta_i},
\end{equation}
which will be a diagonal matrix so, as for the weights, we store and
manipulate the Jacobian as a vector.

The minimizer, $\bm\delta_i$, of (\ref{eq:incr}) satisfies
\begin{equation}
  \label{eq:incrEq}
  \bm P\left(\bLt\trans\bm Z\trans\bm M_i\bm W\bm M_i\bm Z\bLt+\bm I_q\right)\bm P\trans
      \bm\delta_i=\bLt\trans\bm Z\trans\bm M_i\bm W(\yobs-\bm\mu_i) - \bm u_i
\end{equation}
which we solve using the sparse Cholesky factor.  At convergence, the
factor, $\bm L_{\beta,\theta}$, satisfies
\begin{equation}
  \label{eq:CholFactorGLMM}
  \bm L_{\beta,\theta}\bm L_{\beta,\theta}\trans =
  \bm P\left(\bLt\trans\bm Z\trans\bm M\bm W\bm M\bm Z\bLt+\bm I_q\right)\bm P\trans
\end{equation}

\subsection{Evaluating the likelihood for GLMMs using the Laplace approximation}
\label{sec:Laplace}

A second-order Taylor series approximation to $-2\log[f_{\mc
  Y,\mc U}(\yobs,\bm u)]$ based at $\tilde{\bm u}$ provides an approximation of
unscaled conditional density as a multiple of the density for the
multivariate Gaussian $\mathcal{N}(\tilde{\bm u},\bm L\bm L\trans)$.
The change of variable
\begin{equation}
  \label{eq:LaplaceChg}
  \bm u = \tilde{\bm u} + \bm L\bm z
\end{equation}
provides
\begin{equation}
  \label{eq:GLMMLaplace}
  \begin{aligned}
    L(\bm\beta,\bm\theta|\yobs)&=\int_{\mathbb{R}^q}f_{\mc Y,\mc U}(\yobs,\bm u)\,d\bm u\\
    &\approx \tilde{f}\,|\bm L|\, \int_{\mathbb{R}^q}e^{-\|\bm z\|^2/2}\,(2\pi)^{-q/2}\,d\bm z\\
    =\tilde{f}\,\abs(|\bm L|)
  \end{aligned}
\end{equation}
or, on the deviance scale,
\begin{equation}
  \label{eq:LaplaceDev}
  -2\ell(\bm\beta,\bm\theta|\yobs)\approx\sum_{i=1}^nd_i(\yobs,\tilde{\bm u}) +
    \|\tilde{\bm u}\|^2 + \log(|\bm L|^2)+\frac{q}{2}\log(2\pi)
\end{equation}

\subsubsection{Decomposing the deviance for simple models}
\label{sec:simplescalar}

A special, but not uncommon, case is that of scalar random effects
associated with levels of a single grouping factor, $\bm h$.  In this
case the dimension, $q$, of the random effects is the number of levels
of $\bm h$ --- i.e.{} there is exactly one random effect associated
with each level of $\bm h$.  We will write the vector of
variance-covariance parameters, which is one-dimensional, as a scalar,
$\theta$.  The matrix $\bm\Lambda_{\bm\theta}$ is a multiple of the
identity, $\theta\bm I_q$, and $\bm Z$ is the $n\times q$ matrix of
indicators of the levels of $\bm f$.  The permutation matrix, $\bm
P$, can be set to the identity and $\bm L$ is diagonal, but not
necessarily a multiple of the identity.  

Because each element of $\bm\mu$ depends on only one element of $\bm
u$ and the elements of $\mc Y$ are conditionally independent, given
$\mc U=\bm u$, the conditional densities of the $u_j,j=1,\dots,q$
given $\mc Y=\yobs$ are independent.  We partition the indices
$1,\dots,n$ as $\mathbb{I}_j,j=1,\dots,q$ according to the levels of
$\bm h$.  That is, the index $i$ is in $\mathbb{I}_j$ if $h_i=j$.
This partitioning also applies to the deviance residuals in that
the $i$th deviance residual depends only on $u_j$ when $i\in\mathbb{I}_j$.

Writing the univariate conditional densities as
\begin{equation}
  \label{eq:univariateCondDens}
  f_j(\yobs,u_j)=\exp\left(-\frac{\sum_{i\in\mathbb{I}_j}d_i(\yobs, u_j)+u_j^2}{2}\right)(2\pi)^{-1/2}
\end{equation}
we have
\begin{equation}
  \label{eq:vectorCondDens}
  f_{\mc Y,\mc U}(\yobs,\bm u)=\prod_{j=1}^q f_j(\yobs,u_j)
\end{equation}
and
\begin{equation}
  \label{eq:ssLike}
  \begin{aligned}
    L(\bm\beta,\bm\theta|\yobs)=\prod_{j=1}^q\int_{\mathbb{R}}f_j(\yobs,u)\,du
  \end{aligned}
\end{equation}

We consider this special case both because it occurs frequently and
because, for some software, it is the only type of GLMM that can be
fit.  Also, in this particular case we can graphically assess the
quality of the Laplace approximation by comparing the actual integrand
to its approximation.

Consider the \code{cbpp} data on contagious bovine pleuropneumonia
incidence according to season and herd, available in the \pkg{lme4} package.
<<strcbpp>>=
str(cbpp)
@ 
and the model
<<m1>>=
print(m1 <- glmer(cbind(incidence, size-incidence) ~ period + (1|herd),
                  cbpp, binomial), corr=FALSE)
@
This model has been fit by minimizing the Laplace approximation to the
deviance.  We can assess the quality of this approximation by
evaluating the unscaled conditional density at $u_j(z)=\tilde{u_j} +
z/{\bm L_{j,j}}$ and comparing the ratio,
$f_j(\yobs,u)/(\tilde{f_j}\sqrt{2\pi})$, to the standard normal
density, $\phi(z)=e^{-z^2/2}/\sqrt{2\pi}$, as shown in Figure~\ref{fig:densities}.
\begin{figure}[tbp]
  \centering
<<densities,fig=TRUE,echo=FALSE,height=5>>=
zeta <- function(m, zmin=-3, zmax=3, npts=301L) {
    stopifnot (is(m, "glmerMod"),
              length(m@flist) == 1L,    # single grouping factor
              length(m@cnms[[1]]) == 1L) # single column for that grouping factor
    pp <- m@pp
    rr <- m@resp
    u0 <- pp$u0
    sd <- 1/pp$L()@x
    ff <- as.integer(m@flist[[1]])
    fc <- pp$X %*% pp$beta0             # fixed-effects contribution to linear predictor
    ZL <- t(pp$Lambdat %*% pp$Zt)
    dc <- function(z) { # evaluate the unscaled conditional density on the deviance scale
        uu <- u0 + z * sd
        rr$updateMu(fc + ZL %*% uu)
        unname(as.vector(tapply(rr$devResid(), ff, sum))) + uu * uu
    }
    zvals <- seq(zmin, zmax, length.out = npts)
    d0 <- dc(0) # because this is the last evaluation, the model is restored to its incoming state
    list(zvals=zvals,
         sqrtmat=t(sqrt(vapply(zvals, dc, d0, USE.NAMES=FALSE) - d0)) * # signed square root
         array(ifelse(zvals < 0, -1, 1), c(npts, length(u0))))
}
zm <- zeta(m1, -3.750440, 3.750440)
dmat <- exp(-0.5*zm$sqrtmat^2)/sqrt(2*pi)
xyplot(as.vector(dmat) ~ rep.int(zm$zvals, ncol(dmat))|gl(ncol(dmat), nrow(dmat)),
       type=c("g","l"), aspect=0.6, layout=c(5,3),
       xlab="z", ylab="density",
       panel=function(...){
           panel.lines(zm$zvals, dnorm(zm$zvals), lty=2)
           panel.xyplot(...)}
       )
@ 
  \caption{Comparison of univariate integrands (solid line) and standard normal density function (dashed line)}
  \label{fig:densities}
\end{figure}

As we can see from this figure, the univariate integrands are very
close to the standard normal density, indicating that the Laplace
approximation to the deviance is a good approximation in this case.

\section{Adaptive Gauss-Hermite quadrature for GLMMs}
\label{sec:aGQ}
When the integral (\ref{eq:GLMMlike}) can be expressed as a product of
low-dimensional integrals, we can use Gauss-Hermite quadrature to
provide a closer approximation to the integral.  Univariate
Gauss-Hermite quadrature evaluates the integral of a function that is
multiplied by a ``kernel'' where the kernel is a multiple of
$e^{-z^2}$ or $e^{-z^2/2}$.  For statisticians the natural candidate
is the standard normal density, $\phi(z)=e^{-z^2/2}/\sqrt(2\pi)$.
A $k$th-order Gauss-Hermite formula provides knots, $z_i,i=1,...,k$,
and weights, $w_i,i=1,\dots,k$, such that
\begin{displaymath}
  \int_{\mathbb{R}}t(z)\phi(z)\,dz\approx\sum_{i=1}^kw_it(z_i)
\end{displaymath}
The function \code{GHrule} in \pkg{lme4} (based on code in the
\pkg{SparseGrid} package) provides knots and weights relative to the
standard normal kernel for orders $k$ from 1 to 25.  For example,
<<GHrule5>>=
GHrule(5)
@ 

The choice of the value of $k$ depends on the behavior of the function
$t(z)$.  If $t(z)$ is a polynomial of degree $k-1$ then the
Gauss-Hermite formula for orders $k$ or greater provides an exact
answer.  The fact that we want $t(z)$ to behave like a low-order
polynomial is often neglected in the formulation of a Gauss-Hermite
approximation to a quadrature.  The quadrature knots on the $u$ scale
are chosen as
\begin{equation}
  \label{eq:quadraturepts}
  u_{i,j}(z)=\tilde{u_j} + z_i/{\bm L_{j,j}},\quad i=1,\dots,k;\;j=1,\dots,q
\end{equation}
exactly so that the function $t(z)$ should behave like a low-order
polynomial over the region of interest, which is to say the region
where quadrature knots with large weights are located.  The term
``adaptive Gauss-Hermite quadrature'' reflects the fact that the
approximating Gaussian density is scaled and shifted to provide a
second order approximation to the logarithm of the unscaled
conditional density.

Figure~\ref{fig:tfunc}
\begin{figure}[tbp]
  \centering
<<tfunc,fig=TRUE,echo=FALSE>>=
xyplot(as.vector(dmat/dnorm(zm$zvals)) ~ rep.int(zm$zvals, ncol(dmat))|gl(ncol(dmat), nrow(dmat)),
       type=c("g","l"), aspect=0.6, layout=c(5,3),
       xlab="z", ylab="t(z)")
@ 
  \caption{The function $t(z)$, which is the ratio of the normalized
    unscaled conditional density to the standard normal density, for
    each of the univariate integrals in the evaluation of the deviance
    for model \code{m1}.  These functions should behave like low-order
    polynomials.}
  \label{fig:tfunc}
\end{figure}
shows $t(z)$ for each of the unidimensional integrals in the
likelihood for the model \code{m1} at the parameter estimates.

\bibliography{lmer}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
