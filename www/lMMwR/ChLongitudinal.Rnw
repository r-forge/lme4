\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=8,strip.white=TRUE}
\SweaveOpts{keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Long,include=TRUE}
\setkeys{Gin}{width=\textwidth}

<<preliminaries,echo=FALSE,print=FALSE,results=hide>>=
options(width=80, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
library(splines)
library(lattice)
library(Matrix)
library(lme4a)
fm9 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy, REML = 0)
if (file.exists("pr9.rda")) {
    load("pr9.rda")
} else {
    pr9 <- profile(fm9)
    save(pr9, file = "pr9.rda")
}
data(Orthodont, package = "MEMSS")
@

\chapter{Models for Longitudinal Data}
\label{chap:longitudinal}

Longitudinal data consist of repeated measurements on the same subject
(or some other ``experimental unit'') taken over time.  Generally we
wish to characterize the time trends within subjects and between
subjects.  The data will always include the response, the time
covariate and the indicator of the subject on which the measurement
has been made.  If other covariates are recorded, say whether the
subject is in the treatment group or the control group, we may wish to
relate the within- and between-subject trends to such covariates.

In this chapter we introduce graphical and statistical techniques
for the analysis of longitudinal data by applying them to a simple example.

\section{The sleepstudy data}
\label{sec:sleep}

\citet{belenky03:_patter} report on a study of the effects of sleep
deprivation on reaction time for a number of subjects chosen from a
population of long-distance truck drivers.  These subjects were
divided into groups that were allowed only a limited amount of sleep
each night.  We consider here the group of 18 subjects who were
restricted to three hours of sleep per night for the first ten days of
the trial.  Each subject's reaction time was measured several times on
each day of the trial.
<<strsleepstudy>>=
str(sleepstudy)
@ 

In this data frame, the response variable \code{Reaction}, is the average
of the reaction time measurements on a given subject for a given day.
The two covariates are \code{Days}, the number of days of sleep
deprivation, and \code{Subject}, the identifier of the subject on
which the observation was made.
\begin{figure}[tbp]
  \centering
<<sleepxyplot,fig=TRUE,echo=FALSE,height=7,width=7>>=
print(xyplot(Reaction ~ Days | Subject, sleepstudy, aspect = "xy",
             layout = c(6,3), type = c("g", "p", "r"),
             index.cond = function(x,y) coef(lm(y ~ x))[1],
             xlab = "Days of sleep deprivation",
             ylab = "Average reaction time (ms)"))
@ 
  \caption{A lattice plot of the average reaction time versus number
    of days of sleep deprivation by subject for the \code{sleepstudy}
    data.  Each subject's data are shown in a separate panel, along
    with a simple linear regression line fit to the data in that
    panel.  The panels are ordered, from left to right along rows
    starting at the bottom row, by increasing intercept of these
    per-subject linear regression lines.  The subject number is given
    in the strip above the panel.}
  \label{fig:sleepxyplot}
\end{figure}

As recommended for any statistical analysis, we begin by plotting the
data.  The most important relationship to plot for longitudinal data on
multiple subjects is the trend of the response over time by subject,
as shown in Fig.~\ref{fig:sleepxyplot}.  This plot, in which the
data for different subjects are shown in separate panels with the axes
held constant for all the panels, allows for examination of the
time-trends within subjects and for comparison of these patterns
between subjects.  Through the use of small panels in a repeating
pattern Fig.~\ref{fig:sleepxyplot} conveys a great deal of
information, the individual time trends for 18 subjects over 10 days
--- a total of 180 points --- without being overly cluttered.

\subsection{Characteristics of the Data Plot}
\label{sec:DataPlotChar}

The principles of ``Trellis graphics'', developed by Bill
Cleveland\index{Cleveland,William} and his coworkers at Bell Labs and
implemented in the \package{lattice} package for \R{} by Deepayan
Sarkar\index{Sarkar,Deepayan}, have been incorporated in this plot.
As stated above, all the panels have the same vertical and horizontal
scales, allowing us to evaluate the pattern over time for each subject
and also to compare patterns between subjects.  The line drawn in each
panel is a simple least squares line fit to the data in that panel
only.  It is provided to enhance our ability to discern patterns in
both the slope (the typical change in reaction time per day of sleep
deprivation for that particular subject) and the intercept (the
average response time for the subject when on their usual sleep
pattern).

The aspect ratio of the panels (ratio of the height to the width) has
been chosen, according to an algorithm described in
\citet{cleveland93:_visual_data}, to facilitate comparison of slopes.
The effect of choosing the aspect ratio in this way is to have the
slopes of the lines on the page distributed around $\pm 45^\circ$,
thereby making it easier to detect systematic changes in slopes.

The panels have been ordered (from left to right starting at the
bottom row) by increasing intercept.  Because the subject identifiers,
shown in the strip above each panel, are unrelated to the response it
would not be helpful to use the default ordering of the panels, which
is by increasing subject number.  If we did so our perception of
patterns in the data would be confused by the, essentially random,
ordering of the panels. Instead we use a characteristic of the data to
determine the ordering of the panels, thereby enhancing our ability to
compare across panels.  For example, a question of interest to the
experimenters is whether a subject's rate of change in reaction time
is related to the subject's initial reaction time.  If this is the
case then we would expect that the slopes would show an increasing
trend (or, less likely, a decreasing trend) in the left to right,
bottom to top ordering.

There is little evidence in Fig.~\ref{fig:sleepxyplot} of such a
systematic relationship between the subject's initial reaction time
and their rate of change in reaction time per day of sleep
deprivation.  We do see that for all the subjects, except 335,
reaction time increases, more-or-less linearly, with days of sleep
deprivation.  However, there is considerable variation both in the
initial reaction time and in the daily rate of increase in reaction
time.  We can also see that these data are balanced, both with respect
to the number of observations on each subject, and with respect to the
times at which these observations were taken.  This can be confirmed
with a cross-tabulation of \code{Subject} by \code{Days}.
<<xtabssleep>>=
xtabs(~ Subject + Days, sleepstudy)
@ 

In cases like this where there are several observations (10) per
subject and a relatively simple within-subject pattern (more-or-less
linear) we may want to examine coefficients from within-subject
fixed-effects fits.  However, because the subjects constitute a sample
from the population of interest and we wish to drawn conclusions about
typical patterns in the population and the subject-to-subject
variablity of these patterns, we will eventually want to fit mixed
models so we begin doing so.

\section{Mixed-effects models for the sleep data}
\label{sec:SleepMixed}

Based on our preliminary graphical and analytical exploration of these
data, we fit a mixed-effects model with two fixed-effects parameters, the
intercept and slope of the linear time trend for the population, and
two random effects for each subject.  The random effects for a
particular subject are the deviations in intercept and slope of
that subject's time trend from the population values.  
<<fm8>>=
(fm8 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
@

We see that this model incorporates both and intercept and a slope
(w.r.t. \code{Days}) in both the fixed effects and the random effects.
In linear model formulas the intercept term is implicit.  If we had
written the formula as
<<formulafm8,echo=FALSE>>=
Reaction ~ 1 + Days + (1 + Days|Subject)
@
the fitted model would be the same.  Many people prefer to include
the intercept explicitly in the formula to emphasize the relationship
between terms in the formula and coefficients or random effects in the
model.  Others omit these implicit terms to economize on the amount
of typing required.

As this is the first time we have seen a random effects term other
than for a simple, scalar random effect we should discuss the general
form of such terms.  As previously described, the expression on the
right hand side of the vertical bar is evaluated as a factor, the
\emph{grouping factor} for the term.  The expression on the left hand
side is evaluated as a linear model formula, producing a model matrix.
Because the left hand side of \code{(1 + Days|Subject)} produces a
model matrix with two columns, there will be two random effects
associated with each level of \code{Subject}.  That is, there is a
total of 36 random effects generated by this term.  We say that we
have a \emph{vector-valued random effect}, an intercept effect and a
slope effect, for each level of the \code{Subject} factor.
\begin{figure}[tbp]
  \centering
<<fm8Zt,fig=TRUE,echo=FALSE,results=hide,height=3,width=8>>=
print(image(env(fm8)$Zt, sub = NULL, xlab = NULL, ylab = NULL))
@
  \caption{Image of $\vec Z\trans$ for model \code{fm8}}
  \label{fig:fm8Zt}
\end{figure}

As shown in Fig.~ref{fig:fm8Zt}, the 36 columns of $\vec Z$ (rows of
$\vec Z\trans$ in the figure) can be regarded as 18 pairs, one pair
for each level of \code{Subject}. The first column in each pair is the
indicator for that level of \code{Subject}.  The second column in
each pair is the value of the \code{Days} variable but only for the
measurements on that subject.

One way to imagine the process of generating $\vec Z$ is to start with
the $180\times2$ model matrix for the linear model formula \code{1 +
  Days} and the $180\times18$ indicator matrix for the levels of
\code{Subject}.

\begin{figure}[tbp]
  \centering
<<fm8LambdaL,fig=TRUE,echo=FALSE,results=hide,height=3,width=8>>=
print(image(env(fm8)$Lambda, sub = NULL, xlab = expression(Lambda),
            ylab = NULL), split = c(1,1,3,1), more = TRUE)
print(image(tcrossprod(env(fm8)$Lambda), sub = NULL,
            xlab = expression(Sigma), ylab = NULL),
      split = c(2,1,3,1), more = TRUE)
print(image(env(fm8)$L, sub = NULL, xlab = "L", ylab = NULL),
      split = c(3,1,3,1))
@
  \caption{Images of $\Lambda$, $\Sigma$ and $\vec L$ for model \code{fm8}}
  \label{fig:fm8LambdaL}
\end{figure}

The images of $\Lambda$, $\Sigma$ and $L$ for this model
(Fig.~\ref{fig:fm8LambdaL}) show 18 triangular blocks of size 2 along
the diagonal of $\Lambda$, generating 18 square, symmetric blocks of
size 2 along the diagonal of $\Sigma$.  The 18 symmetric blocks on the
diagonal of $\Sigma$ are identical, although this may not be obvious
from the figure.  Overall we estimate two standard deviations and a
correlation for the vector-valued random effect of size 2, as shown in
the model summary.

Often the variances and the covariance of random effects are quoted,
rather than the standard deviations and the correlation shown here.
We have already seen that the variance of a random effect is a poor
scale on which to quote the estimate because confidence intervals on
the variance are so badly skewed.  It is more sensible to assess the
estimates of the standard deviations of random effects or possibly the
logarithms of the standard deviations, when we can be confident that 0
is outside the region of interest.  We do display the estimates of the
variances of the random effects but mostly so that the user can
compare these estimates to those from other software or for cases
where an estimated of a variance is expected (sometimes even required)
to be given in reporting a model fit.

We do not quote estimates of covariances of vector-valued random
effects because the covariance is a difficult scale to interpret.  We
know that a correlation must be between $-1$ and $1$.  A
correlation estimate close to those extremes indicates that
$\Sigma$ is close to singular and the model is not well formulated.


To specify a model with independent random effects by subject for the
intercept and the slope we will use two random-effects terms and write
the formula as
<<formulafm9,echo=FALSE>>=
Reaction ~ Days + (1|Subject) + (0+Days|Subject)
@

In this formula the random effects for the intercept and the random
effects for the slope are modeled as independent random variables
because they are declared in different expressions.  Note that to
specify a random intercept given subject we must explicitly include
the intercept term \code{1} in \code{(1|Subject)} because there are no
other terms.  Similarly in the second expression we suppress the
implicit intercept term by using \code{(0+Days|Subject)}, read as ``no
intercept and \code{Days} by \code{Subject}''.  An alternative
expression for \code{Days} without an intercept by \code{Subject} is
\code{(Days - 1 | Subject)}.

We delay further discussion on the mathematical form of the model and
the interpretation of the model formulae until \S\ref{sec:model_form}.
At this point let us fit these models and examine the parameter
estimates.

We can fit the first model, store the result as \code{fm8}
(sleepstudy model 1), and ask for a brief display of the results with
<<fm8>>=
(fm8 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
@
(The extra set of parentheses surrounding the assignment causes the
fitted model to display itself.  Normally the result of an assignment
is not displayed.)

This brief display includes information on the criterion used to fit
the model (restricted maximum likelihood or REML, see \S\ref{sec:REML}
for a formal definiton), the model formula and the data to which it
was fit, some information on the quality of the fit and information on
the parameter estimates.  For the moment we will concentrate on the
parameter estimates.

The estimates of the fixed effects parameters are
$\widehat{\vec\beta}=(251.41,10.467)\trans$. These represent a typical
initial reaction time (i.e. without sleep deprivation) in the
population of about 250 milliseconds, or 1/4 sec., and a typical
increase in reaction time of a little more than 10 milliseconds per
day of sleep deprivation.

The estimated variance-covariance matrix for the random effects is
displayed by giving the variances of these random variables, the
corresponding standard deviations and any estimated correlations.
Note that the columns labeled \code{Variance} and 
\code{Std.Dev.} in this section are redundant in that each entry in
the \code{Std.Dev.} column is simply the square root of the corresponding
variance estimate.  These estimates are expressed in both the variance
scale and the standard deviation scale because both are useful in
interpretation.  (Some readers may be tempted to interpret the
elements of the \code{Std.Dev.} column as standard errors of the
variance estimates.  Don't do that. These are not standard errors.)

The estimated subject-to-subject variation in the intercept
corresponds to a standard deviation of about 25 ms. A 95\% prediction
interval on this random variable would be approximately $\pm 50$ ms.
Combining this range with a population estimated intercept of 250
ms. indicates that we should not be surprised by intercepts as low as 200
ms. or as high as 300 ms.  This range is consistent with the reference
lines shown in Figure~\ref{fig:sleepxyplot} and the intervals shown in
Figure~\ref{fig:Sl1}.

Similarly, the estimated subject-to-subject variation in the slope
corresponds to a standard deviation of about 6 ms./day so we would not
be surprised by slopes as low as $10.5 - 2\cdot 6=-1.5$ ms./day or as
high as $10.5 + 2\cdot 6=22.5$ ms./day.  Again, the conclusions from
these rough, ``back of the envelope'' calculations are consistent with
our observations from Figures~\ref{fig:sleepxyplot} and \ref{fig:Sl1}.

The estimated residual standard deviation is about 25 ms. leading us to
expect a scatter around the fitted lines for each subject of up to
$\pm 50$ ms.  From Figure~\ref{fig:sleepxyplot} we can see that some
subjects (309, 372 and 337) appear to have less variation than $\pm
50$ ms. about their within-subject fit but others (308, 332 and 331)
may have more.

Finally, we see the estimated within-subject correlation of the random
effect for the intercept and the random effect for the slope is very
low, $0.066$, confirming our impression that there is little evidence
of a systematic relationship between these quantities.  In other
words, observing a subject's initial reaction time does not give us
much information for predicting whether their reaction time will be
strongly affected by each day of sleep deprivation or not.

By fitting model \code{fm9} with independent random effects for
intercept and slope and comparing this fitted model to \code{fm8} we
can assess this claim using a statistical hypothesis test.
<<fm9>>=
(fm9 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy))
anova(fm9, fm8)
@ 

We can see that the fitted model \code{fm9} is quite similar to
\code{fm8} except for the obvious difference that there is no
within-subject correlation of the random effects in \code{fm9}.  The
estimates of all the other parameters, which are common to the two
models, are practically unchanged.

The call to \code{anova} compares the two fitted models using a
likelihood ratio test, which evaluates the change in the quality of
the fits, as measured by the deviance (defined in
\S\ref{sec:overview-theory}), relative to the change in the number of
parameters.  The results of this test indicate that the model
\code{fm8} does not fit significantly better than the model \code{fm9}
and hence we prefer the model \code{fm9} which has fewer parameters.

We conclude that there is significant variation between subjects in
both the initial reaction time and in the rate of change in reaction
time with respect to days of sleep deprivation but that these changes
are not correlated.  That is knowing a person's initial reaction time
does not help us to predict their response to sleep deprivation.

\section{Assessing the precision of the parameter estimates}
\label{sec:assess-prec-param}

Plots of the profile $\zeta$ (Figure~\ref{fig:pr2plt})
\begin{figure}[ht]
  \centering
<<pr2plt,fig=TRUE,echo=FALSE,height=3.5>>=
print(xyplot(pr9, aspect = 1.3, layout = c(5,1)))
@ 
\caption{Profile zeta plot for each of the parameters in model
  \code{fm9}.  The vertical lines are the endpoints of 50\%, 80\%,
  90\%, 95\% and 99\% profile-based confidence intervals for each
  parameter.}
  \label{fig:pr2plt}
\end{figure}
show that confidence intervals $\sigma_1$ and $\sigma_2$ will be
slightly skewed; those for $\log(\sigma)$ will be symmetric and
well-approximated by methods based on quantiles of the standard normal
distribution and those for the fixed-effects parameters, $\beta_1$ and
$\beta_2$ will be symmetric and slightly over-dispersed relative to
the standard normal.  For example, the 95\% profile-based confidence
intervals are
<<confintpr2>>=
confint(pr2)
@ 

The profile pairs plot (Figure~\ref{fig:pr2pairs})
\begin{figure}[ht]
  \centering
<<pr2pairs,fig=TRUE,echo=FALSE,height=8>>=
print(splom(pr2))
@ 
\caption{Profile pairs plot for the parameters in model \code{fm9}.
  The contour lines correspond to marginal 50\%, 80\%, 90\%, 95\% and
  99\% confidence regions based on the likelihood ratio.  Panels below
  the diagonal represent the $(\zeta_i,\zeta_j)$ parameters; those above
  the diagonal represent the original parameters.}
  \label{fig:pr2pairs}
\end{figure}


In the previous section we used a likelihood ratio test to assess
whether the covariance of the random effects within subject is
significantly different from zero.  This is an example of a
statistical hypothesis test, which is one form of statistical
inference.  Another, related, form of statisticial inference is
assessing the precision of the parameter estimates, say by forming
confidence intervals or more general confidence regions.

For some statistical models it is possible to derive the theoretical
distributions of the parameter estimates and use these theoretical
distributions to create confidence intervals or regions.  At present,
the theoretical tools to analyze the general form of a linear mixed
model in this ``exact'' approach are not available and confidence
intervals and regions are typically created using approximations,
especially what are called ``asymptotic'' approximations that are
expected to perform well for large data sets.

Recently another approach to statistical inference, using Markov chain
Monte Carlo (MCMC) samples from the (Bayesian) posterior distribution
of the parameters, has gained popularity.  Although MCMC is a
computationally intensive approach to inference the current
availability of inexpensive, powerful computers has made it feasible
for models such as linear mixed models.

An advantage of using MCMC samples to assess the precision of the
parameter estimates is that this approach uses the actual distribution
of the parameters and not an approximation.  Furthermore we can use
graphical techniques to visualize these distributions and provide
insight into the behavior of the parameters in the model.

MCMC sampling methods are based on a Bayesian formulation of the
linear mixed model in which the parameters are considered to be random
variables that have a \emph{prior} distribution (prior in the sense of
``before the data are known'') and a \emph{posterior}, or ``after the
data are known'' distribution.  Instead of confidence intervals on the
parameters we will formulate \emph{highest posterior density} (HPD)
intervals \citep{box73:_bayes_infer_statis_analy}.  An 95\% HPD
interval on a parameter is the shortest interval that contains 95\% of the
probability content of the posterior distribution.  It is the Bayesian
equivalent of a 95\% confidence interval on the parameter.


Details of the particular Bayesian formulation of the linear mixed
model that we use, including the choice of prior distributions for the
parameters, are given in \S\ref{sec:bayes-form-model}.

\subsection{Posterior distributions from model \code{fm9}}
\label{sec:post-param-distr}

The function \code{mcmcsamp} applied to a fitted lmer model produces
an MCMC sample from the posterior distribution of the parameter
estimates, from which we can evaluate HPD intervals. Let us create and
store a sample of size 10,000 from the posterior distribution of the
parameters in model \code{fm9}.
<<ss2seed,echo=FALSE,results=hide>>=
set.seed(123454321)
@ 
<<ss2>>=
#ss2 <- mcmcsamp(fm9,10000)
@ 

The \code{HPDinterval} function creates HPD intervals on each of the
parameters in the sample.  By default it returns intervals whose
empirical probability content is 95\% (this can be changed, if
desired).
<<ss2HPD>>=
#HPDinterval(ss2[,-6])
@ 

Notice that in the intervals all the variance parameters are reported
on the logarithm scale.  The reason for taking this transformation is
because the logarithm of a variance tends to be symmetrically
distributed as shown by the density plots in Figure~\ref{fig:ss2density}
<<ss2density,fig=TRUE,echo=FALSE,height=2.9,width=7>>=
#print(densityplot(ss2[,-6],plot.points=FALSE,aspect='xy',
#                  type=c("g","l"),layout=c(3,2)))
@ 
% \begin{figure}[tbp]
%   \centerline{\includegraphics[width=\textwidth]{figs/Long-ss2density}}
%     \caption{Empirical density estimates of a Markov Chain Monte Carlo
%       sample from the posterior distribution of the parameters in
%       model \code{Fm9}.  The variance parameters are shown on the
%       logarithm scale.}
%   \label{fig:ss2density}
% \end{figure}

Not only are the posterior distributions on this scale symmetric, they
are very close to normal distributions as shown by their normal
probability plots (Figure~\ref{fig:ss2qqmath})
<<ss2qqmath,fig=TRUE,echo=FALSE,height=2.5,width=7>>=
#print(qqmath(window(ss2[,-6],thin=10), pch = ".", cex = 1.8,
#             type = c("g", "p"), layout = c(5,1),
#             xlab = "Quantiles of a standard normal distribution"))
@ 
% \begin{figure}[tbp]
%   \centerline{\includegraphics[width=\textwidth]{figs/Long-ss2qqmath}}
%   \caption{Normal probability plots of the Markov Chain Monte Carlo
%     sample from the posterior distribution of the parameters in model
%     \code{fm9}.  The variance and covariance parameters have been
%     transformed to the logarithms of the variances and Fisher's z
%     transformation of the correlations, respectively.}
%   \label{fig:ss2qqmath}
% \end{figure}

\section{Examining the random effects}

  \begin{itemize}
  \item Although the random effects $\vec b$ behave like parameters in
    the linear predictor, technically they are not parameters in the
    model.
  \item Instead of referring to ``estimates'' of the random effects it
    is customary to refer to ``predictors'' - in particular, the best
    linear unbiased predictors or BLUPs.
  \item These values are also the modes of the conditional
    distribution (i.e. given the data $\vec y$ and the estimates of
    $\vec\beta$, $\sigma^2$ and $\Sigma$) of $\vec b$.
  \item For linear mixed model the conditional
    distribution $[\vec b|\vec y,\sigma^2,\Sigma]$ is normal (Gaussian)
    hence the modes are also the conditional means.
  \item The \code{ranef} extractor function returns these conditional
    modes evaluated at the parameter estimates.
  \end{itemize}
<<rr1>>= 
(rr1 <- ranef(fm8))
@ 

  \begin{itemize}
  \item For this model we can combine the BLUPs of the random effects
\begin{figure}[tbp]
  \centering
<<rr1plot,echo=FALSE,fig=TRUE,results=hide,height=7,width=7>>=
print(plot(rr1, aspect = 1, type = c("g", "p")))
@
  \caption{Scatterplot of the conditional modes, or BLUPs, of the
    random effects for model \code{Fm8}.  Each point represents the
    mode of the distribution of the random effects for the intercept
    and slope associated with one of the subjects.}
  \label{fig:rr1plot}
\end{figure}
    and the estimates of the fixed effects to get BLUPs for the
    within-subject coefficients.
  \item These BLUPs will be ``shrunken'' towards the fixed-effects
    estimates relative to the estimated coefficients from only that
    subjects data.  John Tukey called this ``borrowing strength''
    between subjects.
  \item Plotting the shrinkage of the within-subject coefficients
    shows that some of the coefficients are considerably shrunken
    toward the fixed-effects estimates.
  \item However, comparing the within-group and mixed model fitted
    lines shows that large changes in coefficients occur in the noisy
    data.  Precisely estimated within-group coefficients are not
    changed substantially.
  \end{itemize}
\begin{figure}[tbp]
  \centering
<<shrinkage,echo=FALSE,fig=TRUE,height=7,width=7>>=
df <- coef(lmList(Reaction ~ Days | Subject, sleepstudy))
cc1 <- as.data.frame(coef(fm8)$Subject)
names(cc1) <- c("A", "B")
df <- cbind(df, cc1)
with(df,
     print(xyplot(`(Intercept)` ~ Days, aspect = 1,
            x1 = B, y1 = A, 
            panel = function(x, y, x1, y1, subscripts, ...) {
                panel.grid(h = -1, v = -1)
                x1 <- x1[subscripts]
                y1 <- y1[subscripts]
                panel.arrows(x, y, x1, y1, type = "closed", length = 0.1,
                             angle = 15, ...)
                panel.points(x, y,
                             pch = trellis.par.get("superpose.symbol")$pch[2],
                             col = trellis.par.get("superpose.symbol")$col[2])
                panel.points(x1, y1,
                             pch = trellis.par.get("superpose.symbol")$pch[1],
                             col = trellis.par.get("superpose.symbol")$col[1])
            },
                   key = list(space = "top", columns = 2,
                   text = list(c("Mixed model", "Within-group")),
                   points = list(col = trellis.par.get("superpose.symbol")$col[1:2],
                   pch = trellis.par.get("superpose.symbol")$pch[1:2]))
               )))
@ %$
  \caption{Comparison of the within-subject estimates of the intercept
    and slope for each subject and the conditional modes of the
    per-subject intercept and slope.  Each pair of points joined by an
    arrow are the within-subject and conditional mode estimates for
    the same subject.  The arrow points from the within-subject
    estimate to the conditional mode for the mixed-effects model.}
  \label{fig:shrinkage}
\end{figure}
\begin{figure}[tbp]
  \centering
<<shrinkfit,echo=FALSE,fig=TRUE,height=7,width=7>>=
print(xyplot(Reaction ~ Days | Subject, sleepstudy, aspect = "xy",
             layout = c(6,3), type = c("g", "p", "r"),
             coef.list = df[,3:4],
             panel = function(..., coef.list) {
                 panel.xyplot(...)
                 panel.abline(as.numeric(coef.list[packet.number(),]),
                              col.line = trellis.par.get("superpose.line")$col[2],
                              lty = trellis.par.get("superpose.line")$lty[2]
                              )
             },
             index.cond = function(x,y) coef(lm(y ~ x))[1],
             xlab = "Days of sleep deprivation",
             ylab = "Average reaction time (ms)",
             key = list(space = "top", columns = 2,
             text = list(c("Mixed model", "Within-group")),
             lines = list(col = trellis.par.get("superpose.line")$col[2:1],
             lty = trellis.par.get("superpose.line")$lty[2:1]))))
@ 
  \caption{Comparison of the predictions from the within-subject fits
    with those from the conditional modes of the subject-specific
    parameters in the mixed-effects model.}
  \label{fig:shrinkfit}
\end{figure}

\subsection{Prediction intervals on the random effects}
\label{sec:pred-interv-rand}

  \begin{itemize}
  \item For the linear mixed model we can calculate both the means and
    the variances of the random-effects conditional on the estimated
    values of the model parameters, which allows us to calculate
    prediction intervals on the values of individual random
    effects.
  \item We plot the prediction intervals as
    a normal probabity plot so we can see the overall shape of the
    distribution of the means and which of the random effects are
    ``significantly different'' from zero.
  \item Note that failure of the conditional means of the random
    effects to look like a normal (Gaussian) distribution is not
    terribly alarming.  It is the ``prior'' distribution of the random
    effects that is assumed to be normal.  The conditional means or
    BLUPs are strongly influenced by the data and may appear non-normal.
  \end{itemize}
\begin{figure}[tbp]
  \centering
<<caterpillar,fig=TRUE,echo=FALSE,results=hide,height=4,width=7>>=
#print(qqmath(ranef(fm8,post=TRUE))[[1]])
@
  \caption{Prediction intervals on the random effects per subject.}
  \label{fig:caterpillar}
\end{figure}

\section{Model specification for \code{lmer}}
\label{sec:model-spec-codelm}

A linear mixed-effects model to be fit by \code{lmer} is specified by
the \code{formula} argument.  For model \code{fm8} the formula is
<<formulafm8,echo=FALSE>>=
Reaction ~ Days + (Days|Subject)
@ 
which can be read as ``\code{Reaction} is modeled by \code{Days} and
\code{Days} given \code{Subject}''.  That is, the response, which is
the variable named \code{Reaction}, is to be modeled by one conditional
term, \code{(Days|Subject)}, and one unconditional term, \code{Days}.

A conditonal term (any term including the vertical bar, \code{|})
contributes to the random effects specification.  An unconditional
term contributes to the fixed effects specification.

The unconditional terms, together with the data to be fit, generate an
$n\times p$ fixed-effects model matrix $\vec X$ according to the rules
that we describe in \S\ref{sec:model_formulas}.  The dimensions $n$
and $p$ are the number of observations and the dimension of the
fixed-effects parameter vector $\vec\beta$, respectively.  In this case
of model \code{fm8} the only unconditional term, \code{Days}, is the
name of a numeric variable, which is incorporated as a column,
labelled \code{Days} in $\vec X$. By convention, the intercept term,
which generates a column of $1$'s labelled \code{(Intercept)}, is
included implicitly in the model specification.  (This column can be
suppressed if desired, as shown below.)

The first few rows of the $180\times 2$ model matrix $\vec X$ for model
\code{fm8} are
<<headmodelmatrixfm8>>=
head(model.matrix(fm8))
@ 

Each conditional term in the model formula generates a set of random
effects and variance-covariance matrix for these random effects.  In a
conditional term the expression on the right hand side of the \code{|}
is evaluated as a factor called the \emph{grouping factor} for the
term.  Because a factor associates one of a finite set of levels with each
observation, we can consider a factor as dividing the
observations into groups corresponding to the levels of the factor.
Observations within such groups share a set of random effects.
The expression on the left of the \code{|} in a conditional term is
evaluated as a linear model formula and determines the number and form
of the random effects associated with each level of the grouping
factor.  Thus \code{(Days|Subject)} designates an (implicit) intercept
coefficient and a \code{Days} coefficient for each level of the
\code{Subject} grouping factor producing, as we have seen, 36 random
effects --- two for each of the 18 subjects.

Random effects generated by different conditional terms are
independent, as are random effects corresponding to different levels
of the grouping factor in the same conditional term.

Each group of random effects models some of the variation in the
response.  There is one further level of variation in the model - the
``per-observation'' or ``residual'' noise.  It is the unexplained
variation or what is ``left over'' after we have modeled all the other
sources of variation in the model.  This level of variation is modeled
as an $n$-dimensional vector $\vec\epsilon\sim\mathcal{N}(\vec0,\sigma^2\vec I)$
where $\vec I$ is an identity matrix.  That is, the elements of $\vec\epsilon$
is independent and identical normal random variates with mean zero and
variance $\sigma^2$.

The general model allows for multiple conditional terms in a model
specification generating multiple groups of random effects.  Let $k$
be the number of conditional terms, $n_i,i=1,\dots,k$ be the number of
levels of the grouping factor for the $i$th such term and $q_i$ be the
number of random effects associated with each level of the grouping
factor.

If we represent all the random effects as a vector $\vec b$, of length
$q=\sum_{i=1}^kq_in_i$, we can write
the model as
\begin{equation}
  \label{eq:lmmGeneral}
  \vec y = \vec X\vec\beta + \vec Z\vec b + \vec\epsilon,\quad
  \vec b\sim\mathcal{N}\left(\vec0,\Sigma\right),
  \vec\epsilon\sim\mathcal{N}\left(\vec0,\sigma^2\vec I\right),\vec b\perp\vec\epsilon
\end{equation}
where $\vec Z$ is an $n\times q$ model matrix generated from the
conditional terms, $\Sigma$ is a $q\times q$ variance-covariance
matrix, also generated from the conditional terms, and the symbol
$\perp$ denotes independent random variables.

Although the total number of random effects, $q$, can be large and
hence the dimensions of $\vec Z$ and $\Sigma$ in the general form
(\ref{eq:lmmGeneral}) can be very large, these matrices are sparse and
patterned and thus are determined by a relatively small number of
values.

For model \code{fm8}, $k=1$, $q_1=2$ and $n_i=18$ so, as we have seen,
$q=36$. However, the assumptions of independence of random effects
associated with different subjects means that the $36\times 36$ matrix
$\Sigma$ consists of the $2\times 2$ matrix $\Sigma_1$
repeated $18$ times in the pattern
\begin{equation}
  \label{eq:bSigmafm8}
  \Sigma=
  \begin{bmatrix}
    \Sigma_1 & \vec0     & \hdots & \vec0\\
    \vec0     & \Sigma_1 & \hdots & \vec0\\
    \vdots    & \vdots    & \ddots & \vdots\\
    \vec0     & \vec0     & \hdots & \Sigma_1
  \end{bmatrix}
\end{equation}

Also, although the model matrix $\vec Z$ has dimension $180\times 36$,
all but two of the elements in any given row are known to be zero.
Each row corresponds to one and only one subject and the model for
that row incorporates only the two random effects associated with that
subject. The other $34$ random effects associated with other subjects
are multiplied by zero.

The matrix $\vec Z$ is evaluated and stored as a sparse matrix.  The
first few rows of $\vec Z$ for model \code{fm8} are
<<Zfm8,echo=FALSE>>=
head(t(env(fm8)$Zt))
@ 
These rows correspond to the first $6$ observations on the first subject.
In the representation as a sparse matrix an element that is known to
be zero prints as a `.'. (The (1,2) element of this matrix does have
the value zero because the value of the \code{Days} variable is zero
for this observation.  It could have another value if, for example, we
renumbered the \code{Days} and thus is not a systematic zero in the
matrix.)

Easier to understand, perhaps, is the image of $\vec Z\trans$ in
Figure~\ref{fig:fm9Zt}.
\begin{figure}[tbp]
  \centering
<<fm9Zt,fig=TRUE,echo=FALSE,results=hide,height=3,width=8>>=
print(image(env(fm9)$Zt, sub = NULL, xlab = NULL, ylab = NULL))
@
\caption{Image of $\vec Z\trans$, the transpose of $\vec Z$, the random
  effects model matrix in model \code{fm9}}
  \label{fig:fm9Zt}
\end{figure}



In the model $\vec b$ is a random variable
\begin{equation}
  \label{eq:bdist}
  \vec b\sim\mathcal{N}(\vec0,\Sigma)
\end{equation}
and it is the elements of $\Sigma$ that we estimate.  Because we
assume that random effects associated with different grouping factors
are indep
which we can write as
If we write the
model matrix for the entire random effects vector as $\vec Z$ and the
variance-cov
matrix, which we call $\vec Z$, for the random effects vector.

contribute to another model matrix, which we call
$\vec Z$, through a slightly more complicated mechanism.  In a
conditional term the expression on the left of the \code{|} is
interpreted as a linear model formula and used to create a model
matrix while the expression on the right of the \code{|} is evaluated as a
factor, called the \emph{grouping factor} for the term.  We allow
multiple conditional terms in a model specification so we will refer
to the $i$th conditional term even though there is only one such
conditional term in the model specification for model \code{fm8}.

If the model matrix from the expression on the left of the $i$th
conditional has $q_i$ columns and the grouping factor has $n_i$ levels
then the expression contributes $q_in_i$ columns to the matrix $\vec Z$.

\section{Conclusions from the example}

  \begin{itemize}
  \item Carefully plotting the data is enormously helpful in
    formulating the model.
  \item It is relatively easy to fit and evaluate models to data like
    these, from a balanced designed experiment.
  \item For a linear mixed model the estimates of the fixed effects
    typically have a symmetric distribution close to a Gaussian distribution.
  \item The distribution of the variance components or the covariances
    are not symmetric, which is why we transform these parameters to a
    symmetric scale.
  \item We use the MCMC sample to create confidence (actually HPD)
    intervals on the fixed-effects parameters.  We could also use the
    parameter estimates and standard errors.
  \item The ``estimates'' (actually BLUPs) of the random effects can
    be considered as penalized estimates of these parameters in that
    they are shrunk towards the origin.
  \item Most of the prediction intervals for the random effects
    overlap zero.
  \end{itemize}
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
\begin{prob}\label{probL1}
    Check the structure of documentation, structure and a summary of
  the \code{Orthodont} data set.
  \begin{enumerate}
  \item Create an \code{xyplot} of the \code{distance} versus
    \code{age} by \code{Subject} for the female subjects
    only.  You can use the optional argument \code{subset = Sex ==
      "Female"} in the call to \code{xyplot} to achieve this.  Use
    the optional argument \code{type = c("g","p","r")} to add
    reference lines to each panel.
  \item Enhance the plot by choosing an aspect ratio for which the
    typical slope of the reference line is around 45$^o$.  You can set
    it manually (something like \code{aspect = 4}) or with an
    automatic specification (\code{aspect = "xy"}).  Change the layout
    so the panels form one row (\code{layout = c(11,1)}).
  \item Order the panels according to increasing response at age 8.
    This is achieved with the optional argument \code{index.cond}
    which is a function  of arguments \code{x} and \code{y}.  In this
    case you could use \code{index.cond = function(x,y) y[x == 8]}.
    Add meaningful axis labels.  Your final plot should be like
    \begin{center}
<<orthofem,echo=FALSE,fig=TRUE,height=4.5>>=
print(xyplot(distance ~ age|Subject, Orthodont, subset = Sex == "Female",
             index.cond = function(x,y) y[x == 8],
             aspect = 'xy', layout = c(11,1), type = c("g","p","r"),
             xlab = "Age (yr)",
             ylab = "Distance from pituitary to pterygomaxillary fissure (mm)"))
@     
    \end{center}

  \item Fit a linear mixed model to the data for the females only with
    random effects for the intercept and for the slope by subject,
    allowing for correlation of these random effects within subject.
    Relate the fixed effects and the random effects' variances and
    covariances to the variability shown in the figure.
  \item Produce a ``caterpillar plot'' of the random effects for
    intercept and slope.  Does the plot indicate correlated random effects?
  \item Consider what the Intercept coefficient and random effects
    represents.  What will happen if you center the ages by
    subtracting 8 (the baseline year) or 11 (the middle of the age range)?
  \item Repeat for the data from the male subjects.
  \end{enumerate}
\end{prob}
\begin{prob}
\item Fit a model to both the female and the male subjects in the
  \code{Orthodont} allowing for differences by sex in the
  fixed-effects for intercept (probably with respect to the centered
  age range) and slope.
\end{prob}
