\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=8,strip.white=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Intro,include=TRUE}
\setkeys{Gin}{width=\textwidth}

<<preliminaries,echo=FALSE,print=FALSE,results=hide>>=
options(width=65, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
library(splines)
library(lattice)
library(Matrix)
library(lme4a)
fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff)
fm1ML <- update(fm1, REML = FALSE)
VCREML <- VarCorr(fm1)
VCML <- VarCorr(fm1ML)
##' Min y and arg min y for equally spaced x and corresponding y
##' points in a grid.
gmin <- function(x, y)
{
    if (missing(x))
        x <- seq_along(y)
    mpos <- which.min(y)
    if (mpos %in% c(1L, length(y))) return(c(x[mpos], y[mpos]))
    conv <- array(c(0,-1,1,2,0,-2,0,1,1), c(3,3))/2
    ycoef <- conv %*% y[mpos + -1:1]
    pos <- -ycoef[2]/(2*ycoef[3])
    stopifnot(-1 < pos, pos < 1)
    b <- c(1, pos, pos^2)
    c(crossprod(conv %*% x[mpos + -1:1], b), crossprod(ycoef, b))
}
if (file.exists("grd1.rda")) {
    load("grd1.rda")
    sigB <- sort(unique(grd1$sigB))
    sigmas <- sort(unique(grd1$sigma))
    deviance <- array(grd1$deviance + deviance(fm1ML),
                      c(length(sigB), length(sigmas)))
    REML <- array(grd1$REML + deviance(fm1),
                  c(length(sigB), length(sigmas)))
} else {
    sigmaDev <- function(fenv, theta, sigma)
    {
        fenv@setPars(theta)
        sigmasq <- sigma^2
        dc <- devcomp(fenv)
        dev <- dc$cmp
        unname(dev["ldL2"] +  dev["prss"]/sigmasq +
               c(dev["ldRX2"],0) +
               dc$dims[c("nmp","n")] * log(2*pi*sigmasq))
    }
    sigmaDev1 <- function(fenv, pars)
    {
        pars <- as.numeric(pars)
        stopifnot((lp <- length(pars)) > 1)
        sigmaDev(fenv, pars[-lp]/pars[lp], pars[lp])
    }
    fm1env <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, doFit = FALSE)
    sigB <- seq(0, 225,len = 101)
    sigmas <- seq(30, 100,len = 101)
    grd1 <- expand.grid(sigB = sigB, sigma = sigmas)
    attr(grd1, "out.attrs") <- NULL          # save a bit of space
    vals <- apply(t(grd1[, 1:2]), 2, sigmaDev1, fenv = fm1env)
    grd1$REML <- vals[1,] - deviance(fm1)
    grd1$deviance <- vals[2,] - deviance(fm1ML)
    deviance <- array(vals[2,], c(length(sigB), length(sigmas)))
    REML <- array(vals[1,], c(length(sigB), length(sigmas)))
    save(grd1, file = "grd1.rda")
}
@

\chapter{A simple linear mixed-effects model}
\label{chap:ExamLMM}

In this book we describe the theory behind a type of statistical model
called \emph{mixed-effects} models and the practice of fitting and
analyzing such models using the \package{lme4} package for \R{}.
These models are used in many different disciplines.  Because the
descriptions of the models can vary markedly between disciplines, we
begin by describing what mixed-effects models are and by exploring a
very simple example of one type of mixed model, the \emph{linear mixed
  model}.

%% FIXME: Review this paragraph once the chapter is complete
This simple example allows us to illustrate the use of the \code{lmer}
function in the \package{lme4} package for fitting such models and
analyzing the fitted model.  Building from the example we describe the
general form of linear mixed models that can be fit using \code{lmer}.

\section{Mixed-effects models}
\label{sec:memod}

Mixed-effects models, like many other types of statistical models,
describe a relationship between a \emph{response} variable and some of
the \emph{covariates} that have been measured or observed along with
the response.  In mixed-effects models at least one of the covariates
is a \emph{categorical} covariate representing experimental or
observational ``units'' in the data set.  In the example from the
chemical industry that is given in this chapter, the observational
unit is the batch of an intermediate product used in production of a
dye.  In medical and social sciences the observational units are often
the human or animal subjects in the study.  In agriculture the
experimental units may be the plots of land or the specific plants
being studied.

In all of these cases the categorical covariate or covariates are
observed at a set of discrete \emph{levels}.  We may use numbers, such
as subject identifiers, to designate the particular levels that we
observed but these numbers are simply labels.  The important
characteristic of a categorical covariate is that, at each observed
value of the response, the covariate takes on the value of one of a
set of distinct levels.

Parameters associated with the particular levels of a covariate are
sometimes called the ``effects'' of the levels.  If the set of
possible levels of the covariate is fixed and reproducible we model
the covariate using \emph{fixed-effects} parameters.  If the levels
that we observed represent a random sample from the set of all
possible levels we incorporate \emph{random effects} in the model.

There are two things to notice about this distinction between
fixed-effects parameters and random effects.  First, the names are
misleading because the distinction between fixed and random is more a
property of the levels of the categorical covariate than a property of
the effects associated with them.  Secondly, we distinguish between
``fixed-effects parameters'', which are indeed parameters in the
statistical model, and ``random effects'', which, strictly speaking,
are not parameters.  As we will see shortly, random effects are
unobserved random variables.

To make the distinction more concrete, suppose that we wish to model
the annual reading test scores for students in a school district and
that the covariates recorded with the score include a student
identifier and the student's gender. Both of these are categorical
covariates.  The levels of the gender covariate, male and female, are
fixed.  If we consider data from another school district or we
incorporate scores from earlier tests, we will not change those
levels.  On the other hand, the students whose scores we observed
would generally be regarded as a sample from the set of all possible
students whom we could have observed.  Adding more data, either from
more school districts or from results on previous or subsequent tests,
will increase the number of distinct levels of the student identifier.

\emph{Mixed-effects models} or, more simply, \emph{mixed models} are
statistical models that incorporate both fixed-effects parameters and
random effects.  Because of the way that we will define random
effects, a model with random effects always includes at least one
fixed-effects parameter.  Thus, any model with random effects is a
mixed model.

We characterize the statistical model in terms of two random
variables: a $q$-dimensional vector of random effects represented by
the random variable $\mathcal{\bm B}$ and an $n$-dimensional response
vector represented by the random variable $\mathcal{\bm Y}$.  We
observe the value, $\bm y$, of $\mathcal{\bm Y}$.  We do not observe
the value of $\mathcal{\bm B}$.

When formulating the model we describe the unconditional distribution
of $\mathcal{\bm B}$ and the conditional distribution, $(\mathcal{\bm
  Y}|\mathcal{\bm B}=\bm b)$.  The descriptions of the distributions
involve the form of the distribution and the values of certain
parameters.  We use the observed values of the response and the
covariates to estimate these parameters and to make inferences about
them.

That the big picture.  Now let's make this more concrete by describing
a particular, versatile class of mixed models called linear mixed
models and by studying a simple example of such a model.  First we
will describe the data in the example.

\section{The \texttt{Dyestuff} and
  \texttt{Dyestuff2} data}
\label{sec:DyestuffData}

Models with random effects have been in use for a long time.  The
first edition of the classic book, \emph{Statistical Methods in
  Research and Production}, edited by O.L. Davies, was published in
1947 and contained examples of the use of random effects to
characterize batch-to-batch variability in chemical processes.  The
data from one of these examples are available as the \code{Dyestuff}
data in the \package{lme4} package. In this section we describe and
plot these data and introduce a second example, the \code{Dyestuff2}
data, described in \citet{box73:_bayes_infer_statis_analy}.

\subsection{The \texttt{Dyestuff} data}
\label{sec:dyestuff}

The \code{Dyestuff} data are described in \citet[Table~6.3,
p.~131]{davies72:_statis_method_in_resear_and_produc}, the fourth
edition of the book mentioned above, as coming from
\begin{quote}
  an investigation to find out how much the
   variation from batch to batch in the quality of an intermediate
   product (H-acid) contributes to the variation in the yield of the
   dyestuff (Naphthalene Black 12B) made from it.  In the experiment six
   samples of the intermediate, representing different batches of works
   manufacture, were obtained, and five preparations of the dyestuff
   were made in the laboratory from each sample.  The equivalent yield
   of each preparation as grams of standard colour was determined by
   dye-trial.
\end{quote}

To access these data within \R{} we must first attach the \code{lme4}
package to our session using
<<lme4>>=
library(lme4a)
@
Note that the \code{"$>$"} symbol in the line shown is the prompt in
\R{} and not part of what the user types. The \package{lme4} package
must be attached before any of the data sets or functions in the
package can be used.  If typing this line results in an error report
stating that there is no package by this name then you must first
install the package.

In what follows, we will assume that the \package{lme4} package has
been installed and that it has been attached to the \R{} session
before any of the code shown has been run.

The \code{str} function in \R{} provides a concise description of the
structure of the data
<<strDyestuff>>=
str(Dyestuff)
@
from which we see that it consists of $30$ observations of the
\code{Yield}, the response variable, and of the covariate,
\code{Batch}, which is a categorical variable stored as a
\code{factor} object.  If the labels for the factor levels are
arbitrary, as they are here, we will use letters instead of numbers
for the labels.  That is, we label the batches as \code{"A"} through
\code{"F"} rather than \code{"1"} through \code{"6"}.  When the labels are
letters it is clear that the variable is categorical.  When the labels
are numbers a categorical covariate can be mistaken for a numeric
covariate, with unintended consequences.

It is a good practice to apply \code{str} to any data frame the first
time you work with it and to check carefully that any categorical
variables are indeed represented as factors.

The data in a data frame are viewed as a table with columns
corresponding to variables and rows to observations. The functions
\code{head} and \code{tail} print the first or last few rows
(the default value of ``few'' happens to be $6$ but we can specify
another value if we so choose)
<<headDyestuff>>=
head(Dyestuff)
@ 
or we could ask for a \code{summary} of the data
<<summaryDyestuff>>=
summary(Dyestuff)
@ 

\begin{figure}[tbp]
  \centering
<<Dyestuffdot,fig=TRUE,echo=FALSE,height=3.5>>=
set.seed(1234543)
print(dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff,
              ylab = "Batch", jitter.y = TRUE, pch = 21,
              xlab = "Yield of dyestuff (grams of standard color)",
              type = c("p", "a")))
@   
\caption[Yield of dyestuff from 6 batches of an intermediate]{Yield of
  dyestuff (Napthalene Black 12B) for 5 preparations from each of 6
  batches of an intermediate product (H-acid).  The line joins the
  mean yields from the batches, which have been ordered by increasing
  mean yield.  The vertical positions are ``jittered'' slightly to
  avoid over-plotting.  Notice that the lowest yield for batch A was
  observed for two distinct preparations from that batch.}
  \label{fig:Dyestuffdot}
\end{figure}
Although the \code{summary} does show us an important property of the
data, namely that there are exactly $5$ observations on each batch --- a
property that we will describe by saying that the data are
\emph{balanced} with respect to \code{Batch} --- we usually learn much
more about the structure of such data from plots like
Figure~\ref{fig:Dyestuffdot} than we can from
numerical summaries.

In Figure~\ref{fig:Dyestuffdot} we can see that there is considerable
variability in yield, even for preparations from the same batch, but
there is also noticeable batch-to-batch variability.  For example,
four of the five preparations from batch F provided lower yields than
did any of the preparations from batches C and E.  

This plot, and essentially all the other plots in this book,
were created using Deepayan Sarkar's \package{lattice} package for
\R{}.  In \citet{sarkar08:_lattic} he describes how one would create
such a plot.  Because this book was created using Sweave
\citep{lmucs-papers:Leisch:2002}, the exact code used to create the
plot, as well as the code for all the other figures and calculations
in the book, is available on the web site for the book.  In
section~\ref{sec:lattice} we review some of the principles of
lattice graphics, such as reordering the levels of the \code{Batch}
factor by increasing mean response, that enhance the informativeness
of the plot.  At this point we will concentrate on the information
conveyed by the plot and not on how the plot is created.

In section \ref{sec:DyestuffLMM} we will use mixed models to
quantify the variability in yield between batches.  For the time being
let us just note that the particular batches used in this experiment
are a selection or sample from the set of all batches that we wish to
consider.  Furthermore, the extent to which one particular batch tends
to increase or decrease the mean yield of the process --- in other
words, the ``effect'' of that particular batch on the yield --- is not
as interesting to us as is the extent of the variability between
batches.  For the purposes of designing, monitoring and controlling a
process we want to predict the yield from future batches, taking into
account the batch-to-batch variability and the within-batch
variability.  Being able to estimate the extent to which a particular
batch in the past increased or decreased the yield is not usually an
important goal for us.  We will model the effects of the batches as
random effects rather than as fixed-effects parameters.

\subsection{The \texttt{Dyestuff2} data}
\label{sec:dyestuff2}

The \code{Dyestuff2} data are simulated data presented in \citet[Table
5.1.4, p. 247]{box73:_bayes_infer_statis_analy} where the authors state
\begin{quote}
  These data had to be constructed for although examples of this sort
  undoubtedly occur in practice they seem to be rarely published.
\end{quote}
The structure and summary
<<strDye2>>=
str(Dyestuff2)
summary(Dyestuff2)
@
are intentionally similar to those of the \code{Dyestuff} data.  As
can be seen in Figure~\ref{fig:Dyestuff2dot}
\begin{figure}[tbp]
  \centering
<<Dyestuff2dot,fig=TRUE,echo=FALSE,height=3.5>>=
print(dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff2,
              ylab = "Batch", jitter.y = TRUE, pch = 21,
              xlab = "Simulated response (dimensionless)",
              type = c("p", "a")))
@   
\caption[Simulated data similar in structure to the \code{Dyestuff}
data]{Simulated data presented in
  \citet{box73:_bayes_infer_statis_analy} with a structure similar to
  that of the \code{Dyestuff} data.  These data represent a case where
  the batch-to-batch variability is small relative to the within-batch
  variability.}
  \label{fig:Dyestuff2dot}
\end{figure}
the batch-to-batch variability in these data is small compared to the
within-batch variability.  In some approaches to mixed models it can
be difficult to fit models to such data.  Paradoxically, small
``variance components'' can be more difficult to estimate than large
variance components.

The methods we will present are not compromised when estimating small
variance components.

\section{Fitting linear mixed models}
\label{sec:FittingLMMs}

Before we formally define a linear mixed model, let's go ahead
and fit models to these data sets
%I know that I find it easier
%to understand definitions, equations and derivations when I have one
%or more examples to which I can relate the mathematics. I expect
%this is true for many others, too.
%
using \code{lmer}.  Like most model-fitting functions in \R{},
\code{lmer} takes, as its first two arguments, a \emph{formula}
specifying the model and the \emph{data} with which to evaluate the
formula.  This second argument, \code{data}, is optional but
recommended.  It is usually the name of a data frame, such as those we
examined in the last section. Throughout this book all model
specifications will be given in this formula/data format.

We will explain the structure of the formula after we have
considered an example.
\subsection{A model for the \texttt{Dyestuff} data}
\label{sec:DyestuffLMM}

We fit a model to the \code{Dyestuff} data allowing for an overall
level of the \code{Yield} and for an additive random effect for each
level of \code{Batch}
<<fm1>>=
fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff)
print(fm1)
@ 
In the first line we call the \code{lmer} function to fit a model with formula
<<fm1formula,echo=FALSE>>=
Yield ~ 1 + (1|Batch)
@
applied to the \code{Dyestuff} data and assign the result to the
name \code{fm1}.  (The name is arbitrary.  I happen to use names that
start with \code{fm}, indicating ``fitted model''.)

As is customary in \R{}, there is no output shown after this
assignment.  We have simply saved the fitted model as an object named
\code{fm1}.  In the second line we display some information about the
fitted model by applying \code{print} to \code{fm1}.  In later
examples we will condense these two steps into one but here it helps
to emphasize that we save the result of fitting a model then apply
various \emph{extractor} functions to the fitted model to get a brief
summary of the model fit or to obtain the values of some of the
estimated quantities.

\subsubsection{Details of the printed display}
\label{sec:printedDetails}

The printed display of a model fit with \code{lmer} has four major
sections: a description of the model that was fit, some statistics
characterizing the model fit, a summary of properties of the random
effects and a summary of the fixed-effects parameter estimates.  We
consider each of these sections in turn.

The description section states that this is a linear mixed model in
which the parameters have been estimated as those that minimize the
REML criterion (explained in section \ref{sec:REML}).  The
\code{formula} and \code{data} arguments are displayed for later
reference.  If other, optional arguments affecting the fit, such as a
\code{subset} specification, were used, they too will be displayed
here.

For models fit by the REML criterion the only statistic describing the
model fit is the value of the REML criterion itself.  An alternative
set of parameter estimates, the maximum likelihood estimates, are
obtained by specifying the optional argument \code{REML = FALSE}.
<<fm1ML>>=
(fm1ML <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, REML = FALSE))
@ 
(Notice that this code fragment also illustrates a way to condense
the assignment and the display of the fitted model into a single
step. The redundant set of parentheses surrounding the assignment
causes the result of the assignment to be displayed. We will use this
device often in what follows.  This code fragment also illustrates, in
the second line, the continuation prompt, \code{"+"}.  When an
incomplete \R{} expression is submitted, the prompt is changed from
\code{">"} to \code{"+"}, indicating that additional input is required
before the expression can be evaluated.)

The display of a model fit by maximum likelihood provides several
other model-fit statistics such as Akaike's Information Criterion
(\code{AIC})~\citep{saka:ishi:kita:1986}, Schwarz's Bayesian
Information Criterion (\code{BIC})~\citep{schw:1978}, the
log-likelihood (\code{logLik}) at the parameter estimates, and the
deviance (negative twice the log-likelihood) at the parameter
estimates.  These are all statistics related to the model fit and
are used to compare different models fit to the same data.

At this point the important thing to note is that the default
estimation criterion is the REML criterion.  Generally the REML
estimates of variance components are preferred to the ML estimates.
However, when comparing models it is safest to refit all the models
using the maximum likelihood criterion.  We will discuss comparisons
of model fits later in section~\ref{sec:compmod}.

The third section is the table of estimates of parameters associated
with the random effects.  There are two sources of variability in the
model we have fit, a batch-to-batch variability in the level of the
response and the residual or per-observation variability --- also called
the within-batch variability.  The name ``residual'' is used in
statistical modeling to denote the part of the variability that cannot be
explained or modeled with the other terms.  It is the variation in the
observed data that is ``left over'' after we have determined the
estimates of the parameters in the other parts of the model.

Some of the variability in the response is associated with the
fixed-effects terms.  In this model there is only one such term,
labeled as the \code{(Intercept)}.  The name ``intercept'', which is
better suited to models based on straight lines written in a
slope/intercept form, should be understood to represent an overall
``typical'' or mean level of the response in this case.  (In case you
are wondering about the parentheses around the name, they are included
so that you can't accidentally create a variable with a name that
conflicts with this name.)  The line labeled \code{Batch} in the
random effects table shows that the random effects added to the
\code{(Intercept)} term, one for each level of the \code{Batch}
factor, are modeled as random variables whose unconditional variance
is estimated as \Sexpr{sprintf("%.2f", unlist(VarCorr(fm1)))} gm.$^2$
in the REML fit and as
\Sexpr{sprintf("%.2f", unlist(VarCorr(fm1ML)))} gm.$^2$
in the ML fit.  The corresponding standard deviations
are \Sexpr{sprintf("%.2f", sqrt(VarCorr(fm1)[["Batch"]][1,1]))} gm. for the
REML fit and \Sexpr{sprintf("%.2f", sqrt(VarCorr(fm1ML)[["Batch"]][1,1]))}
gm. for the ML fit.

Note that the last column in the random effects summary table is the
estimate of the variability expressed as a standard deviation rather
than as a variance.  These are provided because it is usually easier
to visualize standard deviations, which are on the scale of the
response, than it is to visualize the magnitude of a variance.  The
values in this column are a simple re-expression (the square root) of
the estimated variances. Do not confuse them with the standard errors
of the variance estimators, which are not given here.  In
section~\ref{sec:MCMC} we explain why we do not provide standard
errors of variance estimates.

The line labeled \code{Residual} in this table gives the estimate of
the variance of the residuals (also in gm.$^2$) and its corresponding
standard deviation.  For the REML fit the estimated standard deviation
of the residuals is \Sexpr{round(attr(VarCorr(fm1), "sc"), 2)} gm. and
for the ML fit it is also \Sexpr{round(attr(VarCorr(fm1ML), "sc"), 2)}
gm.  (Generally these estimates do not need to be equal.  They happen
to be equal in this case because of the simple model form and the
balanced data set.)

The last line in the random effects table states the number of
observations to which the model was fit and the number of levels of
any ``grouping factors'' for the random effects.  In this case we have
a single random effects term, \code{(1|Batch)}, in the model formula
and the grouping factor for that term is \code{Batch}.  There will be
a total of six random effects, one for each level of \code{Batch}.

The final part of the printed display gives the estimates and standard
errors  of any fixed-effects parameters in the model.  The only
fixed-effects term in the model formula is the \code{1}, denoting a
constant which, as explained above, is labeled as \code{(Intercept)}.
For both the REML and the ML estimation criterion the estimate of this
parameter is \Sexpr{round(fixef(fm1)[1], 2)} gm. (equality is again a
consequence of the simple model and balanced data set).  The standard
error of the intercept estimate is
\Sexpr{round(coef(summary(fm1))[1,2], 2)} gm. for the REML fit and
\Sexpr{round(coef(summary(fm1ML))[,2], 2)} gm. for the ML fit.

\subsection{Assessing variability of the parameter estimates}
\label{sec:variability}

The mixed-effects model fit as \code{fm1} or \code{fm1ML} has three
parameters for which we obtained estimates.  These parameters are
$\sigma_1$, the standard deviation of the random effects, $\sigma$,
the standard deviation of the residual or ``per-observation'' noise
term and $\beta_0$ the fixed-effects parameter that is labelled as the
intercept.  A standard error for the estimate $\widehat{\beta_0}$ is
provided but no standard errors are given for the estimates
$\widehat{\sigma_1}$ and $\widehat{\sigma}$.

The reason that we do not simply quote standard errors for the
estimated standard deviations is because they are misleading.  Even
more misleading is the common practice of quoting a standard error for
an estimated variance.  Quoting a parameter estimate and a standard
error for the estimator gives the impression that we can summarize the
precision of the estimate with a series of confidence intervals that
are symmetric about the estimate.  In other words we are assuming that
decreasing a parameter from its estimate by an amount $\delta$ has the
same effect on the quality of the model fit as does increasing the
parameter by $\delta$.  It is not realistic to expect that this is
true of measures of variability, such as standard deviations and variances.

Instead of making the unrealistic assumption that the precision of
these parameter estimates can be summarized by standard errors we
evaluate the quality of the fit for nearby values of the parameter and
form confidence intervals based on the change in the deviance.  This
is called \emph{profiling} the deviance with respect to a parameter.
We plot the profiles, obtained by applying the function \code{profile}
to the fitted model, on the scale of the square root of the change in
the deviance, as shown in Figure~\ref{fig:fm1absprof}.
\begin{figure}[tb]
  \centering
<<fm1absprof,fig=TRUE,echo=FALSE,height=2.5>>=
pr1 <- profile(fm1ML, delta = 0.2)
print(xyplot(pr1, aspect = 0.7, strip=FALSE, strip.left = TRUE))
@   
\caption{Profiled deviance, on the scale $|\zeta|$, the square root of
  the change in the deviance, for each of the parameters in model
  \code{fm1ML}.  The intervals shown are 50\%, 80\%, 90\%, 95\% and
  99\% confidence intervals based on the profile likelihood.}
  \label{fig:fm1absprof}
\end{figure}

Because we will use such plots of the profiled deviance extensively
for assessing the uncertainty in parameter estimates, we will describe
this particular plot qualitatively, but in some detail, here.  Formal
definitions and equations will be given in the next section.

Recall that one of the measures of the quality of a model fit is the
\emph{deviance}, which is negative twice the log-likelihood evaluated
at the parameter estimates.  The maximum likelihood estimates are the
parameter values that minimize the deviance,
$d(\sigma_1,\sigma,\beta_0)$, over all feasible values of the
parameters.

To perform a hypothesis test of the form $H_0:\sigma_1=80$ versus
$H_a:\sigma_1\ne 80$ we could consider the minimum deviance achieved
when $\sigma_1=80$ minus the global minimum deviance.  This difference,
\begin{displaymath}
  \min_{\sigma,\beta_0}d(80,\sigma,\beta_0)-
  d(\widehat{\sigma}_1, \widehat{\sigma}, \widehat{\beta}_0),
\end{displaymath}
is the likelihood ratio test statistic. (The deviance is a multiple of
the logarithm of the likelihood so ratios of likelihoods correspond to
differences in the deviance.)  This test statistic explicitly compares
how well we can fit the model for this specific value of one of the
parameters to the best possible fit of the model.  Note that this
quantity is based on two model fits: the unconstrained fit and a
second fit subject to the constraint $\sigma_1=80$.   We are not
basing our test on how well we think we should be able to fit the
constrained model --- based on the global parameter estimates and
approximate standard errors --- we are actually re-fitting the model
subject to the constraint.

The theory of likelihood ratio tests states that the test statistic
would be compared to the quantiles of the $\chi_k^2$ distribution
where $k$ is the number of constraints, which is 1 in this case.  But
a $\chi_1^2$ distribution is simply the square of the standard
normal distribution, $\mathcal{N}(0,1)$, so we plot its square root,
which we denote as
\begin{equation}
  \label{eq:abszetadef}
  |\zeta(\sigma_1)|=\sqrt{
  \min_{\sigma,\beta_0}d(\sigma_1,\sigma,\beta_0)-
  d(\widehat{\sigma}_1, \widehat{\sigma}, \widehat{\beta}_0)},
\end{equation}
with similar definitions for the other parameters.  (The perceptive
reader will have notice that the second profile plot is on the scale
of $\log(\sigma)$, not $\sigma$.  We will discuss this choice of scale
later.)

The intervals shown on the plots are 50\%, 80\%, 90\%, 95\% and 99\%
confidence intervals based on this likelihood ratio test.  That is,
the bounds on the 95\% confidence intervals are the points where
$|\zeta|=1.960$.  We use the \R{} function \code{profile} to create the
profiled deviance object and \code{xyplot} to display the plot.
<<pr1,eval=FALSE>>=
pr1 <- profile(fm1ML)
@ 
The
\code{confint} extractor is used to obtain the endpoints of a
particular confidence interval.
<<confint1>>=
confint(pr1)
@ 

Often the \emph{signed square root} provides a more meaningful scale
on which to plot the likelihood ratio test statistic, as in
Figure~\ref{fig:fm1prof}.
\begin{figure}[tb]
  \centering
<<fm1prof,fig=TRUE,echo=FALSE,height=3.5>>=
print(xyplot(pr1, aspect = 1.3, absVal = FALSE))
@   
  \caption{Signed square root, $\zeta$, of the likelihood ratio test
    statistic for each of the parameters in model \code{fm1ML}.  The
    vertical lines are the endpoints of 50\%, 80\%, 90\%, 95\% and 99\%
    confidence intervals derived from this test statistic.}
  \label{fig:fm1prof}
\end{figure}
The signed square root assigns a negative sign to $\zeta$ for
parameter values less than the estimate and a positive sign for
parameter values greater than the estimate providing a curve whose
interpretation is similar to that of a normal probability plot of data
or residuals.

\subsubsection{Interpretation of the shape of the $\zeta$ plot}

We will use $\phi$ to represent a parameter or a transformation of a
parameter in the model.  For the model we are considering $\phi$ can
be $\sigma_1$ or $\sigma_1^2$ or $\log(\sigma)$ or $\sigma$ or
$\sigma^2$ or $\beta_0$ or one of many other possibilities.  In a plot
of $\zeta(\phi)$ versus $\phi$ the estimate, $\widehat{\phi}$, is the
point where $\zeta = 0$ and the slope of the curve at $\widehat{\phi}$
is the inverse of the standard error of $\phi$.  There is more than
one way of calculating a standard error of a parameter estimate and in
chapter~\ref{cha:Theory} we will clarify that this standard error is
based on a quantity called ``the observed information matrix''.  That
level of detail is not necessary here.  All we need to know is that
the inverse slope, written $s_\phi$, is that standard error of the
parameter estimate, which is a measure of its precision.

If the plot of $\zeta$ versus $\phi$ is close to a straight line over
the region of interest then our $(1-\alpha)$ confidence interval on
$\phi$ will be close to
\begin{equation}
  \label{eq:confint}
  \widehat{\phi}\pm z_{\alpha/2}s_{\phi} 
\end{equation}
where $z_{\alpha/2}$ is the $\alpha/2$ upper quantile of the standard
normal (also called Gaussian) distribution.  For example,
$z_{0.025}=1.960$ so a 95\% confidence interval on the parameter
$\phi$ would be calculated as
\begin{equation}
  \label{eq:confint95}
  \widehat{\phi}\pm 1.960 s_{\phi} 
\end{equation}
or, effectively, the estimate plus-or-minus two standard errors of the
estimate.

We should realize that confidence intervals of the form
(\ref{eq:confint}) are based on an approximation.  Similarly,
performing a hypothesis test of the form $H_0:\phi = \phi_0$
\textrm{vs.} $H_a:\phi\ne\phi_0$ by calculating the observed
``z-statistic''
\begin{equation}
  \label{eq:zobs}
  z_{\text{obs}}=\frac{\widehat{\phi} - \phi_0}{s_\phi}
\end{equation}
and evaluating a p-value as
$2\cdot\mathrm{P}[\mathcal{Z}>|z_{\text{obs}}|]$ is also an
approximation.

Of course, we don't need to perform hypothesis tests of the form
$H_0:\phi=\phi_0$ \emph{vs.} $H_a:\phi\ne\phi_0$ in this way
because can fit the model without any constraints on the parameters
then fit the model subject to the constraint $\phi=\phi_0$ and compare
the two model fits using the likelihood ratio.  Furthermore the
confidence intervals we obtain are based on ``inverting'' such
hypothesis tests.  Because we are fitting the model under both
conditions and comparing the quality of the fits we obtain more
realistic information regarding whether the parameter value $\phi_0$
is reasonable.  We do not need to resort to fitting the model once
only and keeping our fingers crossed while using approximations to
perform statistical inference.

This situation is unfortunately characteristic of modern statistics.
The well-known statistical methods shown in many texts are often based
on unnecessary approximations.  We have the ability to fit models many
times subject to different constraints on the parameters so we can
actually examine the effective variability in the parameter estimates
rather than resorting to approximations.

Returning to consideration of confidence intervals of the form
(\ref{eq:confint}) and hypothesis tests based on (\ref{eq:zobs}),
these will be suitable methods when the profile plot (the plot of
$\zeta$ versus $\phi$) is reasonably straight. In those cases, of
course, the methods based on $\zeta$ will produce confidence intervals
or p-values for hypothesis tests of the expected form.  The important
cases are when the profile plot is not reasonably straight over the
region of interest.  In those cases the confidence intervals are not
symmetric or not determined from quantiles of the standard normal nor
are p-values based on the standard normal distribution.

As shown above, we can create confidence intervals on parameters or
transformed parameter values using the profile and the \code{confint}
extractor function so we don't really need to examine the profile
plot.  However, the profile plots do provide us with a valuable visual
evaluation of the precision of the parameter estimates and we will
use them frequently.

We can read a profile plot similar to the way that we read a
quantile-quantile plot, such as a normal probability plot, of observed
data or of residuals from a fitted model.  That is, we evaluate a
profile plot for skewness and for over-dispersion.  We will
speak of these properties as applying to the distribution of the
parameter estimator.  Technically that is not quite correct (the
profile plot indicates the sensitivity of the model fit to the
parameter values) but I find it a convenient way of discussing
patterns in the profile plots.

With this in mind we can examine the patterns in
Figure~\ref{fig:fm1prof}.  The profile plot for $\log(\sigma)$ is
quite straight indicating that the local approximation based on a
standard error is quite good.  Notice, however, that this pattern is
with respect to $\log(\sigma)$, not $\sigma$ or, even worse, $\sigma^2$
see that the sigmoidal (i.e. like an
elongated ``S'' curve) pattern for the profile plot of $\beta_0$ in Figure~\ref{fig:fm1prof} the
parameter $\beta_0$ is very close to symmetric about the estimate
$\widehat{\beta_0}$ but over-dispersed relative to a normal distribution.

\subsubsection{Precision of the variance components}
\label{sec:precvarcomp}

When we summarize our knowledge of a parameter in a fitted model by
giving its estimate and a standard error of this estimate we are
assuming that the quality of fit for different values of this
parameter, as measured by the signed square root, $\eta$, say, will be
symmetric about the estimate.  Furthermore, if we are to use a z
test'' or confidence intervals based on quantiles of the standard
normal, then we are assuming that the plot of $\zeta$ will be
reasonably close to a straight line.  In that case the standard error
of the parameter corresponds to the inverse of the slope of the line.

We can see that the only panel in Figure~\ref{fig:fm1prof} that is at
all close to a straight line is the middle panel, which shows $\zeta$
versus $\log(\sigma)$, not $\zeta$ versus $\sigma$ or, even worse,
$\zeta$ versus $\sigma^2$, as shown in Figure~\ref{fig:sigmaprof}.
\begin{figure}[tb]
  \centering
<<sigmaprof,fig=TRUE,echo=FALSE>>=
zeta <- sqrt(qchisq(c(0.5,0.8,0.9,0.95,0.99), 1))
zeta <- c(-rev(zeta), 0, zeta)
spl <- attr(pr1, "forward")[[2]]
endpts <- predict(attr(pr1, "backward")[[2]], zeta)$y

fr <- data.frame(zeta = rep.int(zeta, 2),
                 endpts = c(exp(endpts), exp(2*endpts)),
                 pnm = gl(2, length(zeta)))
levels(fr$pnm) <- c(expression(sigma), expression(sigma^2))
print(xyplot(zeta ~ endpts|pnm, fr, type = "h",
             scales = list(x = list(relation = "free")),
             xlab = NULL, ylab = expression(zeta), aspect = 1.3,
             strip = strip.custom(
             factor.levels = expression(sigma, sigma^2)),
             panel = function(...) {
                 panel.grid(h = -1, v = -1)
                 panel.xyplot(...)
                 lims <- log(current.panel.limits()$xlim)
                 if (panel.number() == 2) lims <- lims/2
                 pr <- predict(spl, seq(lims[1], lims[2], len = 101))
                 panel.lines(exp(pr$x * panel.number()), pr$y)
             }))
@   
  \caption{Signed square root, $\zeta$, of the likelihood ratio test statistic as a function of $\sigma$ and as a function of $\sigma^2$.   The vertical lines are the endpoints of 50\%, 80\%, 90\%, 95\% and 99\% confidence intervals.}
\end{figure}

\section{The linear mixed-effects probability model}
\label{sec:Probability}

In explaining some of parameter estimates related to the random
effects we have used terms such as ``unconditional distribution''
from the theory of probability.  Before proceeding further we should
clarify the linear mixed-effects probability model.  In the first part
of this section we define several terms and concepts that will be used
throughout the book.  I would recommend that subsection to all
readers.  

\subsection{Definitions of the random variables}
\label{sec:defnRV}

As mentioned in section~\ref{sec:memod}, the mixed-effects probability
model is based on two vector-valued random variables: the
$q$-dimensional vector of random effects, $\bc B$, and the
$n$-dimensional response vector, $\bc Y$.  In our model for the
\code{Dyestuff} data the dimension of the response vector is $n=30$
and the dimension of the random effects vector is $q=6$.  

In all the forms of mixed models that we will consider, the random effects
vector has a multivariate normal (also called Gaussian) distribution
with mean vector $\bm 0$ and with a parameterized, $q\times q$,
symmetric, variance-covariance matrix that we will write as
$\bm\Sigma_\theta$.  The notation indicates that this symmetric
$q\times q$ matrix $\bm\Sigma$ depends on a parameter vector that is
written as $\bm\theta$.  Even though some of the elements of
$\bm\theta$ may determine covariances and not variances, we shall
refer to them collectively as the \emph{variance-component
  parameters}.

We write this distribution as
\begin{equation}
  \label{eq:unconditionalB}
  \bc{B}\sim\mathcal{N}(\bm 0,\bm\Sigma_\theta) .
\end{equation}
Because this distribution does not depend on the value, $\bm y$, of
the random variable, $\bc Y$, we say it is the \emph{unconditional}
distribution of $\bc B$.

The probability model for the response, $\bc Y$, is most easily
described by stating the conditional distribution $(\bc Y|\bc B=\bm
b)$, which is the distribution of the response vector, $\bc Y$,
assuming that the value of the random effects vector, $\bc B$, is
known to be $\bm b$.  (In practice we never know the value $\bm b$
but, for the purpose of formulating the model, we will assume that we
do.)  For all the forms of mixed models that we will consider, $\bm b$
changes the conditional distribution of $\bc Y$ by changing the
conditional mean, $\bm\mu_{\bc Y|\bc B=\bm b}$.  Furthermore, the
conditional mean response depends on $\bm b$ and on the fixed-effects
parameter vector $\bm\beta$ only through the \emph{linear predictor}
\begin{equation}
  \label{eq:linpred}
  \bm Z\bm b+\bm X\bm\beta
\end{equation}
where $\bm Z$ and $\bm X$ are known \emph{model matrices} of
the appropriate size.

This description applies to all forms of mixed models considered in
this book.  In the case of a linear mixed model we can be more
specific about the conditional mean and the conditional distribution.
For linear mixed models the conditional mean, $\bm\mu_{\bc Y|\bc B=\bm
  b}$, is exactly the linear predictor, and the conditional
distribution is a ``spherical'' Gaussian distribution.  That is
\begin{equation}
  \label{eq:condmeanY}
  \bm\mu_{\bc Y|\bc B=\bm b}=\bm Z\bm b+\bm X\bm\beta
\end{equation}
and
\begin{equation}
  \label{eq:LMMconddist}
  (\bc Y|\bc B=\bm b)\sim\mathcal{N}\left(\bm Z\bm b+\bm
    X\bm\beta,\sigma^2\bm I_n\right) .
\end{equation}

The expression $\bm I_n$ denotes the identity matrix of size $n$.
This is the $n\times n$ matrix whose diagonal elements are all unity
and whose off-diagonal elements are all zero.  The parameter
$\sigma$ is called the \emph{common scale parameter}.  Its square is the
variance of the residual ``noise'' terms that cannot be explained by
other parts of the model.  The name ``spherical'' is applied to
Gaussian distributions of the form (\ref{eq:LMMconddist}) because the
contours of constant probability density are spheres centered at $\bm
Z\bm b+\bm X\bm\beta$ in the $n$-dimensional response space

Because the conditional mean must be an $n$-dimensional vector, the
model matrix $\bm Z$ must be $n\times q$ and the model matrix $\bm X$
must be $n\times p$, where $p$ is the dimension of the fixed-effects
parameter vector, $\bm\beta$.  For the model fit to the
\code{Dyestuff} data, $p=1$ and the matrix $\bm X$ is a $30\times 1$
matrix, all of whose elements are unity.  The fixed-effects term
\code{1} in a model formula generates a column of ones in the
fixed-effects model matrix, $\bm X$.  For the model being considered
here, this column of ones is the only column in $\bm X$.

The form of the random-effects model matrix, $\bm Z$, and the form of
the variance-covariance matrix, $\bm\Sigma_\theta$, and the method by which
$\bm\Sigma_\theta$ is determined from the value of $\bm\theta$ are all based
on the random-effects terms in the model formula.  As stated earlier,
there is one random effects term, \code{(1 | Batch)}, in the formula
for this model.  Random-effects terms are those that contain the
vertical bar, \code{"|"}, character.  The \code{Batch} variable is the
grouping factor for the random effects described by this term.  An
expression for the grouping factor, usually just the name of a
variable, occurs to the right of the vertical bar.  If the expression
on the left of the vertical bar is \code{1}, as it is here, we
describe the term as a \emph{simple, scalar, random-effects term}.
The designation ``scalar'' means there will be exactly one random
effect generated for each level of the grouping factor.  A simple,
scalar term generates a block of indicator columns --- the indicators for
the grouping factor --- in $\bm Z$.  Because there is only one
random-effects term in this model and because that term is a simple,
scalar term, the model matrix $\bm Z$ for this model is the indicator
matrix for the levels of \code{Batch}.

The transpose of this matrix, of dimension $6\times 30$, is stored as
a sparse matrix in the environment of the fitted model as \code{Zt}.
<<fm1Zt>>=
env(fm1)$Zt
@ 
A sparse matrix is one in which most of the elements are known to be
zero.  These elements are represented by a \code{"."} in this display.
The nature of an indicator matrix is such that only one element in
each row of the indicator matrix, corresponding to a column of the
transpose of the indicator matrix that is shown above, is non-zero.
Sparse matrix
methods~\citep{davis06:_direc_method_for_spars_linear_system} for
numerical linear algebra provide special techniques for storing and
manipulating such matrices.  These techniques are the basis of the
numerical methods implemented in \code{lmer}.

Often we will show the structure of sparse matrices as an image like
Figure~\ref{fig:fm1Ztimage}.
\begin{figure}[tbp]
  \centering
<<fm1Ztimage,fig=TRUE,echo=FALSE,height=2.5>>=
print(image(env(fm1)$Zt, sub=NULL))
@   
  \caption[Image of the random-effects model matrix for
  \texttt{fm1}]{Image of the transpose of the random-effects model
    matrix, $\bm Z$, for model \code{fm1}.  The non-zero elements,
    which are all unity, are shown as darkened squares.  The zero
    elements are blank.}
  \label{fig:fm1Ztimage}
\end{figure}
Especially for large matrices, the image of the sparse matrix conveys
its structure more compactly than does the printed representation.

When the model contains only one random-effects term and that term is
a simple, scalar term, then the variance-covariance matrix
$\bm\Sigma_\theta$ is a non-negative multiple of $\bm I_q$, the
$q\times q$ identity matrix.  Although we will defer until later the
discussion of the exact form of $\bm\Sigma_\theta$ for other
models, we will now introduce a transformation of
$\bm\Sigma_\theta$ that we will use throughout.

\subsubsection{The relative covariance factor}
\label{sec:relcovfac}

A variance-covariance matrix like $\bm\Sigma_\theta$ is required to be
symmetric and, in addition, to be \emph{positive semi-definite}, which
means, in effect, that $\bm\Sigma_\theta$ has the matrix equivalent of a
``square root''.  Recall that a scalar variance, such as $\sigma^2$ in
(\ref{eq:LMMconddist}), must be non-negative and that the standard
deviation, $\sigma\ge0$, is the square root of the variance.  A
variance-covariance matrix has a similar property.  There must be a
matrix, say $\bm\Lambda_\theta$, such that when it is multiplied
by its transpose it produces the original matrix,
$\bm\Sigma_\theta$.  This is not quite like ``squaring'' the
matrix because the factor, $\bm\Lambda_\theta$, is multiplied by
its transpose, so as to create a product that is symmetric, as
$\bm\Sigma_\theta$ must be.

It turns out that we can simplify many subsequent formulas if we
define $\bm\Lambda_\theta$ to be a multiple of the square-root
factor, which we call the \emph{relative covariance factor}, defined
to satisfy
\begin{equation}
  \label{eq:relcovfac}
  \bm\Sigma_\theta=\sigma^2\bm\Lambda_\theta
  \bm\Lambda_\theta\trans .
\end{equation}
The symbol $\trans$ denotes the transpose of a matrix. The scalar
$\sigma^2$ is the square of the common scale parameter introduced in
(\ref{eq:LMMconddist}).

The term ``relative'' indicates that $\bm\Lambda_\theta$ is the factor
of the variance, $\bm\Sigma_\theta$, of $\bc B$ relative to the
variance, $\sigma^2$, in the conditional distribution $(\bc Y|\bc
B=\bm b)$.  It happens that the way that we generate
$\bm\Lambda_\theta$ it ends up being lower triangular and corresponds
to the left Cholesky factor of $\bm\Sigma_\theta/\sigma^2$.  A left
Cholesky factor is often written $\bm L$; because this matrix contains
parameter values we denote it with the corresponding Greek letter,
$\bm\Lambda$.

For the \code{Dyestuff} model, which has only one simple, scalar
random-effects term, $\bm\Sigma_\theta$ and
$\bm\Lambda_\theta$ are both multiples of $\bm I_6$, the identity
matrix of size six.  Furthermore, $\bm\theta$ is one-dimensional so we
will write it as $\theta$.  We set
\begin{equation}
  \label{eq:lambdatheta}
  \bm\Lambda_\theta = \theta\bm I_6
\end{equation}
subject to the constraint that $\theta\ge0$.


The latter part of this section covers
some of the finicky but important details of the matrix representation
of the model and the computational methods employed.  Readers who
would prefer to avoid exposure to such details should feel free to do
so.

\subsection{A penalized least squares problem}
\label{sec:PLS}

The general form of a linear mixed model incorporates two parameter
vectors, $\bm\beta$ and $\bm\theta$, and one scalar parameter,
$\sigma^2$.  To evaluate the maximum likelihood estimates,
$\widehat{\bm\beta}$, $\widehat{\bm\theta}$ and $\widehat{\sigma^2}$,
we could attempt to optimize the likelihood of the parameters, given
the model and the observed data, with respect to all of these
parameters simultaneously.  However, for this particular model we can
simplify the optimization problem because, for any value of
$\bm\theta$, we can determine the conditional estimates,
$\widehat{\bm\beta}_\theta$ and $\widehat{\sigma^2}_\theta$,
of the other parameters in a direct computation.

Furthermore, this computation, determining the solution of the penalized
linear least squares problem
\begin{equation}
  \label{eq:PLS}
  \min_{\bm u,\bm\beta}
  \left\|
    \begin{bmatrix}\bm y\\\bm 0\end{bmatrix} -
    \begin{bmatrix}
      \bm Z\bm\Lambda_\theta & \bm X\\
      \bm I_q & \bm 0
    \end{bmatrix}
    \begin{bmatrix}\bm u\\\bm\beta\end{bmatrix}
  \right\|^2 =
  \min_{\bm u,\bm\beta}\left(
    \left\|\bm y - \bm Z\bm\Lambda_\theta\bm u -
      \bm X\bm\beta\right\|^2 + \|\bm u\|^2\right)
\end{equation}
as the vectors, $\widehat{\bm\beta}_\theta$ and $\tilde{\bm u}_\theta$,
that satisfy
\begin{equation}
  \label{eq:plsSol}
  \begin{bmatrix}
    \bm\Lambda_\theta\trans\bm Z\trans\bm
    Z\bm\Lambda_\theta+\bm I_q&\bm\Lambda_\theta\trans\bm
    Z\trans\bm X\\
    \bm X\trans\bm Z\bm\Lambda_\theta&\bm X\trans\bm X
  \end{bmatrix}
  \begin{bmatrix}\tilde{\bm u}\\\widehat{\bm\beta}_\theta\end{bmatrix}
  =
  \begin{bmatrix}
    \bm\Lambda_\theta\trans\bm Z\trans\bm y\\\bm X\trans\bm y
  \end{bmatrix} ,
\end{equation}
provides, as we have seen, the conditional mean, $\bm\mu_{\bc B|\bm
  Y=\bm y}$, and conditional variance of the random effects, $\bc B$,
given the observed data, $\bc Y=\bm y$, plus a concise expression for
the log-likelihood and the REML criterion.

An effective way of determining the solution to (\ref{eq:plsSol}) is
to create the Cholesky decomposition of the matrix on the left.  In
considering this step, however, we must bear in mind that the matrix
$\bm Z$ is quite sparse and, in some cases, can be very large.  I have
myself fit linear mixed models in which both the number of rows and
the number of columns in $\bm Z$ exceeded one million.  

Dealing with matrices of this size requires considerable care in
deciding how to structure the computation.  The key to fitting mixed
models with a complex structure to very large data sets using the
\package{lme4} package is the ability to determine the sparse Cholesky
factor of $\bm\Lambda_\theta\trans\bm Z\trans\bm
Z\bm\Lambda_\theta+\bm I_q$ for many different values of
$\bm\theta$.

The Cholesky decomposition of large, sparse matrices like this has the
interesting characteristic that reordering the rows and columns of
the matrix can change, sometimes dramatically, the number of nonzero
elements in the factor. Decreasing the number of nonzeros in the
factor will increase the speed of the decomposition, which is
important when this needs to be done for many values of $\bm\theta$.
We will write a \emph{fill-reducing permutation}, determined in this
case by the Approximate Minimal Degree algorithm~\citep{Davis:1996},
as a $q\times q$ permutation matrix, $\bm P$.  (In practice it is only
the permutation, which is a reordering of the numbers $1$ to $q$, that
is created and stored but it is convenient to represent this
permutation as a matrix in the formulas.)  The permutation depends
only on the positions of the nonzeros in the matrix $\bm
Z\bm\Lambda(\bm\theta_0)$ where $\bm\theta_0$ is an initial estimate
or ``starting value'' for the parameter $\bm\theta$.

To allow for later generalizations of the linear mixed model we write
the decomposition as
\begin{equation}
  \label{eq:bigChol2}
  \begin{bmatrix}
    \bm L_\theta            & \bm 0\\
    \bm R_{ZX}\trans  & \bm R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L_\theta\trans   & \bm R_{ZX}\\
    \bm 0         & \bm R_X
  \end{bmatrix} =
  \begin{bmatrix}
    \bm U_\theta\trans\bm U_\theta+\bm I_q & \bm U_\theta\trans\bm V\\
    \bm V\trans\bm U_\theta         & \bm V\trans\bm V
  \end{bmatrix}
\end{equation}
where
\begin{equation}
  \label{eq:UV}
  \bm U_\theta=\bm Z\bm\Lambda_\theta\text{ and }
  \bm V=\bm X .
\end{equation}
The solution to (\ref{eq:plsSol}), which would now be written as,
\begin{equation}
  \label{eq:UVplsSol}
  \begin{bmatrix}
    \bm P\trans\bm L_\theta      & \bm 0\\
    \bm R_{ZX}\trans  & \bm R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L_\theta\trans\bm P   & \bm R_{ZX}\\
    \bm 0         & \bm R_X
  \end{bmatrix} =
  \begin{bmatrix}\tilde{\bm u}_\theta\\\widehat{\bm\beta}_\theta\end{bmatrix}
  =
  \begin{bmatrix}
    \bm U_\theta\trans\bm y\\\bm V\trans\bm y
  \end{bmatrix} ,
\end{equation}
produces both the conditional estimate,
$\widehat{\bm\beta}_\theta$, of the fixed-effects parameters and
the conditional mean $\bm\mu_{\bc B|\bc Y=\bm y}=\bm\Lambda_\theta\tilde{\bm
  u}$, of the random effects.  If $r^2_\theta$ is the minimum penalized
residual sum of squares,
\begin{equation}
  \label{eq:minpls}
  r^2_\theta=\|\bm y-\bm U_\theta\tilde{\bm u}_\theta-
  \bm V\widehat{\bm\beta}_\theta\|^2+\|\tilde{\bm u}_\theta\|^2
\end{equation}
then the conditional maximum likelihood estimate of $\sigma^2$ is
$\widehat{\sigma^2}_\theta=\frac{r^2}{n}$ and the conditional REML
estimate of $\sigma^2$ is
$\widehat{\sigma^2}_R(\theta)=\frac{r^2}{n-p}$.

Although we have not written this explicitly, the matrices $\bm L_\theta$,
$\bm R_{ZX}$ and $\bm R_X$ all depend on the value of $\bm\theta$.
Both the $q\times q$ matrix $\bm L_\theta$ and the $p\times p$
matrix $\bm R_X(\theta)$ are triangular: $\bm L_\theta$ is
lower triangular and $\bm R_X(\theta)$ is upper triangular.
Because these matrices are square and triangular their determinants,
written $|\bm L|$ and $|\bm R_X|$, are easily evaluated as the product
of the diagonal elements in the matrix.  The determinant of a matrix
is the measure of the size of the matrix that is used in evaluating
probability densities of random variables after transformation.
