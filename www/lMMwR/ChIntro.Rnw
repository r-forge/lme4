\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=8,strip.white=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Intro,include=TRUE}
\setkeys{Gin}{width=\textwidth}

<<preliminaries,echo=FALSE,print=FALSE,results=hide>>=
options(width=65, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
library(lattice)
library(Matrix)
library(lme4a)
fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff)
fm1ML <- update(fm1, REML = FALSE)
VCREML <- VarCorr(fm1)
VCML <- VarCorr(fm1ML)
gmin <- function(x, y)
{
    if (missing(x))
        x <- seq_along(y)
    mpos <- which.min(y)
    if (mpos %in% c(1L, length(y))) return(c(x[mpos], y[mpos]))
    conv <- array(c(0,-1,1,2,0,-2,0,1,1), c(3,3))/2
    ycoef <- conv %*% y[mpos + -1:1]
    pos <- -ycoef[2]/(2*ycoef[3])
    stopifnot(-1 < pos, pos < 1)
    b <- c(1, pos, pos^2)
    c(crossprod(conv %*% x[mpos + -1:1], b), crossprod(ycoef, b))
}
if (file.exists("grd1.rda")) {
    load("grd1.rda")
    sigB <- sort(unique(grd1$sigB))
    sigmas <- sort(unique(grd1$sigma))
    deviance <- array(grd1$deviance + deviance(fm1ML),
                      c(length(sigB), length(sigmas)))
    REML <- array(grd1$REML + deviance(fm1),
                  c(length(sigB), length(sigmas)))
} else {
    sigmaDev <- function(fenv, theta, sigma)
    {
        fenv@setPars(theta)
        sigmasq <- sigma^2
        dc <- devcomp(fenv)
        dev <- dc$cmp
        base <- unname(dev["ldL2"] +  dev["prss"]/sigmasq)
        l2ps <- log(2*pi*sigmasq)
        base + unname(c(dev["ldRX2"],0)) + dc$dims[c("nmp","n")] * l2ps
    }
    sigmaDev1 <- function(fenv, pars)
    {
        pars <- as.numeric(pars)
        stopifnot((lp <- length(pars)) > 1)
        sigmaDev(fenv, pars[-lp]/pars[lp], pars[lp])
    }
    fm1env <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, doFit = FALSE)
    sigB <- seq(0, 225,len = 101)
    sigmas <- seq(30, 100,len = 101)
    grd1 <- expand.grid(sigB = sigB, sigma = sigmas)
    attr(grd1, "out.attrs") <- NULL          # save a bit of space
    vals <- apply(t(grd1[, 1:2]), 2, sigmaDev1, fenv = fm1env)
    grd1$REML <- vals[1,] - deviance(fm1)
    grd1$deviance <- vals[2,] - deviance(fm1ML)
    deviance <- array(vals[2,], c(length(sigB), length(sigmas)))
    REML <- array(vals[1,], c(length(sigB), length(sigmas)))
    save(grd1, file = "grd1.rda")
}
@

\chapter{A simple linear mixed-effects model}
\label{chap:ExamLMM}

In this book we describe the theory behind a type of statistical model
called \emph{mixed-effects} models and the practice of fitting and
analyzing such models using the \package{lme4} package for \R{}.
These models are used extensively in many subject areas.  Because the
descriptions of the models can vary markedly between disciplines, we
begin by describing what mixed-effects models are and by exploring a
very simple example of one type of mixed model, the \emph{linear mixed
  model}.

%% FIXME: Review this paragraph once the chapter is complete
This simple example allows us to illustrate the use of the \code{lmer}
function to fit such models.  Building from the example we describe
the general form of linear mixed models that can be fit using
\code{lmer}.

\section{Mixed-effects models}
\label{sec:memod}

Mixed-effects models, like many other types of statistical models,
describe a relationship between a \emph{response} variable and some of
the \emph{covariates} that have been measured or observed along with
the response.  In mixed-effects models at least one of the covariates
is a \emph{categorical} covariate representing experimental or
observational ``units'' in the data set.  In the example from the
chemical industry that is given in this chapter, the observational
unit is the batch of an intermediate product used in production of a
dye.  In medical and social sciences the observational units are often
the human or animal subjects in the study.  In agriculture the
experimental units may be the plots of land or the specific plants
being studied.

In all of these cases the categorical covariate or covariates are
observed at a set of discrete \emph{levels}.  We may use numbers, such
as subject identifiers, to designate the particular levels that we
observed but these numbers are simply labels.  The important
characteristic of a categorical covariate is that, at each observed
value of the response, the covariate takes on the value of one of a
set of distinct levels.

Parameters associated with the particular levels of a covariate are
sometimes called the ``effects'' of the levels.  If the set of
possible levels of the covariate is fixed and reproducible we model
the covariate using \emph{fixed-effects} parameters.  If the levels
that we observed represent a random sample from the set of all
possible levels we incorporate \emph{random effects} in the model.

There are two things to notice about this distinction between
fixed-effects parameters and random effects.  First, the names are
misleading because the distinction between fixed and random is more a
property of the levels of the categorical covariate than a property of
the effects associated with them.  Secondly, we distinguish between
``fixed-effects parameters'', which are indeed parameters in the
statistical model, and ``random effects'', which, strictly speaking,
are not parameters.  As we will see shortly, random effects are
unobserved random variables.

To make the distinction more concrete, suppose that we wish to model
the annual reading test scores for students in a school district and
that the covariates recorded with the score include a student
identifier and the student's gender. Both of these are categorical
covariates.  The levels of the gender covariate, male and female, are
fixed.  If we consider data from another school district or we
incorporate scores from earlier tests, we will not change those
levels.  On the other hand, the students whose scores we observed
would generally be regarded as a sample from the set of all possible
students whom we could have observed.  Adding more data, either from
more school districts or from results on previous or subsequent tests,
will increase the number of distinct levels of the student identifier.

\emph{Mixed-effects models} or, more simply, \emph{mixed models} are
statistical models that incorporate both fixed-effects parameters and
random effects.  Because of the way that we will define random
effects, a model with random effects always includes at least one
fixed-effects parameter.  Thus, any model with random effects is a
mixed model.

We characterize the statistical model in terms of two random
variables: a $q$-dimensional vector of random effects represented by
the random variable $\mathcal{\bm B}$ and an $n$-dimensional response
vector represented by the random variable $\mathcal{\bm Y}$.  We
observe the value, $\bm y$, of $\mathcal{\bm Y}$.  We do not observe
the value of $\mathcal{\bm B}$.

When formulating the model we describe the unconditional distribution
of $\mathcal{\bm B}$ and the conditional distribution, $(\mathcal{\bm
  Y}|\mathcal{\bm B}=\bm b)$.  The descriptions of the distributions
involve the form of the distribution and the values of certain
parameters.  We use the observed values of the response and the
covariates to estimate these parameters and to make inferences about
them.

That the big picture.  Now let's make this more concrete by describing
a particular, versatile class of mixed models called linear mixed
models and by studying a simple example of such a model.  First we
will describe the data in the example.

\section{The \texttt{Dyestuff} and
  \texttt{Dyestuff2} data}
\label{sec:DyestuffData}

Models with random effects have been in use for a long time.  The
first edition of the classic book, \emph{Statistical Methods in
  Research and Production}, edited by O.L. Davies, was published in
1947 and contained examples of the use of random effects to
characterize batch-to-batch variability in chemical processes.  The
data from one of these examples are available as the \code{Dyestuff}
data in the \package{lme4} package. In this section we describe and
plot these data and introduce a second example, the \code{Dyestuff2}
data, described in \citet{box73:_bayes_infer_statis_analy}.

\subsection{The \texttt{Dyestuff} data}
\label{sec:dyestuff}

The \code{Dyestuff} data are described in \citet[Table~6.3,
p.~131]{davies72:_statis_method_in_resear_and_produc}, the fourth
edition of the book mentioned above, as coming from
\begin{quote}
  an investigation to find out how much the
   variation from batch to batch in the quality of an intermediate
   product (H-acid) contributes to the variation in the yield of the
   dyestuff (Naphthalene Black 12B) made from it.  In the experiment six
   samples of the intermediate, representing different batches of works
   manufacture, were obtained, and five preparations of the dyestuff
   were made in the laboratory from each sample.  The equivalent yield
   of each preparation as grams of standard colour was determined by
   dye-trial.
\end{quote}

To access these data within \R{} we must first attach the \code{lme4}
package to our session using
<<lme4>>=
library(lme4a)
@
Note that the \code{"$>$"} symbol in the line shown is the prompt in
\R{} and not part of what the user types. The \package{lme4} package
must be attached before any of the data sets or functions in the
package can be used.  If typing this line results in an error report
stating that there is no package by this name then you must first
install the package.

In what follows, we will assume that the \package{lme4} package has
been installed and that it has been attached to the \R{} session
before any of the code shown has been run.

The \code{str} function in \R{} provides a concise description of the
structure of the data
<<strDyestuff>>=
str(Dyestuff)
@
from which we see that it consists of $30$ observations of the
\code{Yield}, the response variable, and of the covariate,
\code{Batch}, which is a categorical variable stored as a
\code{factor} object.  If the labels for the factor levels are
arbitrary, as they are here, we will use letters instead of numbers
for the labels.  That is, we label the batches as \code{"A"} through
\code{"F"} rather than \code{"1"} through \code{"6"}.  When the labels are
letters it is clear that the variable is categorical.  When the labels
are numbers a categorical covariate can be mistaken for a numeric
covariate, with unintended consequences.

It is a good practice to apply \code{str} to any data frame the first
time you work with it and to check carefully that any categorical
variables are indeed represented as factors.

The data in a data frame are viewed as a table with columns
corresponding to variables and rows to observations. The functions
\code{head} and \code{tail} print the first or last few rows
(the default value of ``few'' happens to be $6$ but we can specify
another value if we so choose)
<<headDyestuff>>=
head(Dyestuff)
@ 
or we could ask for a \code{summary} of the data
<<summaryDyestuff>>=
summary(Dyestuff)
@ 

\begin{figure}[tbp]
  \centering
<<Dyestuffdot,fig=TRUE,echo=FALSE,height=3.5>>=
set.seed(1234543)
print(dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff,
              ylab = "Batch", jitter.y = TRUE, pch = 21,
              xlab = "Yield of dyestuff (grams of standard color)",
              type = c("p", "a")))
@   
\caption[Yield of dyestuff from 6 batches of an intermediate]{Yield of
  dyestuff (Napthalene Black 12B) for 5 preparations from each of 6
  batches of an intermediate product (H-acid).  The line joins the
  mean yields from the batches, which have been ordered by increasing
  mean yield.  The vertical positions are ``jittered'' slightly to
  avoid over-plotting.  Notice that the lowest yield for batch A was
  observed for two distinct preparations from that batch.}
  \label{fig:Dyestuffdot}
\end{figure}
Although the \code{summary} does show us an important property of the
data, namely that there are exactly $5$ observations on each batch --- a
property that we will describe by saying that the data are
\emph{balanced} with respect to \code{Batch} --- we usually learn much
more about the structure of such data from plots like
Figure~\ref{fig:Dyestuffdot} than we can from
numerical summaries.

In Figure~\ref{fig:Dyestuffdot} we can see that there is considerable
variability in yield, even for preparations from the same batch, but
there is also noticeable batch-to-batch variability.  For example,
four of the five preparations from batch F provided lower yields than
did any of the preparations from batches C and E.  

This plot, and essentially all the other plots in this book,
were created using Deepayan Sarkar's \package{lattice} package for
\R{}.  In \citet{sarkar08:_lattic} he describes how one would create
such a plot.  Because this book was created using Sweave
\citep{lmucs-papers:Leisch:2002}, the exact code used to create the
plot, as well as the code for all the other figures and calculations
in the book, is available on the web site for the book.  In
section~\ref{sec:lattice} we review some of the principles of
lattice graphics, such as reordering the levels of the \code{Batch}
factor by increasing mean response, that enhance the informativeness
of the plot.  At this point we will concentrate on the information
conveyed by the plot and not on how the plot is created.

In section \ref{sec:DyestuffLMM} we will use mixed models to
quantify the variability in yield between batches.  For the time being
let us just note that the particular batches used in this experiment
are a selection or sample from the set of all batches that we wish to
consider.  Furthermore, the extent to which one particular batch tends
to increase or decrease the mean yield of the process --- in other
words, the ``effect'' of that particular batch on the yield --- is not
as interesting to us as is the extent of the variability between
batches.  For the purposes of designing, monitoring and controlling a
process we want to predict the yield from future batches, taking into
account the batch-to-batch variability and the within-batch
variability.  Being able to estimate the extent to which a particular
batch in the past increased or decreased the yield is not usually an
important goal for us.  We will model the effect of the batch through
random effects instead of fixed-effects parameters.

\subsection{The \texttt{Dyestuff2} data}
\label{sec:dyestuff2}

The \code{Dyestuff2} data are simulated data presented in \citet[Table
5.1.4, p. 247]{box73:_bayes_infer_statis_analy} where the authors state
\begin{quote}
  These data had to be constructed for although examples of this sort
  undoubtedly occur in practice they seem to be rarely published.
\end{quote}
The structure and summary
<<strDye2>>=
str(Dyestuff2)
summary(Dyestuff2)
@ 
are intentionally similar to those of the \code{Dyestuff} data.  As
seen in Figure~\ref{fig:Dyestuff2dot}
\begin{figure}[tbp]
  \centering
<<Dyestuff2dot,fig=TRUE,echo=FALSE,height=3.5>>=
print(dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff2,
              ylab = "Batch", jitter.y = TRUE, pch = 21,
              xlab = "Simulated response (dimensionless)",
              type = c("p", "a")))
@   
\caption[Simulated data similar in structure to the \code{Dyestuff}
data]{Simulated data presented in
  \citet{box73:_bayes_infer_statis_analy} with a structure similar to
  that of the \code{Dyestuff} data.  These data represent a case where
  the batch-to-batch variability is small relative to the within-batch
  variability.}
  \label{fig:Dyestuff2dot}
\end{figure}
the batch-to-batch variability in these data is small compared to the
within-batch variability.  In some approaches to mixed models it can
be difficult to fit models to such data.  Paradoxically, small
``variance components'' can be more difficult to estimate than large
variance components.

The methods we will present are not compromised when estimating small
variance components.

\section{Fitting linear mixed models}
\label{sec:FittingLMMs}

Before we formally define a linear mixed model, let's go ahead
and fit models to these data sets
%I know that I find it easier
%to understand definitions, equations and derivations when I have one
%or more examples to which I can relate the mathematics. I expect
%this is true for many others, too.
%
using \code{lmer}.  Like most model-fitting functions in \R{},
\code{lmer} takes, as its first two arguments, a \emph{formula}
specifying the model and the \emph{data} with which to evaluate the
formula.  This second argument, \code{data}, is optional but
recommended.  It is usually the name of a data frame, such as those we
examined in the last section. Throughout this book all model
specifications will be given in this formula/data format.

We will explain the structure of the formula after we have
considered an example.
\subsection{A model for the \texttt{Dyestuff} data}
\label{sec:DyestuffLMM}

We fit a model to the \code{Dyestuff} data allowing for an overall
level of the \code{Yield} and for an additive random effect for each
level of \code{Batch}
<<fm1>>=
fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff)
print(fm1)
@ 
In the first line we call the \code{lmer} function to fit a model with formula
<<fm1formula,echo=FALSE>>=
Yield ~ 1 + (1|Batch)
@ 
applied to the \code{Dyestuff} data and
assign the result to the name \code{fm1}.  (The name is arbitrary.  I
happen to use names that start with \code{fm}, meaning ``fitted model''.)

As is customary in \R{}, there is no output shown after this
assignment.  We have simply saved the fitted model as an object named
\code{fm1}.  In the second line we display some information about the
fitted model by applying \code{print} to \code{fm1}.  In later
examples we will condense these two steps into one but here it helps
to emphasize that we save the result of fitting a model then apply
various \emph{extractor} functions to the fitted model to get a brief
summary of the model fit or to obtain the values of some of the
estimated quantities.

\subsubsection{Details of the printed display}
\label{sec:printedDetails}

The printed display of a model fit with \code{lmer} has four major
sections: a description of the model that was fit, some statistics
characterizing the model fit, a summary of properties of the random
effects and a summary of the fixed-effects parameter estimates.  We
consider each of these sections in turn.

The description section states that this is a linear mixed model in
which the parameters have been estimated as those that minimize the
REML criterion (explained in section \ref{sec:REML}).  The
\code{formula} and \code{data} arguments are displayed for later
reference.  If other, optional arguments affecting the fit, such as a
\code{subset} specification, were used, they too will be displayed
here.

The model-fit statistics section displays the value of Akaike's
Information Criterion (\code{AIC})~\citep{saka:ishi:kita:1986}, Schwarz's
Bayesian Information Criterion (\code{BIC})~\citep{schw:1978}, the profiled
log-likelihood (\code{logLik}) at the parameter estimates, the
profiled deviance at the parameter estimates and the REML criterion on
the deviance scale.  These are all statistics related to the model fit
and they are primarily used to compare different models fit to the
same data.  We discuss them in more detail in
section~\ref{sec:logLik}.  For now we will simply state that the
parameter estimates are chosen to minimize the REML criterion as it is
displayed here.  The other model fit statistics are close to but not
exactly at their optimal values.  To get the optimal value of the
log-likelihood, the deviance and the AIC and BIC criterion we would
request the maximum likelihood (ML) estimates by including the
optional argument \code{REML=FALSE} in the call to \code{lmer}.

<<fm1ML>>=
(fm1ML <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, REML = FALSE))
@ 
(Notice that this code fragment also illustrates a way to condense
the assignment and the display of the fitted model into a single
step. The redundant set of parentheses surrounding the assignment
causes the result of the assignment to be displayed. We will use this
device often in what follows.  This code fragment also illustrates, in
the second line, the continuation prompt, \code{"+"}.  When an
incomplete \R{} expression is submitted, the prompt is changed from
\code{">"} to \code{"+"}, indicating that additional input is required
before the expression can be evaluated.)

Here we can see that the estimates of the variances of the random
effects, discussed later in this section, are noticeably different in
the REML and ML fits but the model-fit criteria, especially the
deviance and the REML criteria, have changed very little.  This is
related to our use of the profiled estimation criteria, explained in
section~\ref{sec:logLik}.  At this point the important thing to note is
that the default estimation criterion is the REML criterion.
Generally the REML estimates of variance components are preferred to
the ML estimates.  However, when comparing models it is safest to
refit all the models using the maximum likelihood criterion.  We will
discuss comparisons of model fits later.

The third section is the table of estimates of parameters associated
with the random effects.  There are two sources of variability in the
model we have fit, a batch-to-batch variability in the level of the
response and the residual or per-observation variability --- also called
the within-batch variability.  The name ``residual'' is used in
statistical modeling to denote part of the variability that cannot be
explained or modeled with the other terms.  It is the variation in the
observed data that is ``left over'' after we have determined the
estimates of the parameters in the other parts of the model.

Some of the variability in the response is associated with the
fixed-effects terms.  In this model there is only one such term,
labeled as the \code{(Intercept)}.  The name ``intercept'', which is
better suited to models based on straight lines written in a
slope/intercept form, should be understood to represent an overall
``typical'' or mean level of the response in this case.  (In case you
are wondering about the parentheses around the name, they are included
so that you can't accidentally create a variable with a name that
conflicts with this name.)  The line labeled \code{Batch} in the
random effects table shows that the random effects added to the
\code{(Intercept)} term, one for each level of the \code{Batch}
factor, are modeled as random variables whose unconditional variance
is estimated as \Sexpr{sprintf("%.2f", unlist(VarCorr(fm1)))} gm.$^2$
in the REML fit and as
\Sexpr{sprintf("%.2f", unlist(VarCorr(fm1ML)))} gm.$^2$
in the ML fit.  The corresponding standard deviations
are \Sexpr{round(sqrt(VarCorr(fm1)[["Batch"]][1,1]), 2)} gm. for the
REML fit and \Sexpr{round(sqrt(VarCorr(fm1ML)[["Batch"]][1,1]), 2)}
gm. for the ML fit.

Note that the last column in the random effects summary table is the
estimate of the variability expressed as a standard deviation rather
than as a variance.  These are provided because it is usually easier
to visualize standard deviations, which are on the scale of the
response, than it is to visualize the magnitude of a variance.  The
values in this column are a simple re-expression (the square root) of
the estimated variances. Do not confuse them with the standard errors
of the variance estimators, which are not given here.  In
section~\ref{sec:MCMC} we explain why we do not provide standard
errors of variance estimates.

The line labeled \code{Residual} in this table gives the estimate of
the variance of the residuals (also in gm.$^2$) and its corresponding
standard deviation.  For the REML fit the estimated standard deviation
of the residuals is \Sexpr{round(attr(VarCorr(fm1), "sc"), 2)} gm. and
for the ML fit it is also \Sexpr{round(attr(VarCorr(fm1ML), "sc"), 2)}
gm.  (Generally these estimates do not need to be equal.  They happen
to be equal in this case because of the simple model form and the
balanced data set.)

The last line in the random effects table states the number of
observations to which the model was fit and the number of levels of
any ``grouping factors'' for the random effects.  In this case we have
a single random effects term, \code{(1|Batch)}, in the model formula
and the grouping factor for that term is \code{Batch}.  There will be
a total of six random effects, one for each level of \code{Batch}.

The final part of the printed display gives the estimates and standard
errors  of any fixed-effects parameters in the model.  The only
fixed-effects term in the model formula is the \code{1}, denoting a
constant which, as explained above, is labeled as \code{(Intercept)}.
For both the REML and the ML estimation criterion the estimate of this
parameter is \Sexpr{round(fixef(fm1)[1], 2)} gm. (equality is again a
consequence of the simple model and balanced data set).  The standard
error of the intercept estimate is
\Sexpr{round(coef(summary(fm1))[1,2], 2)} gm. for the REML fit and
\Sexpr{round(coef(summary(fm1ML))[,2], 2)} gm. for the ML fit.

\subsubsection{Degrees of freedom and p-values
  for the fixed-effects parameters}
\label{sec:DFpval}

In general, a ``standard error'' is the estimated standard deviation
of the estimator of a parameter.  Frequently these are used to create
confidence intervals on the underlying parameter value or to perform
hypothesis tests related to the parameter.  The fixed-effects
parameters in a linear model are often expressed in such a way that a
value of zero for a parameter is of interest because, if the parameter
could reasonably be zero, then the term could be eliminated from the
model.

If the model involves only fixed-effects parameters then the ratio of
the parameter estimate to its standard error has, under the null
hypothesis that the true parameter value is zero, a Student's T
distribution with a known number of degrees of freedom.  This
distribution can be used to perform hypothesis tests and to create
confidence intervals on the parameter.

This approach can be carried over to some special cases, notably those
with simple, balanced data sets, of mixed-effects models.  However,
for the general case of mixed models this test statistic does not have
a Student's T distribution.  There have been many attempts to
approximate the distribution of this statistic by a Student's T
distribution with the degrees of freedom and possibly a multiplier
calculated according to some formula.  None of these are wholly
satisfactory, in my opinion, and rather than incorporate results from
a technique that I cannot justify and do not trust, I have chosen to
state the value of this ratio but not associate degrees of freedom or
a p-value with the value.

Many users find the omission of p-values for the fixed-effects
parameters to be alarming and more than a little vexing --- to the
extent that one of the questions in the R Frequently Asked Questions
(FAQ) list is ``Why are p-values not displayed when using lmer()?''.
The short answer is that I don't know how to evaluate such p-values in
a reliable and mathematically justifiable way for the general case and
have chosen not to do it, rather than doing it poorly.

Another part of the answer is that I feel statisticians have been too
successful in promoting the use of hypothesis testing and of p-values
to express the result of the tests --- to the extent that p-values are
sometimes used as the only summary quoted from an analysis.  I prefer
to consider the parameters in the context of the model and the
available data and only look at the results of a particular hypothesis
test when it makes sense to consider it.

For this model applied to these data we are not interested in whether
the mean yield across all batches of the intermediate product would be
zero, or even negative.  To get a mean yield close to zero we would
need to have at least some of the samples from some of the batches
providing a negative yield, which is physically impossible.
Performing a hypothesis test for such a nonsensical value of a
parameter is, well, nonsense but if the p-value for the test were
stated in the summary it would inevitably end up being quoted in some
reports somewhere.

A more sensible approach in this case would be to form a confidence
interval on the mean yield across batches or to form a prediction
interval on the mean yield of a future batch.  For these purposes it
would be helpful if we could summarize the distribution of the
estimator as a Student's T distribution with known degrees of freedom
or as another known distribution.  Regrettably, we can't do that in
general.  We do, however, provide another approach to
obtaining interval estimates of parameters using Markov chain Monte
Carlo (MCMC) techniques in section~\ref{sec:MCMC}.

\section{The linear mixed-effects probability model}
\label{sec:Probability}

In explaining some of parameter estimates related to the random
effects we have used terms such as ``unconditional distribution''
from the theory of probability.  Before proceeding further we should
clarify the linear mixed-effects probability model.  In the first part
of this section we define several terms and concepts that will be used
throughout the book.  I would recommend that subsection to all
readers.  

\subsection{Definitions of the random variables}
\label{sec:defnRV}

As mentioned in section~\ref{sec:memod}, the mixed-effects probability
model is based on two vector-valued random variables: the
$q$-dimensional vector of random effects, $\bc B$, and the
$n$-dimensional response vector, $\bc Y$.  In our model for the
\code{Dyestuff} data the dimension of the response vector is $n=30$
and the dimension of the random effects vector is $q=6$.  

In all the forms of mixed models that we will consider, the random effects
vector has a multivariate normal (also called Gaussian) distribution
with mean vector $\bm 0$ and with a parameterized, $q\times q$,
symmetric, variance-covariance matrix that we will write as
$\bm\Sigma_\theta$.  The notation indicates that this symmetric
$q\times q$ matrix $\bm\Sigma$ depends on a parameter vector that is
written as $\bm\theta$.  Even though some of the elements of
$\bm\theta$ may determine covariances and not variances, we shall
refer to them collectively as the \emph{variance-component
  parameters}.

We write this distribution as
\begin{equation}
  \label{eq:unconditionalB}
  \bc{B}\sim\mathcal{N}(\bm 0,\bm\Sigma_\theta) .
\end{equation}
Because this distribution does not depend on the value, $\bm y$, of
the random variable, $\bc Y$, we say it is the \emph{unconditional}
distribution of $\bc B$.

The probability model for the response, $\bc Y$, is most easily
described by stating the conditional distribution $(\bc Y|\bc B=\bm
b)$, which is the distribution of the response vector, $\bc Y$,
assuming that the value of the random effects vector, $\bc B$, is
known to be $\bm b$.  (In practice we never know the value $\bm b$
but, for the purpose of formulating the model, we will assume that we
do.)  For all the forms of mixed models that we will consider, $\bm b$
changes the conditional distribution of $\bc Y$ by changing the
conditional mean, $\bm\mu_{\bc Y|\bc B=\bm b}$.  Furthermore, the
conditional mean response depends on $\bm b$ and on the fixed-effects
parameter vector $\bm\beta$ only through the \emph{linear predictor}
\begin{equation}
  \label{eq:linpred}
  \bm Z\bm b+\bm X\bm\beta
\end{equation}
where $\bm Z$ and $\bm X$ are fixed and known \emph{model matrices} of
the appropriate size.

This description applies to all forms of mixed models considered in
this book.  In the case of a linear mixed model we can be more
specific about the conditional mean and the conditional distribution.
For linear mixed models the conditional mean, $\bm\mu_{\bc Y|\bc B=\bm
  b}$, is exactly the linear predictor, and the conditional
distribution is a ``spherical'' Gaussian distribution.  That is
\begin{equation}
  \label{eq:condmeanY}
  \bm\mu_{\bc Y|\bc B=\bm b}=\bm Z\bm b+\bm X\bm\beta
\end{equation}
and
\begin{equation}
  \label{eq:LMMconddist}
  (\bc Y|\bc B=\bm b)\sim\mathcal{N}\left(\bm Z\bm b+\bm
    X\bm\beta,\sigma^2\bm I_n\right) .
\end{equation}

The expression $\bm I_n$ denotes the identity matrix of size $n$.
This is the $n\times n$ matrix whose diagonal elements are all unity
and whose off-diagonal elements are all zero.  The parameter
$\sigma^2$ is called the \emph{common scale parameter}.  It is the
variance of the residual ``noise'' terms that cannot be explained by
other parts of the model.  The name ``spherical'' is applied to
Gaussian distributions of the form (\ref{eq:LMMconddist}) because
contours of constant probability density are spheres centered at $\bm
Z\bm b+\bm X\bm\beta$ in the $n$-dimensional response space

Because the conditional mean must be an $n$-dimensional vector, the
model matrix $\bm Z$ must be $n\times q$ and the model matrix $\bm X$
must be $n\times p$, where $p$ is the dimension of the fixed-effects
parameter vector, $\bm\beta$.  For the model fit to the
\code{Dyestuff} data, $p=1$ and the matrix $\bm X$ is a $30\times 1$
matrix, all of whose elements are unity.  The fixed-effects term
\code{1} in a model formula generates a column of ones in the
fixed-effects model matrix, $\bm X$.  For the model being considered
here, this column of ones is the only column in $\bm X$.

The form of the random-effects model matrix, $\bm Z$, and the form of
the variance-covariance matrix, $\bm\Sigma$, and the method by which
$\bm\Sigma$ is determined from the value of $\bm\theta$ are all based
on the random-effects terms in the model formula.  As stated earlier,
there is one random effects term, \code{(1 | Batch)}, in the formula
for this model.  Random-effects terms are those that contain the
vertical bar, \code{"|"}, character.  The \code{Batch} variable is the
grouping factor for the random effects described by this term.  An
expression for the grouping factor, usually just the name of a
variable, occurs to the right of the vertical bar.  If the expression
on the left of the vertical bar is \code{1}, as it is here, we
describe the term as a \emph{simple, scalar, random-effects term}.
The designation ``scalar'' means there will be exactly one random
effect generated for each level of the grouping factor.  A simple,
scalar term generates a block of indicator columns --- the indicators for
the grouping factor --- in $\bm Z$.  Because there is only one
random-effects term in this model and because that term is a simple,
scalar term, the model matrix $\bm Z$ for this model is the indicator
matrix for the levels of \code{Batch}.

The transpose of this matrix, of dimension $6\times 30$, is stored as
a sparse matrix in the environment of fitted model object as
\code{Zt}.
<<fm1Zt>>=
env(fm1)$Zt
@ 
A sparse matrix is one in which most of the elements are known to be
zero.  These elements are represented by a \code{"."} in this display.
The nature of an indicator matrix is such that only one element in
each row of the indicator matrix, corresponding to a column of the
transpose of the indicator matrix that is shown above, is non-zero.
Sparse matrix
methods~\citep{davis06:_direc_method_for_spars_linear_system} for
numerical linear algebra provide special techniques for storing and
manipulating such matrices.  These techniques are the basis of the
numerical methods implemented in \code{lmer}.

Often we will show the structure of sparse matrices as an image like
Figure~\ref{fig:fm1Ztimage}.
\begin{figure}[tbp]
  \centering
<<fm1Ztimage,fig=TRUE,echo=FALSE,height=2.5>>=
print(image(env(fm1)$Zt, sub=NULL))
@   
  \caption[Image of the random-effects model matrix for
  \texttt{fm1}]{Image of the transpose of the random-effects model
    matrix, $\bm Z$, for model \code{fm1}.  The non-zero elements,
    which are all unity, are shown as darkened squares.  The zero
    elements are blank.}
  \label{fig:fm1Ztimage}
\end{figure}
Especially for large matrices, the image of the sparse matrix conveys
its structure more compactly than does the printed representation.

When the model contains only one random-effects term and that term is
a simple, scalar term, then the variance-covariance matrix
$\bm\Sigma_\theta$ is a non-negative multiple of $\bm I_q$, the
$q\times q$ identity matrix.  Although we will defer until later the
discussion of the exact form of $\bm\Sigma_\theta$ for other
models, we will now introduce a transformation of
$\bm\Sigma_\theta$ that we will use throughout.

\subsubsection{The relative covariance factor}
\label{sec:relcovfac}

A variance-covariance matrix like $\bm\Sigma$ is required to be
symmetric and, in addition, to be \emph{positive semi-definite}, which
means, in effect, that $\bm\Sigma$ has the matrix equivalent of a
``square root''.  Recall that a scalar variance, such as $\sigma^2$ in
(\ref{eq:LMMconddist}), must be non-negative and that the standard
deviation, $\sigma\ge0$, is the square root of the variance.  A
variance-covariance matrix has a similar property.  There must be a
matrix, say $\bm\Lambda_\theta$, such that when it is multiplied
by its transpose it produces the original matrix,
$\bm\Sigma_\theta$.  This is not quite like ``squaring'' the
matrix because the factor, $\bm\Lambda_\theta$, is multiplied by
its transpose, so as to create a product that is symmetric, as
$\bm\Sigma_\theta$ must be.

It turns out that we can simplify many subsequent formulas if we
define $\bm\Lambda_\theta$ to be a multiple of the square-root
factor, which we call the \emph{relative covariance factor}, defined
to satisfy
\begin{equation}
  \label{eq:relcovfac}
  \bm\Sigma_\theta=\sigma^2\bm\Lambda_\theta
  \bm\Lambda_\theta\trans .
\end{equation}
The symbol $\trans$ denotes the transpose of a matrix and $\sigma^2$
is the common scale parameter defined in (\ref{eq:LMMconddist}).

The term ``relative'' indicates that $\bm\Lambda_\theta$ is the
factor of the variance, $\bm\Sigma$, of $\bc B$ relative to the
variance, $\sigma^2$, in the conditional distribution $(\bc Y|\bc
B=\bm b)$.  It happens that the way that we generate
$\bm\Lambda_\theta$ it ends up being lower triangular and
corresponds to the left Cholesky factor of
$\bm\Sigma_\theta/\sigma^2$.  A left Cholesky factor is often
written $\bm L$; because the matrix being considered here is composed
of variance and covariance parameters we denote it with the
corresponding Greek letter, $\bm\Lambda$.

For the \code{Dyestuff} model, which has only one simple, scalar
random-effects term, $\bm\Sigma_\theta$ and
$\bm\Lambda_\theta$ are both multiples of $\bm I_6$, the identity
matrix of size six.  Furthermore, $\bm\theta$ is one-dimensional so we
will write it as $\theta$.  We set
\begin{equation}
  \label{eq:lambdatheta}
  \bm\Lambda_\theta = \theta\bm I_6
\end{equation}
subject to the constraint that $\theta\ge0$.

\subsection{The profiled ML criteria}
\label{sec:log-likelihood}

At this point we will simply describe how the profiled ML and REML
criteria can be evaluated, leaving the detailed derivations until
later.  These criteria are ``profiled'' in the sense that they are
functions of the parameter $\bm\theta$ only.  Dependence on the other
parameters, $\bm\beta$ and $\sigma^2$, has been removed by
determining their optimal values, given a particular value of
$\bm\theta$.  

Given a value of $\bm\theta$, we create
\begin{equation}
  \label{eq:Umat}
  \bm U_\theta=\bm Z\bm\Lambda_\theta,
\end{equation}
which will be an $n\times q$ sparse matrix like $\bm Z$, and evaluate
the $q\times q$ lower triangular sparse Cholesky factor, $\bm
L_\theta$ that satisfies
\begin{equation}
  \label{eq:Lmat}
  \bm L\bm L\trans=\bm U\trans\bm U+\bm I_q .
\end{equation}

It is this step that is the key to the methods in the \package{lme4}
package for fitting mixed models.  For the simple example we are
considering this is trivial because $\bm U(\theta)=\bm Z(\theta\bm
I_6)=\theta\bm Z$ and $\bm U\trans\bm U+\bm I_6=(1 + 5\theta^2)\bm
I_6$ so a suitable $\bm L(\theta)=\sqrt{1 + 5\theta^2}\bm I_6$.  The
calculations are straightforward in this case.  However, the
\code{lmer} function has been used to fit linear mixed models for
which the number of rows and the number of columns in $\bm U$ were
both more than one million and the structure of $\bm U\trans\bm U+\bm
I_q$ was much more complex than a simple multiple of an identity
matrix.  The ability to compute this factorization reasonably quickly
for many different values of $\bm\theta$ in cases like this is what
makes the methods in the \package{lme4} package feasible.

Because $\bm L_\theta$ is square and lower triangular, its
determinant, written $|\bm L|$, is simply the product of its diagonal
elements.  The determinant is the measure of the size of
the matrix that is used in evaluating probability densities of random
variables after transformation.

After evaluating the minimum penalized residual sum of squares
\begin{equation}
  \label{eq:PLRSS}
  r^2_\theta = \min_{\bm u,\bm\beta}\|\bm y-\bm U\bm u-\bm
  X\bm\beta\|^2+\|\bm u\|^2
\end{equation}
(details given below) we can express the \emph{profiled deviance},
which is negative twice the logarithm of the profiled likelihood, as
\begin{equation}
  \label{eq:profDev}
  -2\tilde{\ell}_\theta=\log(|\bm L|^2)+n\left[1+
    \log\left(\frac{2\pi r^2}{n}\right)\right] .
\end{equation}
We minimize the profiled deviance with respect to $\bm\theta$,
producing the ML estimate, $\widehat{\bm\theta}$, from which we derive
the ML estimates of the other parameters, $\widehat{\bm\beta}$ and
$\widehat{\sigma^2}$.

Before describing the details of the calculation of $r^2_\theta$
and the calculation of the other parameter estimates, let us consider
the form of the profiled deviance, (\ref{eq:profDev}), as it relates
to our model for the \code{Dyestuff} data set and to a similar model
fit to the \code{Dyestuff2} data.

\subsubsection{Profiled deviance for the \texttt{Dyestuff} model}

We can follow the progress of the iterations in optimizing the
profiled deviance for the model fit to the \code{Dyestuff} data by
including the optional argument \code{verbose = 1} in the call to \code{lmer}
<<fm1MLverb>>=
fm1ML <- lmer(Yield ~ 1 + (1 |Batch), Dyestuff, verbose = 1, REML = FALSE)
@ 
Each line of the ``verbose'' output consists of the iteration number,
the current value of the profiled deviance and the current values of
the parameter vector $\bm\theta$, which, for this model, is a scalar,
$\theta$.  It can be seen that the optimization converges very quickly
to an minimum deviance of \Sexpr{round(unname(deviance(fm1ML)), 3)} at
$\hat{\theta}=\Sexpr{round(env(fm1ML)[["theta"]], 3)}$.

Some of the components of the deviance can be obtained with the
\code{devcomp} extractor.
<<fm1MLdev>>=
round(devcomp(fm1ML)$cmp,2)
@
The elements named \code{"ldL2"} and \code{"prss"} are $\log(|\bm
L|^2)$ and $r^2_\theta$, the minimum penalized weighted residual sum
of squares, respectively.  We can evaluate the deviance as
<<fm1MLdev2>>=
dev <- devcomp(fm1ML)$cmp
n <- nrow(model.frame(fm1ML))
(calc <- dev["ldL2"] + n * (1 + log(2 * pi * dev["prss"]/n)))
all.equal(unname(calc), unname(dev["deviance"]))
@ 

Because $\theta$ is one-dimensional in this model we can examine each
of the components as they depend on $\theta$.  In Figure~\ref{fig:DyeML}
\begin{figure}[tbp]
  \centering
<<DyeML,fig=TRUE,echo=FALSE,results=hide,height=5>>=
fm1env <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, REML=FALSE, doFit = FALSE)
theta <- seq(0,3,0.01)
res <-
    as.data.frame(t(sapply(theta,
                           function(x) {
                               fm1env@setPars(x)
                               rho <- env(fm1env)
                               dc <- devcomp(fm1env)
                               sigma <- sqrt(dc$cmp["prss"]/dc$dims[c("n","nmp")])
                               c(theta = x, dc$cmp, sigma,
                                 with(rho, c(usqr = crossprod(u),
                                             rss = crossprod(y-fitted))))
                           })))
res$lprss <- n * (1 + log(2 * pi * res$prss/n))
print(xyplot(deviance + ldL2 + lprss ~ theta,
             res, xlab = expression(theta), ylab = NULL, outer = TRUE,
             scales = list(y = list(relation = "free", rot = 0),
                           x = list(axs = 'i')),
	     strip = FALSE, strip.left = TRUE,
	     type = c("g","l"), layout = c(1,3)))
@   
\caption[Profiled deviance and its components for the \code{Dyestuff}
model]{The profiled deviance and its components as a function of
  $\theta$ in the simple model fit to the \code{Dyestuff} data.  The
  panel labeled \code{ldL2} is $\log(|\bm L(\theta)|^2)$ and the
  panel labeled \code{lprss} is $n[1+\log(2\pi r^2(\theta))/n)]$.}
  \label{fig:DyeML}
\end{figure}
we show the deviance as a function of $\theta$ (bottom panel) and the
two additive terms, $\log(|\bm L(\theta)|^2)$ and $n[1+\log(2\pi
r^2/n)]$ in (\ref{eq:profDev}).  In this figure we can see that the
logarithm of the determinant of $\bm L$ increases as $\theta$
increases.  For this particular model and data set we can write the
log-determinant as
\begin{equation}
  \label{eq:ldL2Dye}
  \log(|\bm L(\theta)|^2) =
  \log\left(\left(1+5\theta^2\right)^6\right),\quad\theta\ge 0 .
\end{equation}
It is easy to establish that $\log(|\bm L(0)|^2)=\log(1)=0$ and that
this log-determinant is monotone increasing.  For large values of
$\theta$ the log-determinant approaches $6\log(5)+12\log(\theta)$.
The important thing to note is that it continues to increase as
$\theta$ increases and that it behaves like a multiple of
$\log(\theta)$ for large values of $\theta$.

The other term, $n[1+\log(2\pi r^2(\theta)/n)]$, depends on $\theta$
only through the minimum penalized residual sum of squares,
$r^2(\theta)$.  In Figure~\ref{fig:prssCom}
\begin{figure}[tbp]
  \centering
<<prssCom,fig=TRUE,echo=FALSE,>>=
print(xyplot(prss + usqr + rss ~ theta,
             res, xlab = expression(theta), ylab = NULL, outer = TRUE,
             scales = list(y = list(relation = "free", rot = 0),
                           x = list(axs = 'i')),
	     strip = FALSE, strip.left = TRUE,
	     type = c("g","l"), layout = c(1,3)))
@   
  \caption[Components of $r^2(\theta)$]{Components of the penalized
    sum of square residuals, $r^2(\theta)$ (labeled \code{pwrss}).
    The residual sum of squares, $\|\bm y-\bm U(\theta)\tilde{\bm
      u}(\theta) - \bm X\widehat{\bm\beta}(\theta)\|^2$, where
    $\tilde{\bm u}$ and $\widehat{\bm\beta}(\theta)$ jointly minimize the
    penalized least squares criterion, is labeled \code{wrss}.  The
    penalty term, $\|\tilde{\bm u}\|^2$, is labeled \code{usqr}.}
  \label{fig:pwrssCom}
\end{figure}
we plot $r^2(\theta)$ (labeled \code{pwrss}) and its two components
$\|\tilde{\bm u}(\theta)\|^2$ (labeled \code{usqr}) and $\|\bm y-\bm
U(\theta)\tilde{\bm u}(\theta)-\bm X\widehat{\bm\beta}(\theta)\|^2$
(labeled \code{wrss}), where
\begin{equation}
  \label{eq:jointmin}
  \begin{bmatrix}
    \tilde{\bm u}(\theta)\\
    \widehat{\bm\beta}(\theta)
  \end{bmatrix} =
  \arg\min_{\bm u,\bm\beta}\left[
  \|\bm y-\bm U(\theta)\bm u-\bm X\bm\beta\|^2+\|\bm u\|^2\right]
\end{equation}
The notation ``$\arg\min$'' indicates that $\tilde{\bm u}$ and
$\widehat{\bm\beta}$ are the values of $\bm u$ and $\bm\beta$ that
jointly minimize the penalized sum of squared residuals.  They satisfy
the equation
\begin{equation}
  \label{eq:PLSjointSol}
  \begin{bmatrix}
    \bm U\trans\bm U + \bm I & \bm U\trans\bm X\\
    \bm X\trans\bm U         & \bm X\trans\bm X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\bm u}(\theta)\\
    \widehat{\bm\beta}(\theta)
  \end{bmatrix} =
  \begin{bmatrix}\bm U\trans\bm y\\\bm X\trans\bm y\end{bmatrix} .
\end{equation}

In Figure~\ref{fig:pwrssCom} the plot of the sum of squared residuals,
$\|\bm y-\bm U(\theta)\tilde{\bm u}(\theta)- \bm
X\widehat{\bm\beta}(\theta)\|^2$, labeled \code{wrss}, versus $\theta$
has its maximum at $\theta = 0$.  This component quickly decreases
then levels out as $\theta$ increases.  This sum of squared residuals
is bounded above and below.  The upper bound is
<<wrssup>>=
deviance(lm(Yield ~ 1, Dyestuff))
@
which is the sum of squared residuals from a linear model with only
the fixed-effects term.  The lower bound is
<<wrsslow>>=
deviance(lm(Yield ~ 1 + Batch, Dyestuff))
@ 
which is the sum of squared residuals for a linear model with
both the fixed-effects and the random-effects terms from the mixed
model appearing as fixed-effects terms.  Symbolically
\begin{equation}
  \label{eq:wrssbounds}
  \min_{\bm u,\bm\beta}\|\bm y-\bm U\bm u-\bm X\bm\beta\|^2\le
  \|\bm y-\bm U\tilde{\bm u}-\bm X\widehat{\bm\beta}\|^2
  \le \min_{\bm\beta}\|\bm y-\bm X\bm\beta\|^2 .
\end{equation}

It turns out that these bounds also apply to the penalized residual
sum of squares, labeled \code{pwrss} in
Figure~\ref{fig:pwrssCom}. We can either reason that
\begin{equation}
  \label{eq:pwrssbounds}
  \min_{\bm u,\bm\beta}\|\bm y-\bm U\bm u-\bm X\bm\beta\|^2\le
  \min_{\bm u,\bm\beta}\|\bm y-\bm U\bm u-\bm X\bm\beta\|^2+\|\bm u\|^2
  \le \min_{\bm\beta}\|\bm y-\bm X\bm\beta\|^2 .
\end{equation}
or check the pattern in the plot of $\|\tilde{\bm u}(\theta)\|^2$,
labeled \code{usqr} in  Figure~\ref{fig:pwrssCom}.  It is zero when
$\theta=0$ and, as $\theta\rightarrow\infty$, this term tends to
zero.

Returning to the deviance for the mixed model and its components, as
plotted in Figure~\ref{fig:DyeML}, we can see that the term labeled
\code{lprss} will be monotone decreasing with finite upper and lower
bounds, because it is an increasing function of $r^2(\theta)$, which
itself has finite upper and lower bounds.  The term labeled
\code{ldL2} is bounded below at zero but is unbounded above and
monotone increasing.  The minimum deviance, which is the sum of these
two terms, must occur at a finite $\theta$.  In other words we can't
have $\hat{\theta}=\infty$.

We can, however, have $\hat{\theta}=0$, which is exactly what occurs
for the same model fit to the \code{Dyestuff2} data.
<<fm2ML>>=
(fm2ML <- lmer(Yield ~ 1 + (1 |Batch), Dyestuff2, verbose = 1,
               REML = FALSE))
@ 
Figure~\ref{fig:Dye2ML}
\begin{figure}[tbp]
  \centering
<<Dye2ML,fig=TRUE,echo=FALSE,results=hide,height=9>>=
fm2MLpart <- lmer(Yield ~ 1 + (1|Batch), Dyestuff2, REML=FALSE, doFit = FALSE)
theta <- seq(0,3,0.01)
res2 <-
    as.data.frame(t(sapply(theta,
                           function(x) {
                               fm2MLpart@setPars(x)
                               rho <- env(fm2MLpart)
                               c(devcomp(fm2MLpart)$cmp, theta = x,
                                 with(rho, c(usqr = crossprod(u),
                                             rss = crossprod(y-fitted))))
                           })))
res2$lprss <- n * (1 + log(2 * pi * res2$prss/n))
print(xyplot(deviance + ldL2 + lprss + prss + usqr + rss ~ theta,
             res2, xlab = expression(theta), ylab = NULL, outer = TRUE,
             scales = list(y = list(relation = "free", rot = 0),
                           x = list(axs = 'i')),
	     strip = FALSE, strip.left = TRUE,
	     type = c("g","l"), layout = c(1,6)))
@   
\caption[Profiled deviance and its components for the \code{Dyestuff2}
model]{The profiled deviance and its components as a function of
  $\theta$ in the simple model fit to the \code{Dyestuff2} data.  The
  panel labeled \code{ldL2} is $\log(|\bm L(\theta)|^2)$ and the panel
  labeled \code{lprss} is $n[1+\log(2\pi r^2(\theta))/n)]$.  The
  panel labeled \code{prss} is $r^2(\theta)$, with components
  \code{usqr} and \code{rss}.}
  \label{fig:Dye2ML}
\end{figure}
shows the deviance for this model, and its components, as a function
of $\theta$, from which we can see that the deviance is indeed
minimized at $\theta=0$.

<<rsqrvals,echo=FALSE,results=hide>>=
rsqr1up <- deviance(lm(Yield ~ 1, Dyestuff))
devcomp1up <- n * (1 + log(2 * pi * rsqr1up/n))
devcomp1REup <- (n - 1) * (1 + log(2 * pi * rsqr1up/(n - 1)))
rsqr1low <- deviance(lm(Yield ~ 1 + Batch, Dyestuff))
devcomp1low <- n * (1 + log(2 * pi * rsqr1low/n))
devcomp1RElow <- (n - 1) * (1 + log(2 * pi * rsqr1low/(n - 1)))
@

Comparing Figure~\ref{fig:Dye2ML} to Figure~\ref{fig:DyeML} we can see
that the relationships between the components of the deviance,
$\log(|\bm L|^2)$ (\code{ldL2}) and $n[1+\log(2\pi r^2/n)]$
(\code{lprss}), and $\theta$ are similar for the two models.  In fact,
$\log(|\bm L|^2)$ versus $\theta$ is exactly the same for the two
models.  For $n[1+\log(2\pi r^2/n)]$, however, the shape of the
relationship to $\theta$ in Figure~\ref{fig:Dye2ML} is similar to that
in Figure~\ref{fig:DyeML} but the range on the vertical axis is very
different.  Recall that for the \code{Dyestuff} model, the range of
the penalized residual sum of squares, $r^2(\theta)$, was
\begin{equation}
  \label{eq:rsqrrange}
  \Sexpr{sprintf("%.1f",rsqr1low)}
  \le r^2(\theta) \le
  \Sexpr{sprintf("%.1f",rsqr1up)}
\end{equation}
producing the bounds
\begin{equation}
  \label{eq:devcomprange}
  \Sexpr{sprintf("%.3f", devcomp1low)}
  \le n\left[1+\log\left(\frac{2\pi r^2(\theta)}{n}\right)\right] \le
  \Sexpr{sprintf("%.3f", devcomp1up)} .
\end{equation}

<<rsqrvals2,echo=FALSE,results=hide>>=
rsqr2up <- deviance(lm(Yield ~ 1, Dyestuff2))
devcomp2up <- n * (1 + log(2 * pi * rsqr2up/n))
rsqr2low <- deviance(lm(Yield ~ 1 + Batch, Dyestuff2))
devcomp2low <- n * (1 + log(2 * pi * rsqr2low/n))
@

On the other hand, the range of the penalized residual sum of squares
for the \code{Dyestuff2} model is much narrower because
<<wrss2up>>=
deviance(lm(Yield ~ 1, Dyestuff2))
@
and
<<wrss2low>>=
deviance(lm(Yield ~ 1 + Batch, Dyestuff2))
@
In this case the bounds on the deviance component are
\begin{equation}
  \label{eq:devcomp2range}
  \Sexpr{round(devcomp2low, 3)}
  \le n\left[1+\log\left(\frac{2\pi r^2(\theta)}{n}\right)\right] \le
  \Sexpr{round(devcomp2up, 3)} .
\end{equation}
and this range is too narrow to produce an estimate $\hat{\theta}$
that is greater than zero.

\subsection{Interpretation of the deviance components}
\label{sec:devcompinterp}

The two components of the profiled deviance, $\log(|\bm L(\theta)|^2)$
and $n[1 + \log(2\pi r^2(\theta)/n)]$, can be considered as measuring
the complexity of the model and the fidelity of the fitted model to
the data, respectively.  With these interpretations, maximum
likelihood estimation of the parameters in a mixed-effects model can
be considered as falling into the class of statistical techniques
known as ``smoothing'' techniques.  In these techniques a family of
models, controlled by one or more ``smoothing parameters'', is
proposed and the smoothing parameter(s) are chosen according to some
criterion that balances complexity of the model against fidelity to
the data.

In the models for the \code{Dyestuff} and \code{Dyestuff2} data, the
parameter $\theta$ controls the complexity as measured by $\log(|\bm
L(\theta)|^2)$.  The simplest model, corresponding to $\theta = 0$,
ignores the \code{Batch} factor and formulates the same prediction for
all the samples from all the batches.  The most complex model,
corresponding to $\theta=\infty$, formulates a separate prediction for
each batch and ignores any commonality between the batches.

Fidelity to the data, measured either by the residual sum of squares
or by the penalized residual sum of squares, $r^2(\theta)$, increases
as $\theta$ increases.  (A lower residual sum of squares corresponds
to greater fidelity of the model's predictions to the observed data
values.)

In the case of the \code{Dyestuff} data, increasing $\theta$ in the
range $[0, 0.753]$ produces an increase in fidelity that more than
compensates for the increase in complexity.  After $\theta=0.753$
increasing the value of $\theta$ cannot produce sufficiently greater
fidelity to the observed data to compensate for the increase in model
complexity, according to the ML criterion.

In the case of the \code{Dyestuff2} data the fidelity component does not change sufficiently with increasing $\theta$ to compensate for the increased model complexity and the optimal value of $\theta$ is at the boundary, $\hat{\theta}=0$.

\subsection{The REML criterion}
\label{sec:REML}

To obtain the maximum likelihood (ML) estimates we minimize the
profiled deviance (\ref{eq:profDev}) as a function of $\bm\theta$. The
criterion that is labeled \code{REMLdev} in the display of a fitted
model is on the same scale as the deviance, which is negative twice
the log-likelihood.  To obtain the REML estimates of the parameters we
minimize this criterion with respect to $\bm\theta$ then determine the
conditional estimates, $\widehat{\bm\beta}(\widehat{\bm\theta})$ and
$\widehat{\sigma^2}(\widehat{\bm\theta})$.

To explain how the profiled REML criterion is evaluated we must
provide a bit more detail regarding the solution of the penalized
least squares problem, (\ref{eq:PLSjointSol}). Recall that $\tilde{\bm
  u}_\theta$ and $\widehat{\bm\beta}_\theta$ are the solutions
to
\begin{displaymath}
  \begin{bmatrix}
    \bm U\trans\bm U + \bm I & \bm U\trans\bm X\\
    \bm X\trans\bm U         & \bm X\trans\bm X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\bm u}_\theta\\
    \widehat{\bm\beta}_\theta
  \end{bmatrix} =
  \begin{bmatrix}\bm U\trans\bm y\\\bm X\trans\bm y\end{bmatrix} .
\end{displaymath}
where
\begin{displaymath}
  \bm U_\theta=\bm Z\bm\Lambda_\theta .
\end{displaymath}
Furthermore, the Cholesky factor, $\bm L_\theta$, is the sparse,
lower-triangular matrix that satisfies (\ref{eq:Lmat}),
\begin{displaymath}
  \bm L_\theta\bm L\trans_\theta=\bm U\trans_\theta\bm U_\theta+\bm I_q .
\end{displaymath}

We can extend the Cholesky factor of $\bm U\trans\bm U+\bm I_q$ to
create the factorization
\begin{equation}
  \label{eq:bigChol}
  \begin{bmatrix}
    \bm L            & \bm 0\\
    \bm R_{ZX}\trans  & \bm R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L\trans   & \bm R_{ZX}\\
    \bm 0         & \bm R_X
  \end{bmatrix} =
  \begin{bmatrix}
    \bm U\trans\bm U+\bm I_q & \bm U\trans\bm X\\
    \bm X\trans\bm U         & \bm X\trans\bm X
  \end{bmatrix}
\end{equation}
of the matrix defining the system of equations
(\ref{eq:PLSjointSol}).  In (\ref{eq:bigChol}) $\bm R_X$ is
a $p\times p$ upper triangular matrix and $\bm R_{ZX}$ is a $q\times
p$ matrix.  Both are stored as dense matrices.  $\bm R_{ZX}$ is the
solution to the triangular system
\begin{equation}
  \label{eq:RZX}
  \bm L\bm R_{ZX}=\bm U\trans\bm X
\end{equation}
and $\bm R_X$ is the upper (or ``right'', as opposed to left) Cholesky
factor satisfying
\begin{equation}
  \label{eq:RX}
  \bm R\trans\bm R=\bm X\trans\bm X-\bm R_{ZX}\trans\bm R_{ZX} .
\end{equation}
(The inconsistency of notation, using the left Cholesky factor, $\bm L$, of the
sparse matrix and the right Cholesky factor, $\bm R_X$, of the dense
matrix, reflects the differences in treatment of sparse and dense
matrices in \R{}.  The methods for determining the Cholesky factor of
sparse matrices return the left factor and those for dense matrices
return the right factor.)

The profiled REML criterion, expressed on the deviance scale, is 
\begin{equation}
  \label{eq:REMLdev}
  -2\tilde{\ell}_R(\theta)=\log(|\bm L|^2|\bm R_X|^2)+(n-p)\left[1+
    \log\left(\frac{2\pi r^2}{n-p}\right)\right] .
\end{equation}
Let $\widehat{\bm\theta_R}$ be the value of $\bm\theta$ that minimizes
this criterion. The REML estimates of the fixed-effects parameters are
the conditional estimates, $\widehat{\bm\beta}_\theta$, satisfying
(\ref{eq:PLSjointSol}), evaluated at $\widehat{\bm\theta_R}$.  The
REML estimate of $\sigma^2$, the common scale parameter, is
\begin{equation}
  \label{eq:REMLsigma}
  \widehat{\sigma^2_R}=\frac{r^2(\widehat{\bm\theta_R})}{n-p} .
\end{equation}

This last expression provides a hint of the motivation for the REML
criterion. For a linear model without any random effects, which could
be expressed as
\begin{equation}
  \label{eq:LinearNoRE}
  \bc Y\sim\mathcal{N}(\bm X\bm\beta, \sigma^2\bm I_n) ,
\end{equation}
the maximimum likelihood estimate of $\sigma^2$ is
\begin{equation}
  \label{eq:lmsigmaML}
  \widehat{\sigma^2_{\mathit{ML}}}=\frac{\|\bm y-\bm X\widehat{\bm\beta}\|^2}{n}
\end{equation}
but the unbiased estimate
\begin{equation}
  \label{eq:lmsigmaR}
  \widehat{\sigma^2_R}=\frac{\|\bm y-\bm X\widehat{\bm\beta}\|^2}{n-p}
\end{equation}
is usually preferred.  The estimate $\widehat{\sigma^2_R}$ from
(\ref{eq:lmsigmaR}) accounts for the fact that the residual vector at
the parameter estimates, $\bm y-\bm X\widehat{\bm\beta}$, must be
orthogonal to the $p$ columns of the model matrix $\bm X$ and hence
lies in an $(n-p)$-dimensional subspace of the sample space.  This
dimension, $n-p$, is the number of degrees of freedom associated with
this residual vector.  The name ``degrees of freedom'' reflects that
fact that, if we knew $n-p$ of the components of the residual vector,
then we could determine what the other $p$ components must be from the
orthogonality conditions, the so-called ``normal equations'', that
they must satisfy.

The REML estimation criterion for linear mixed models is an attempt to
generalize the concept of estimating variance components from the
residual space.  (The term REML is understood to stand for either
``residual maximum likelihood'' or ``restricted maximum likelihood''.)
In some ways this attempt is successful.  For example, the REML
estimate of $\sigma^2$ does reduce to the usual estimate for the
variance in linear models when the random effects are eliminated from
the mixed model.  In the case of the \code{Dyestuff2} data the summary
of model fit by REML
<<Dye2REML>>=
lmer(Yield ~ 1 + (1 | Batch), Dyestuff2)
@ 
coincides with the summary of a linear model
<<Dye2lm>>=
summary(lm(Yield ~ 1, Dyestuff2))
@
in the sense that the estimated residual standard deviation in the
linear mixed model corresponds to the ``Residual standard error'' in
the linear model.  The estimate of the $1$-dimensional fixed-effects
vector, $\beta$, and its standard error also coincide in these two
fitted models.

Another consideration that many consider to be important is that the
REML estimates coincide with estimates of variance components that are
formulated by equating ``expected mean squares'' with ``observed mean
squares'' in certain simple cases.  We won't go into detail regarding
these estimation methods because they apply, for the most part, to
data that are completely balanced.  One may perhaps obtain such data
from a designed experiment --- although expecting that a balanced
experimental design will always produce a balanced data set is
contrary to ``Murphy's Law'' --- but data in an observational study
are almost never balanced.  It is not worthwhile considering
approximate methods that apply primarily to balanced data when more
mathematically justifiable methods like maximum likelihood or REML
estimation with wide applicability are available.

In other ways the attempt to generalize the unbiased estimator of a
variance in linear models to the REML estimates of variance components
in linear mixed models is unsuccessful.  For one thing these
estimators are not necessarily unbiased.  Also, despite the ``ML''
part of the name REML, these estimators are not maximum likelihood
estimators, even in a ``restricted'' sense, and they do not share all
the properties of maximum likelihood estimators.  This is why it is
advisable to refit models using \code{REML = FALSE} before attempting
to compare models.

Figure~\ref{fig:Dye1REMLcomp}
\begin{figure}[tbp]
  \centering
<<Dye1REMLcomp,fig=TRUE,echo=FALSE,height=6.5>>=
res <- within(res,
          {
              lprssRE <- (n - 1) * (1 + log(2 * pi * prss/(n - 1)))
              ldSum <- ldL2 + ldRX2
          })
print(xyplot(REML + ldL2 + ldRX2 + ldSum + lprssRE ~ theta,
             res, xlab = expression(theta), ylab = NULL, outer = TRUE,
             scales = list(y = list(relation = "free", rot = 0),
                           x = list(axs = 'i')),
	     strip = FALSE, strip.left = TRUE,
	     type = c("g","l"), layout = c(1,5)))

@   
\caption[Components of the REML criterion for the \code{Dyestuff}
model]{Components of the REML criterion for the model fit to the
  \code{Dyestuff} data.  The component labeled \code{ldL2} is
  $\log(|\bm L|^2)$, as before, and the component labeled \code{ldRX2}
  is $\log(|\bm R_X|^2)$.  Their sum is the component labeled
  \code{ldSum}. The component labeled \code{lprssRE} is $(n-1)[1 +
  \log(2\pi r^2/(n-p))]$.  The REML criterion, labeled \code{REML}, is
  the sum of \code{ldSum} and \code{lprssRE}.}
  \label{fig:Dye1REMLcomp}
\end{figure}
shows the REML criterion for the simple model fit to the
\code{Dyestuff} data and the components of this criterion.  As we
noted previously, the component $\log(|\bm L|^2)$ increases with
increasing $\theta$.  The other two additive components, $\log(|\bm
R_X|^2)$ and $(n-1)[1 + \log(2\pi r^2/(n-p))]$ decrease with
increasing $\theta$.  The range of the fidelity component
\begin{equation}
  \label{eq:devcomprange}
  \Sexpr{sprintf("%.3f", devcomp1RElow)}
  \le (n - 1)\left[1+\log\left(\frac{2\pi r^2(\theta)}{n-1}\right)\right] \le
  \Sexpr{sprintf("%.3f", devcomp1REup)} .
\end{equation}
is wider than the range of the fidelity component in the deviance
(\ref{eq:devcomprange}).  This wider range, which emphasizes the
fidelity component relative to the complexity component, $\log(|\bm
L|^2)$, combined with the contribution of the decreasing component,
$\log(|\bm R_X|^2)$, results in 
\begin{equation}
  \label{eq:thetacomp}
  \widehat{\theta_R}=
  \Sexpr{sprintf("%.4f", fm1@getPars())} >
    \Sexpr{sprintf("%.4f", fm1ML@getPars())}
      =\widehat{\theta_{\mathit{ML}}}
\end{equation}

Figure~\ref{fig:Dye2REMLcomp}
\begin{figure}[tbp]
  \centering
<<Dye2REMLcomp,fig=TRUE,echo=FALSE,height=6.5>>=
res2 <- within(res2,
           {
               lprssRE <- (n - 1) * (1 + log(2 * pi * prss/(n - 1)))
               ldSum <- ldL2 + ldRX2
           })
print(xyplot(REML + ldL2 + ldRX2 + ldSum + lprssRE ~ theta,
             res2, xlab = expression(theta), ylab = NULL, outer = TRUE,
             scales = list(y = list(relation = "free", rot = 0),
             x = list(axs = 'i')),
	     strip = FALSE, strip.left = TRUE,
	     type = c("g","l"), layout = c(1,5)))
@   
\caption[Components of the REML criterion for the \code{Dyestuff2}
model]{Components of the REML criterion for the model fit to the
  \code{Dyestuff2} data.  The component labeled \code{ldL2} is
  $\log(|\bm L|^2)$, as before, and the component labeled \code{ldRX2}
  is $\log(|\bm R_X|^2)$.  Their sum is the component labeled
  \code{ldSum}. The component labeled \code{lprssRE} is $(n-1)[1 +
  \log(2\pi r^2/(n-p))]$.  The REML criterion, labeled \code{REML}, is
  the sum of \code{ldSum} and \code{lprssRE}.}
  \label{fig:Dye2REMLcomp}
\end{figure}
shows the components of the REML criterion for the simple mixed model
fit to the \code{Dyestuff2} data.  The components $\log(|\bm L|^2)$
(labeled \code{ldL2}) and $\log(|\bm R_X|^2)$ (labeled \code{ldRX2})
are the same as those shown in Figure~\ref{fig:Dye1REMLcomp} for the
model fit to the \code{Dyestuff} data.  These components,
characterizing model complexity, depend only on the values of the
covariates and the form of the model, which are the same in these two
models.

The fidelity component, $(n-1)[1 + \log(2\pi r^2/(n-p))]$, labeled
\code{lprssRE}, decreases as $\theta$ increases but its range is too
narrow to move $\widehat{\theta_R}$ away from zero.

\section{Conditional means and variances
  of the random effects}
\label{sec:condmean}

We have discussed estimates of the variance components and of the
fixed-effects parameters in a linear mixed model but we have not yet
considered ``estimates'' of the random effects themselves.  As
mentioned in section~\ref{sec:memod}, it is inaccurate to speak of
estimating the random effects because they are not parameters of the
probability model described in the last section.  In that model, the
random effects, $\bc B$, are unobserved random variables.  

From the unconditional distribution, $\bc B\sim\mathcal{N}(\bm0,
\sigma^2\bm\Lambda_\theta\bm\Lambda\trans_\theta)$ and the
conditional distribution, $(\bc Y|\bc B=\bm b)\sim\mathcal{N}(\bm Z\bm
b+\bm X\bm\beta,\sigma^2\bm I_n)$ we can determine the conditional
distribution of the random effects given the observed data, written
$(\bc B|\bc Y=\bm y)$.

Like the distributions of $\bc B$ and $(\bc Y|\bc B=\bm b)$, the
conditional distribution $(\bc B|\bc Y=\bm y)$ is a multivariate
Gaussian (or ``normal'') distribution that depends on the (unknown)
values of the parameters in the model and on the observed data vector
$\bm y$. We can characterize such a multivariate Gaussian distribution
by providing the conditional mean, $\bm\mu_{\bc B|\bc Y=\bm y}$, and
the conditional variance-covariance matrix, $\text{Var}(\bc B|\bc
Y=\bm y)$.  These quantities depend upon the model parameters and we
evaluate them at the estimated values of the parameters.

The components of the conditional mean vector,  $\bm\mu_{\bc
  B|\bc Y=\bm y}$, evaluated at the parameter estimates are sometimes
called the \emph{Best Linear Unbiased Predictors}, or BLUPs, of the
random effects.  Although the acronym is attractive I don't find this
description particularly informative (What is a ``linear unbiased
predictor'' and in what sense are these the ``best''?) and prefer to
describe these values as the conditional mean vector.

The conditional mean of the random effects given the observed data
depends on the parameters $\bm\theta$ and $\bm\beta$ but not on
$\sigma^2$.  It is easier to derive expressions for this conditional
mean if we first consider an intermediate random variable, $\bc U$, with
unconditional distribution
\begin{equation}
  \label{eq:Uunconddist}
  \bc U\sim\mathcal{N}(\bm 0, \sigma^2\bm I_q) .
\end{equation}
Because the unconditional distribution of $\bc U$ is a spherical
multivariate Gaussian we refer to these as the spherical or ``unit''
random effects.

The relationship between $\bc U$ and $\bc B$,
\begin{equation}
  \label{eq:BLambdaU}
  \bc B=\bm\Lambda_\theta\bc U,
\end{equation}
provides the required distribution $\bc B\sim\mathcal{N}(\bm
0,\sigma^2\bm\Lambda_\theta\bm\Lambda\trans_\theta$.

If we knew the values of the parameters $\bm\theta$ and $\bm\beta$
then we could evaluate the conditional mean of the spherical random
effects, $\bm\mu_{\bc U|\bc Y=\bm y}$, as
\begin{equation}
  \label{eq:condmeanU}
  (\bm U\trans_\theta\bm U+\bm I_q)\bm\mu_{\bc U|\bc Y=\bm y} =
  \bm L_\theta\bm L\trans_\theta\bm\mu_{\bc U|\bc Y=\bm y}=
  \bm U\trans_\theta\left(\bm y - \bm X\bm\beta\right) .
\end{equation}

At the estimated parameter values the conditional mean is $\bm\mu_{\bc
  U|\bc Y=\bm y}=\tilde{\bm u}(\widehat{\bm\theta})$.  That is, the
solution of the penalized least squares problem (\ref{eq:jointmin})
provides both the conditional estimate of the fixed effects
parameters, $\widehat{\bm\beta}_\theta$, and the conditional mean
of the spherical random effects, $\tilde{\bm u}_\theta$, for a
given value of $\bm\theta$.

The conditional mean of $\bc B$, evaluated at the parameter estimates,
is 
\begin{equation}
  \label{eq:condmeanB}
  \bm\mu_{\bc B|\bc Y=\bm y} = \bm\Lambda(\widehat{\bm\theta})
  \tilde{\bm u}(\widehat{\bm\theta}) .
\end{equation}
The conditional variance of the spherical random effects is
\begin{equation}
  \label{eq:condVarU}
  \text{Var}(\bc U|\bc Y=\bm y)=\sigma^2
  \left(\bm L_\theta\bm L\trans_\theta\right)^{-1},
\end{equation}
hence 
\begin{equation}
  \label{eq:condVarB}
  \text{Var}(\bc B|\bc Y=\bm y)=\sigma^2
  \bm\Lambda_\theta
  \left(\bm L_\theta\bm L\trans_\theta\right)^{-1}
  \bm\Lambda\trans_\theta .
\end{equation}

In the simple model for the \code{Dyestuff} data that we have been
considering the matrices $\bm\Lambda(\theta)$ and $\bm L(\theta)$ are
both diagonal --- in fact, they are multiples of the identity matrix
--- so it is easy to compute $\left(\bm L(\theta)\bm
  L\trans_\theta\right)^{-1}$.  In general, though, it is not easy
to compute the inverse of a large, sparse matrix and we do not attempt
to do so.  Instead we evaluate the diagonal elements of
(\ref{eq:condVarB}) at the parameter estimates and use those to form
prediction intervals on the random effects.

The conditional means and the conditional variances can be combined to
provide a prediction interval on the random effect, given the observed
response.  Like many other quantities in this discussion we would need
to evaluate these at the unknown, ``true'' parameter values to obtain
the theoretical prediction intervals.  In practice, we evaluate these
intervals using the estimated values of the parameters in the model.

Figure~\ref{fig:fm1ranef} 
\begin{figure}[tbp]
  \centering
<<fm1ranef,fig=TRUE,echo=FALSE,height=2.5>>=  
print(dotplot(ranef(fm1, postVar = TRUE), strip = FALSE)[[1]])
@ 
\caption{95\% prediction intervals on the random effects for
  \code{Batch} in model \code{fm1} fit to the \code{Dyestuff} data.}
  \label{fig:fm1ranef}
\end{figure}
shows 95\% prediction intervals on the random effects for \code{Batch}
in model \code{fm1} fit to the \code{Dyestuff} data.  As in the
\code{Dyestuff} data plot (Figure~\ref{fig:Dyestuffdot}) we have
reordered the levels of the \code{Batch} factor.  For the plot of the
prediction intervals the ordering is by increasing conditional mean of
the random effects.  Because model \code{fm1} is a simple model fit to
a balanced data set this ordering is exactly the same as the ordering
in the data plot based on increasing mean response.

\section{The ``variance components'' in more detail}
\label{sec:VarComp}

Although prediction intervals on the random effects are often useful
in allowing us to visualize the model, we are usually more interested
in estimating the level of the variability in the response induced by
the selection of experimental or observational units.  The parameter
$\sigma^2$ in the model
\begin{equation}
  \label{eq:LMM}
  \bc B\sim\mathcal{N}\left(\bm 0,
    \sigma^2\bm\Lambda_\theta\bm\Lambda\trans_\theta\right),\:
  (\bc Y|\bc B=\bm b)\sim\mathcal{N}\left(\bm Z\bm b+\bm X\bm\beta,
    \sigma^2\bm I\right)
\end{equation}
is the variance of the residual or per-observation noise.  The
conditional maximum likelihood estimate of $\sigma^2$ is
\begin{equation}
  \label{eq:LMMsigmaML}
  \widehat{\sigma^2}_{\mathit{ML}}=\frac{r^2_\theta}{n}
\end{equation}
and the conditional REML estimate is
\begin{equation}
  \label{eq:LMMsigmaREML}
  \widehat{\sigma^2}_{\mathit{R}}=\frac{r^2_\theta}{n-p} .
\end{equation}

In Figure~\ref{fig:sigmavtheta}
\begin{figure}[tbp]
  \centering
<<sigmavtheta,fig=TRUE,echo=FALSE,height=3.5>>=
print(xyplot(n + nmp ~ theta, res,
             outer = FALSE,
             xlab = expression(theta),
             ylab = expression(hat(sigma)(theta)),
             scales = list(x = list(axs = 'i')),
             auto.key = list(columns = 2,
             text = c("ML", "REML"), lines = TRUE,
             points = FALSE),
             type = c("g","l")))
@   
  \caption[Conditional ML and REML estimates of $\sigma$ versus
  $\theta$]{The conditional ML and REML estimates of $\sigma$ as a
    function of $\theta$ for model \code{fm1}}
  \label{fig:sigmavtheta}
\end{figure}
we show the ML and REML estimates of $\sigma$ as a function of
$\theta$ for the model \code{fm1}.  As can be seen from comparing
(\ref{eq:LMMsigmaML}) to (\ref{eq:LMMsigmaREML}), at a particular
value of $\theta$ these two estimates are multiples of one another.

In a simple model such as \code{fm1} where the unconditional
variance-covariance matrix of $\bc B$ is a multiple of the identity,
\begin{equation}
  \label{eq:uncondvar}
  \text{Var}(\bc B)=
  \sigma^2\bm\Lambda(\theta)\Lambda\trans(\theta)=\sigma^2\theta^2\bm I_q
  \sigma^2_B\bm I_q,
\end{equation}
where $\sigma^2_B=\sigma^2\theta^2$,
we can examine the deviance or the REML criterion as a function of
$\sigma$ and $\sigma_B$.  In Figure~\ref{fig:devcontoursfm1}
\begin{figure}[tbp]
  \centering
<<devcontoursfm1,fig=TRUE,echo=FALSE,height=5.3>>=
profsd <- t(apply(deviance, 1, gmin, x = sigmas))
profsbd <- t(apply(deviance, 2, gmin, x = sigB))
profsR <- t(apply(REML, 1, gmin, x = sigmas))
profsbR <- t(apply(REML, 2, gmin, x = sigB))
print(contourplot(deviance + REML ~ sigB * sigma, grd1,
                  xlab = expression(sigma[B]),
                  ylab = expression(sigma),
                  aspect = 1,
                  at = 
                  qchisq(c(0.5,0.8,0.9,0.95,0.99,0.999), df = 2),
                  labels = list(labels = paste(c(50,80,90,95,99,99.9),
                                "%", sep = "")),
                  panel = function(...)
              {
                  if (panel.number() == 1) {
                      VC <- VCML
                      prs <- profsd
                      prsb <- profsbd
                  } else {
                      VC <- VCREML
                      prs <- profsR
                      prsb <- profsbR
                  }
                  panel.points(attr(VC[[1]],"stddev"),
                               attr(VC,"sc"), pch = 3)
                  panel.grid(h = -1, v = -1)
                  panel.lines(sigB, prs[,1], lty = 2)
                  panel.lines(prsb[,1], sigmas, lty = 3)                  
                  panel.contourplot(...)
              }
                  ))
@   
\caption[Contours of the deviance and the REML criterion for
\code{fm1}]{Contours of the deviance and of the REML criterion for
  model \code{fm1} as a function of $\sigma$ and $\sigma_B$.  The
  levels of the contours correspond to the boundaries of nominal 50\%,
  80\%, 90\%, 95\%, 99\% and 99.9\% joint confidence regions, based on
  a likelihood ratio test statistic.  The \code{"+"} symbol designates
  the estimated values of these parameters.  The lines through the
  estimate are the ``profile traces'' of each parameter on the other.
  The dashed line shows the conditional estimate of $\sigma$, given a
  value of $\sigma_B$.  The dotted line shows the conditional estimate
  of $\sigma_B$, given a value of $\sigma$.}
  \label{fig:devcontoursfm1}
\end{figure}
we show contours of the profiled deviance and the profiled REML
criterion as functions of $\sigma_B$ and $\sigma$.  In this figure we
also show the \emph{profile traces}, which are the conditional estimates
of one parameter, given the other.

The deviance surface itself is shown in Figure~\ref{fig:wireframefm1}.
\begin{figure}[tbp]
  \centering
<<wireframefm1,fig=TRUE,echo=FALSE,height=5>>=
print(wireframe(deviance ~ sigB * sigma,
                grd1, shade = TRUE,
                xlab = expression(sigma[B]),
                ylab = expression(sigma)))
@   
\caption[Profiled deviance surface for model \code{fm1}]{Profiled
  deviance surface for model \code{fm1} as a function of $\sigma$ and
  $\sigma_B$.  Although the $\sigma$ and $\sigma_B$ axes are not
  labeled with the values of these parameters, their extents are the
  same as in Figure~\ref{fig:devcontoursfm1}.}
  \label{fig:wireframefm1}
\end{figure}
If we were to consider the profile of the bottom of that surface, as
seen from the $\sigma_B$ axis, we would be looking at the minimum
value of the deviance for each value of $\sigma_B$, hence the name
``profiled deviance''.  From the other direction, we would see the
minimum value of the deviance for each value of $\sigma$.  These
profiled deviance curves are shown in Figure~\ref{fig:profileddevfm1}.
\begin{figure}[tbp]
  \centering
<<profileddevfm1,fig=TRUE,echo=FALSE,height=4>>=
print(xyplot(dev ~ x|par,
             data.frame(dev = c(profsbd[,2], profsd[,2]),
                        x = c(sigmas, sigB),
                        par = gl(2, length(sigmas))),
             strip = function(..., factor.levels)
             strip.default(..., factor.levels = c(expression(sigma),
                                expression(sigma[B]))),
             ylab = "Deviance", xlab = NULL, type = c("g","l"),
             scales = list(x = list(relation = "free", axs = 'i'))))
@   
  \caption{Profiled deviance in model \code{fm1} for $\sigma$ and $\sigma_B$.}
  \label{fig:profileddevfm1}
\end{figure}

We have added a third panel to Figure~\ref{fig:profileddevfm1} showing
the profiled deviance as a function of the fixed-effects parameter
$\beta$.  There are, in fact, three distinct parameters in the model.
During optimization we use the parameters $\theta$, $\beta$ and
$\sigma$ because we can determine the conditional estimates of $\beta$
and $\sigma$ for a fixed value of $\theta$ and thereby reduce the
complexity of the problem of optimizing the parameter estimates.  Most
summaries of the model, however, are phrased in terms of the
parameters $\sigma^2_B$, $\sigma^2$ and $\beta$.

The point of all of this exploration of the profiled deviance is to
examine the shape of the profiled deviance curves as functions of
these parameters.  It is common when fitting linear mixed models to
summarize the parameter estimates according to the fixed-effects
parameter estimates, $\widehat{\bm\beta}$, and their standard errors
plus the estimates of the ``variance components'',
$\widehat{\sigma^2_B}$ and $\widehat{\sigma^2}$, and their standard
errors.  \citet{west07:_linear_mixed_model}, for example, provide a
guide to using common statistical software for fitting linear mixed
models, and all of the software that they demonstrate, with the
notable exception of the \code{lme} function in the \package{nlme}
package~\citep{pinheiro00:_mixed_effec_model_in_s} for \R{}, provide
exactly this summary of the variance components.

Implicit in such a summary is the assumption that the profiled
deviance for a parameter should be approximately quadratic over the
region of interest.  In Figure~\ref{fig:profileddevfm1} we can see
that the profiled deviance is not close to being a quadratic function
of $\sigma$ or $\sigma_B$.  Expressed as a function of $\sigma^2$ and
$\sigma_B^2$ (Figure~\ref{fig:profileddevsqfm1}),
\begin{figure}[tbp]
  \centering
<<profileddevsqfm1,fig=TRUE,echo=FALSE,height=4>>=
print(xyplot(dev ~ x|par,
             data.frame(dev = c(profsbd[,2], profsd[,2]),
                        x = c(sigmas^2, sigB^2),
                        par = gl(2, length(sigmas))),
             strip = function(..., factor.levels)
             strip.default(..., factor.levels = c(expression(sigma^2),
                                expression(sigma[B]^2))),
             ylab = "Deviance", xlab = NULL, type = c("g","l"),
             scales = list(x = list(relation = "free", axs = 'i'))))
@   
  \caption{Profiled deviance in model \code{fm1} for $\sigma^2$ and $\sigma_B^2$.}
  \label{fig:profileddevsqfm1}
\end{figure}
the profiled deviance is even further from quadratic behavior.

Of course, it is important to consider how close to quadratic the
profiled deviance should be and what do we consider to be ``the region
of interest''.  To answer these questions let us consider a
transformation of the profiled deviance, of the form discussed in
\citet[Ch.~6]{bates88:_nonlin_regres_analy_and_its_applic}, called the
``signed square root'' transformation.  For a particular parameter,
which we will write as $\phi$, we are interested in the difference
between the profiled deviance at $\phi=\phi_0$ and the minimum value
of the deviance.  If we take the square root of that difference, which
must be non-negative, a quadratic profiled deviance would correspond
to a ``V-shaped'' pattern like the absolute value function.  If we
take the negative of the square root on the left side of the estimate
and the positive square root on the right side of the estimate, a
quadratic profiled deviance corresponds to a straight line.
Furthermore, the inverse of the slope of that straight line is the
standard error of the parameter.
In~\citet{bates88:_nonlin_regres_analy_and_its_applic} the transformed
change in the profiled sum of squared residuals for a nonlinear
regression model is called the \emph{profile t} statistic because it
corresponds to the \emph{t} statistic for linear models.  The
corresponding quantity here is more like a \emph{profile z} statistic
because it comes from a likelihood ratio test, not an F test.

\begin{figure}[tbp]
  \centering
%% FIXME: This figure is wrong, currently.  
<<profzfm1,fig=TRUE,echo=FALSE,height=4>>=
print(xyplot(profz ~ x|par,
             data.frame(profz = sqrt(c(profsbd[,2], profsd[,2])-
                        deviance(fm1ML)),
                        x = c(sigmas^2, sigB^2),
                        par = gl(2, length(sigmas))),
             strip = function(..., factor.levels)
             strip.default(..., factor.levels = c(expression(sigma^2),
                                expression(sigma[B]^2))),
             ylab = "Profile z", xlab = NULL, type = c("g","l"),
             scales = list(x = list(relation = "free", axs = 'i'))))
@   
\caption{Profile z statistic in model \code{fm1} for $\sigma$ and
  $\sigma_B$.}
  \label{fig:profzfm1}
\end{figure}


The latter part of this section covers
some of the finicky but important details of the matrix representation
of the model and the computational methods employed.  Readers who
would prefer to avoid exposure to such details should feel free to do
so.

\subsection{A penalized least squares problem}
\label{sec:PLS}

The general form of a linear mixed model incorporates two parameter
vectors, $\bm\beta$ and $\bm\theta$, and one scalar parameter,
$\sigma^2$.  To evaluate the maximum likelihood estimates,
$\widehat{\bm\beta}$, $\widehat{\bm\theta}$ and $\widehat{\sigma^2}$,
we could attempt to optimize the likelihood of the parameters, given
the model and the observed data, with respect to all of these
parameters simultaneously.  However, for this particular model we can
simplify the optimization problem because, for any value of
$\bm\theta$, we can determine the conditional estimates,
$\widehat{\bm\beta}_\theta$ and $\widehat{\sigma^2}_\theta$,
of the other parameters in a direct computation.

Furthermore, this computation, determining the solution of the penalized
linear least squares problem
\begin{equation}
  \label{eq:PLS}
  \min_{\bm u,\bm\beta}
  \left\|
    \begin{bmatrix}\bm y\\\bm 0\end{bmatrix} -
    \begin{bmatrix}
      \bm Z\bm\Lambda_\theta & \bm X\\
      \bm I_q & \bm 0
    \end{bmatrix}
    \begin{bmatrix}\bm u\\\bm\beta\end{bmatrix}
  \right\|^2 =
  \min_{\bm u,\bm\beta}\left(
    \left\|\bm y - \bm Z\bm\Lambda_\theta\bm u -
      \bm X\bm\beta\right\|^2 + \|\bm u\|^2\right)
\end{equation}
as the vectors, $\widehat{\bm\beta}_\theta$ and $\tilde{\bm u}$,
that satisfy
\begin{equation}
  \label{eq:plsSol}
  \begin{bmatrix}
    \bm\Lambda_\theta\trans\bm Z\trans\bm
    Z\bm\Lambda_\theta+\bm I_q&\bm\Lambda_\theta\trans\bm
    Z\trans\bm X\\
    \bm X\trans\bm Z\bm\Lambda_\theta&\bm X\trans\bm X
  \end{bmatrix}
  \begin{bmatrix}\tilde{\bm u}\\\widehat{\bm\beta}_\theta\end{bmatrix}
  =
  \begin{bmatrix}
    \bm\Lambda_\theta\trans\bm Z\trans\bm y\\\bm X\trans\bm y
  \end{bmatrix} ,
\end{equation}
provides, as we shall see, the conditional mean, $\bm\mu_{\bc B|\bm
  Y=\bm y}$, and conditional variance of the random effects, $\bc B$,
given the observed data, $\bc Y=\bm y$, plus a concise expression for
the log-likelihood and the REML criterion.

An effective way of determining the solution to (\ref{eq:plsSol}) is
to create the Cholesky decomposition of the matrix on the left.  In
considering this step, however, we must bear in mind that the matrix
$\bm Z$ is quite sparse and, in some cases, can be very large.  I have
myself fit linear mixed models in which both the number of rows and
the number of columns in $\bm Z$ exceeded one million.  

Dealing with matrices of this size requires considerable care in
deciding how to structure the computation.  The key to fitting mixed
models with a complex structure to very large data sets using the
\package{lme4} package is the ability to determine the sparse Cholesky
factor of $\bm\Lambda_\theta\trans\bm Z\trans\bm
Z\bm\Lambda_\theta+\bm I_q$ for many different values of
$\bm\theta$.

The Cholesky decomposition of large, sparse matrices like this has an
interesting characteristic in that reordering the rows and columns of
the matrix can change, sometimes dramatically, the number of nonzero
elements in the factor. Decreasing the number of nonzeros in the
factor will increase the speed of the decomposition, which is
important when this needs to be done for many values of $\bm\theta$.
We will write a \emph{fill-reducing permutation}, determined in this
case by the Approximate Minimal Degree algorithm~\citep{Davis:1996},
as a $q\times q$ permutation matrix, $\bm P$.  (In practice it is only
the permutation, which is a reordering of the numbers $1$ to $q$, that
is created and stored but it is convenient to represent this
permutation as a matrix in the formulas.)  The permutation depends
only on the positions of the nonzeros in the matrix $\bm
Z\bm\Lambda(\bm\theta_0)$ where $\bm\theta_0$ is an initial estimate
or ``starting value'' for the parameter $\bm\theta$.

To allow for later generalizations of the linear mixed model we write
the decomposition as
\begin{equation}
  \label{eq:bigChol}
  \begin{bmatrix}
    \bm L            & \bm 0\\
    \bm R_{ZX}\trans  & \bm R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L\trans   & \bm R_{ZX}\\
    \bm 0         & \bm R_X
  \end{bmatrix} =
  \begin{bmatrix}
    \bm U\trans\bm U+\bm I_q & \bm U\trans\bm V\\
    \bm V\trans\bm U         & \bm V\trans\bm V
  \end{bmatrix}
\end{equation}
where
\begin{equation}
  \label{eq:UV}
  \bm U_\theta=\bm Z\bm\Lambda_\theta\bm P\trans\text{ and }
  \bm V=\bm X .
\end{equation}
The solution to (\ref{eq:plsSol}), which would now be written as,
\begin{equation}
  \label{eq:UVplsSol}
  \begin{bmatrix}
    \bm L            & \bm 0\\
    \bm R_{ZX}\trans  & \bm R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L\trans   & \bm R_{ZX}\\
    \bm 0         & \bm R_X
  \end{bmatrix} =
  \begin{bmatrix}\tilde{\bm u}\\\widehat{\bm\beta}_\theta\end{bmatrix}
  =
  \begin{bmatrix}
    \bm U\trans\bm y\\\bm V\trans\bm y
  \end{bmatrix} ,
\end{equation}
produces both the conditional estimate,
$\widehat{\bm\beta}_\theta$, of the fixed-effects parameters and
the conditional mean $\bm\mu_{\bc B|\bc Y=\bm y}=\bm\Lambda_\theta\tilde{\bm
  u}$, of the random effects.  If $r^2$ is the minimum penalized
residual sum of squares,
\begin{equation}
  \label{eq:minpls}
  r^2_\theta=\|\bm y-\bm U\tilde{\bm u}_\theta-
  \bm V\widehat{\bm\beta}_\theta\|^2+\|\tilde{\bm u}_\theta\|^2
\end{equation}
then the conditional maximum likelihood estimate of $\sigma^2$ is
$\widehat{\sigma^2}_\theta=\frac{r^2}{n}$ and the conditional REML
estimate of $\sigma^2$ is
$\widehat{\sigma^2}_R(\theta)=\frac{r^2}{n-p}$.

Although we have not written this explicitly, the matrices $\bm L$,
$\bm R_{ZX}$ and $\bm R_X$ all depend on the value of $\bm\theta$.
Both the $q\times q$ matrix $\bm L_\theta$ and the $p\times p$
matrix $\bm R_X(\theta)$ are triangular: $\bm L_\theta$ is
lower triangular and $\bm R_x(\theta)$ is upper triangular.
Because these matrices are square and triangular their determinants,
written $|\bm L|$ and $|\bm R_X|$, are easily evaluated as the product
of the diagonal elements in the matrix.  The determinant of a matrix
is the measure of the size of the matrix that is used in evaluating
probability densities of random variables after transformation.
