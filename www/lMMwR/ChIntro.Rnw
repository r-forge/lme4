\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=8,strip.white=TRUE}
\SweaveOpts{keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Intro,include=TRUE}
\setkeys{Gin}{width=\textwidth}

<<preliminaries,echo=FALSE,print=FALSE,results=hide>>=
options(width=85, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
library(splines)
library(lattice)
library(Matrix)
library(lme4a)
fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff)
fm1ML <- update(fm1, REML = FALSE)
if (file.exists("pr1.rda")) {
    load("pr1.rda")
} else {
    pr1 <- profile(fm1ML, delta = 0.2)
    save(pr1, file = "pr1.rda")
}
@ 
\chapter{A Simple, Linear, Mixed-effects Model}
\label{chap:ExamLMM}

In this book we describe the theory behind a type of statistical model
called \emph{mixed-effects} models and the practice of fitting and
analyzing such models using the \package{lme4} package for \R{}.
These models are used in many different disciplines.  Because the
descriptions of the models can vary markedly between disciplines, we
begin by describing what mixed-effects models are and by exploring a
very simple example of one type of mixed model, the \emph{linear mixed
  model}.

%% FIXME: Review this paragraph once the chapter is complete
This simple example allows us to illustrate the use of the \code{lmer}
function in the \package{lme4} package for fitting such models and
analyzing the fitted model.  Building from the example we describe the
general form of linear mixed models that can be fit using \code{lmer}.

\section{Mixed-effects Models}
\label{sec:memod}

Mixed-effects models, like many other types of statistical models,
describe a relationship between a \emph{response} variable and some of
the \emph{covariates} that have been measured or observed along with
the response.  In mixed-effects models at least one of the covariates
is a \emph{categorical} covariate representing experimental or
observational ``units'' in the data set.  In the example from the
chemical industry that is given in this chapter, the observational
unit is the batch of an intermediate product used in production of a
dye.  In medical and social sciences the observational units are often
the human or animal subjects in the study.  In agriculture the
experimental units may be the plots of land or the specific plants
being studied.

In all of these cases the categorical covariate or covariates are
observed at a set of discrete \emph{levels}.  We may use numbers, such
as subject identifiers, to designate the particular levels that we
observed but these numbers are simply labels.  The important
characteristic of a categorical covariate is that, at each observed
value of the response, the covariate takes on the value of one of a
set of distinct levels.

Parameters associated with the particular levels of a covariate are
sometimes called the ``effects'' of the levels.  If the set of
possible levels of the covariate is fixed and reproducible we model
the covariate using \emph{fixed-effects} parameters.  If the levels
that we observed represent a random sample from the set of all
possible levels we incorporate \emph{random effects} in the model.

There are two things to notice about this distinction between
fixed-effects parameters and random effects.  First, the names are
misleading because the distinction between fixed and random is more a
property of the levels of the categorical covariate than a property of
the effects associated with them.  Secondly, we distinguish between
``fixed-effects parameters'', which are indeed parameters in the
statistical model, and ``random effects'', which, strictly speaking,
are not parameters.  As we will see shortly, random effects are
unobserved random variables.

To make the distinction more concrete, suppose that we wish to model
the annual reading test scores for students in a school district and
that the covariates recorded with the score include a student
identifier and the student's gender. Both of these are categorical
covariates.  The levels of the gender covariate, male and female, are
fixed.  If we consider data from another school district or we
incorporate scores from earlier tests, we will not change those
levels.  On the other hand, the students whose scores we observed
would generally be regarded as a sample from the set of all possible
students whom we could have observed.  Adding more data, either from
more school districts or from results on previous or subsequent tests,
will increase the number of distinct levels of the student identifier.

\emph{Mixed-effects models} or, more simply, \emph{mixed models} are
statistical models that incorporate both fixed-effects parameters and
random effects.  Because of the way that we will define random
effects, a model with random effects always includes at least one
fixed-effects parameter.  Thus, any model with random effects is a
mixed model.

We characterize the statistical model in terms of two random
variables: a $q$-dimensional vector of random effects represented by
the random variable $\bc B$ and an $n$-dimensional response
vector represented by the random variable $\bc Y$.  We
observe the value, $\vec y$, of $\bc Y$.  We do not observe
the value of $\bc B$.

When formulating the model we describe the unconditional distribution
of $\bc B$ and the conditional distribution, $(\bc
  Y|\bc B=\vec b)$.  The descriptions of the distributions
involve the form of the distribution and the values of certain
parameters.  We use the observed values of the response and the
covariates to estimate these parameters and to make inferences about
them.

That the big picture.  Now let's make this more concrete by describing
a particular, versatile class of mixed models called linear mixed
models and by studying a simple example of such a model.  First we
will describe the data in the example.

\section{The \texttt{Dyestuff} and
  \texttt{Dyestuff2} Data}
\label{sec:DyestuffData}

Models with random effects have been in use for a long time.  The
first edition of the classic book, \emph{Statistical Methods in
  Research and Production}, edited by O.L. Davies, was published in
1947 and contained examples of the use of random effects to
characterize batch-to-batch variability in chemical processes.  The
data from one of these examples are available as the \code{Dyestuff}
data in the \package{lme4} package. In this section we describe and
plot these data and introduce a second example, the \code{Dyestuff2}
data, described in \citet{box73:_bayes_infer_statis_analy}.

\subsection{The \texttt{Dyestuff} Data}
\label{sec:dyestuff}

The \code{Dyestuff} data are described in \citet[Table~6.3,
p.~131]{davies72:_statis_method_in_resear_and_produc}, the fourth
edition of the book mentioned above, as coming from
\begin{quote}
  an investigation to find out how much the
   variation from batch to batch in the quality of an intermediate
   product (H-acid) contributes to the variation in the yield of the
   dyestuff (Naphthalene Black 12B) made from it.  In the experiment six
   samples of the intermediate, representing different batches of works
   manufacture, were obtained, and five preparations of the dyestuff
   were made in the laboratory from each sample.  The equivalent yield
   of each preparation as grams of standard colour was determined by
   dye-trial.
\end{quote}

To access these data within \R{} we must first attach the \code{lme4}
package to our session using
<<lme4,eval=FALSE>>=
library(lme4)
@
Note that the \code{"$>$"} symbol in the line shown is the prompt in
\R{} and not part of what the user types. The \package{lme4} package
must be attached before any of the data sets or functions in the
package can be used.  If typing this line results in an error report
stating that there is no package by this name then you must first
install the package.

In what follows, we will assume that the \package{lme4} package has
been installed and that it has been attached to the \R{} session
before any of the code shown has been run.

The \code{str} function in \R{} provides a concise description of the
structure of the data
<<strDyestuff>>=
str(Dyestuff)
@
from which we see that it consists of $30$ observations of the
\code{Yield}, the response variable, and of the covariate,
\code{Batch}, which is a categorical variable stored as a
\code{factor} object.  If the labels for the factor levels are
arbitrary, as they are here, we will use letters instead of numbers
for the labels.  That is, we label the batches as \code{"A"} through
\code{"F"} rather than \code{"1"} through \code{"6"}.  When the labels are
letters it is clear that the variable is categorical.  When the labels
are numbers a categorical covariate can be mistaken for a numeric
covariate, with unintended consequences.

It is a good practice to apply \code{str} to any data frame the first
time you work with it and to check carefully that any categorical
variables are indeed represented as factors.

The data in a data frame are viewed as a table with columns
corresponding to variables and rows to observations. The functions
\code{head} and \code{tail} print the first or last few rows
(the default value of ``few'' happens to be $6$ but we can specify
another value if we so choose)
<<headDyestuff>>=
head(Dyestuff)
@ 
or we could ask for a \code{summary} of the data
<<summaryDyestuff>>=
summary(Dyestuff)
@ 

\begin{figure}[tbp]
  \centering
<<Dyestuffdot,fig=TRUE,echo=FALSE,height=3.5>>=
set.seed(1234543)
print(dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff,
              ylab = "Batch", jitter.y = TRUE, pch = 21,
              xlab = "Yield of dyestuff (grams of standard color)",
              type = c("p", "a")))
@   
\caption[Yield of dyestuff from 6 batches of an intermediate]{Yield of
  dyestuff (Napthalene Black 12B) for 5 preparations from each of 6
  batches of an intermediate product (H-acid).  The line joins the
  mean yields from the batches, which have been ordered by increasing
  mean yield.  The vertical positions are ``jittered'' slightly to
  avoid over-plotting.  Notice that the lowest yield for batch A was
  observed for two distinct preparations from that batch.}
  \label{fig:Dyestuffdot}
\end{figure}
Although the \code{summary} does show us an important property of the
data, namely that there are exactly $5$ observations on each batch --- a
property that we will describe by saying that the data are
\emph{balanced} with respect to \code{Batch} --- we usually learn much
more about the structure of such data from plots like
Figure~\ref{fig:Dyestuffdot} than we can from
numerical summaries.

In Figure~\ref{fig:Dyestuffdot} we can see that there is considerable
variability in yield, even for preparations from the same batch, but
there is also noticeable batch-to-batch variability.  For example,
four of the five preparations from batch F provided lower yields than
did any of the preparations from batches C and E.  

This plot, and essentially all the other plots in this book,
were created using Deepayan Sarkar's \package{lattice} package for
\R{}.  In \citet{sarkar08:_lattic} he describes how one would create
such a plot.  Because this book was created using Sweave
\citep{lmucs-papers:Leisch:2002}, the exact code used to create the
plot, as well as the code for all the other figures and calculations
in the book, is available on the web site for the book.  In
section~\ref{sec:lattice} we review some of the principles of
lattice graphics, such as reordering the levels of the \code{Batch}
factor by increasing mean response, that enhance the informativeness
of the plot.  At this point we will concentrate on the information
conveyed by the plot and not on how the plot is created.

In section \ref{sec:DyestuffLMM} we will use mixed models to
quantify the variability in yield between batches.  For the time being
let us just note that the particular batches used in this experiment
are a selection or sample from the set of all batches that we wish to
consider.  Furthermore, the extent to which one particular batch tends
to increase or decrease the mean yield of the process --- in other
words, the ``effect'' of that particular batch on the yield --- is not
as interesting to us as is the extent of the variability between
batches.  For the purposes of designing, monitoring and controlling a
process we want to predict the yield from future batches, taking into
account the batch-to-batch variability and the within-batch
variability.  Being able to estimate the extent to which a particular
batch in the past increased or decreased the yield is not usually an
important goal for us.  We will model the effects of the batches as
random effects rather than as fixed-effects parameters.

\subsection{The \texttt{Dyestuff2} Data}
\label{sec:dyestuff2}

The \code{Dyestuff2} data are simulated data presented in \citet[Table
5.1.4, p. 247]{box73:_bayes_infer_statis_analy} where the authors state
\begin{quote}
  These data had to be constructed for although examples of this sort
  undoubtedly occur in practice they seem to be rarely published.
\end{quote}
The structure and summary
<<strDye2>>=
str(Dyestuff2)
summary(Dyestuff2)
@
are intentionally similar to those of the \code{Dyestuff} data.  As
can be seen in Figure~\ref{fig:Dyestuff2dot}
\begin{figure}[tbp]
  \centering
<<Dyestuff2dot,fig=TRUE,echo=FALSE,height=3.5>>=
print(dotplot(reorder(Batch, Yield) ~ Yield, Dyestuff2,
              ylab = "Batch", jitter.y = TRUE, pch = 21,
              xlab = "Simulated response (dimensionless)",
              type = c("p", "a")))
@   
\caption[Simulated data similar in structure to the \code{Dyestuff}
data]{Simulated data presented in
  \citet{box73:_bayes_infer_statis_analy} with a structure similar to
  that of the \code{Dyestuff} data.  These data represent a case where
  the batch-to-batch variability is small relative to the within-batch
  variability.}
  \label{fig:Dyestuff2dot}
\end{figure}
the batch-to-batch variability in these data is small compared to the
within-batch variability.  In some approaches to mixed models it can
be difficult to fit models to such data.  Paradoxically, small
``variance components'' can be more difficult to estimate than large
variance components.

The methods we will present are not compromised when estimating small
variance components.

\section{Fitting Linear Mixed Models}
\label{sec:FittingLMMs}

Before we formally define a linear mixed model, let's go ahead and fit
models to these data sets using \code{lmer}.  Like most model-fitting
functions in \R{}, \code{lmer} takes, as its first two arguments, a
\emph{formula} specifying the model and the \emph{data} with which to
evaluate the formula.  This second argument, \code{data}, is optional
but recommended.  It is usually the name of a data frame, such as
those we examined in the last section. Throughout this book all model
specifications will be given in this formula/data format.

We will explain the structure of the formula after we have
considered an example.
\subsection{A Model For the \texttt{Dyestuff} Data}
\label{sec:DyestuffLMM}

We fit a model to the \code{Dyestuff} data allowing for an overall
level of the \code{Yield} and for an additive random effect for each
level of \code{Batch}
<<fm1>>=
fm1 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff)
print(fm1)
@ 
In the first line we call the \code{lmer} function to fit a model with formula
<<fm1formula,echo=FALSE>>=
Yield ~ 1 + (1|Batch)
@
applied to the \code{Dyestuff} data and assign the result to the
name \code{fm1}.  (The name is arbitrary.  I happen to use names that
start with \code{fm}, indicating ``fitted model''.)

As is customary in \R{}, there is no output shown after this
assignment.  We have simply saved the fitted model as an object named
\code{fm1}.  In the second line we display some information about the
fitted model by applying \code{print} to \code{fm1}.  In later
examples we will condense these two steps into one but here it helps
to emphasize that we save the result of fitting a model then apply
various \emph{extractor} functions to the fitted model to get a brief
summary of the model fit or to obtain the values of some of the
estimated quantities.

\subsubsection{Details of the Printed Display}
\label{sec:printedDetails}

The printed display of a model fit with \code{lmer} has four major
sections: a description of the model that was fit, some statistics
characterizing the model fit, a summary of properties of the random
effects and a summary of the fixed-effects parameter estimates.  We
consider each of these sections in turn.

The description section states that this is a linear mixed model in
which the parameters have been estimated as those that minimize the
REML criterion (explained in section \ref{sec:REML}).  The
\code{formula} and \code{data} arguments are displayed for later
reference.  If other, optional arguments affecting the fit, such as a
\code{subset} specification, were used, they too will be displayed
here.

For models fit by the REML criterion the only statistic describing the
model fit is the value of the REML criterion itself.  An alternative
set of parameter estimates, the maximum likelihood estimates, are
obtained by specifying the optional argument \code{REML = FALSE}.
<<fm1ML>>=
(fm1ML <- lmer(Yield ~ 1 + (1|Batch), Dyestuff, REML = FALSE))
@ 
(Notice that this code fragment also illustrates a way to condense
the assignment and the display of the fitted model into a single
step. The redundant set of parentheses surrounding the assignment
causes the result of the assignment to be displayed. We will use this
device often in what follows.  This code fragment also illustrates, in
the second line, the continuation prompt, \code{"+"}.  When an
incomplete \R{} expression is submitted, the prompt is changed from
\code{">"} to \code{"+"}, indicating that additional input is required
before the expression can be evaluated.)

The display of a model fit by maximum likelihood provides several
other model-fit statistics such as Akaike's Information Criterion
(\code{AIC})~\citep{saka:ishi:kita:1986}, Schwarz's Bayesian
Information Criterion (\code{BIC})~\citep{schw:1978}, the
log-likelihood (\code{logLik}) at the parameter estimates, and the
deviance (negative twice the log-likelihood) at the parameter
estimates.  These are all statistics related to the model fit and
are used to compare different models fit to the same data.

At this point the important thing to note is that the default
estimation criterion is the REML criterion.  Generally the REML
estimates of variance components are preferred to the ML estimates.
However, when comparing models it is safest to refit all the models
using the maximum likelihood criterion.  We will discuss comparisons
of model fits later in section~\ref{sec:compmod}.

The third section is the table of estimates of parameters associated
with the random effects.  There are two sources of variability in the
model we have fit, a batch-to-batch variability in the level of the
response and the residual or per-observation variability --- also called
the within-batch variability.  The name ``residual'' is used in
statistical modeling to denote the part of the variability that cannot be
explained or modeled with the other terms.  It is the variation in the
observed data that is ``left over'' after we have determined the
estimates of the parameters in the other parts of the model.

Some of the variability in the response is associated with the
fixed-effects terms.  In this model there is only one such term,
labeled as the \code{(Intercept)}.  The name ``intercept'', which is
better suited to models based on straight lines written in a
slope/intercept form, should be understood to represent an overall
``typical'' or mean level of the response in this case.  (In case you
are wondering about the parentheses around the name, they are included
so that you can't accidentally create a variable with a name that
conflicts with this name.)  The line labeled \code{Batch} in the
random effects table shows that the random effects added to the
\code{(Intercept)} term, one for each level of the \code{Batch}
factor, are modeled as random variables whose unconditional variance
is estimated as \Sexpr{sprintf("%.2f", unlist(VarCorr(fm1)))} gm.$^2$
in the REML fit and as
\Sexpr{sprintf("%.2f", unlist(VarCorr(fm1ML)))} gm.$^2$
in the ML fit.  The corresponding standard deviations
are \Sexpr{sprintf("%.2f", sqrt(VarCorr(fm1)[["Batch"]][1,1]))} gm. for the
REML fit and \Sexpr{sprintf("%.2f", sqrt(VarCorr(fm1ML)[["Batch"]][1,1]))}
gm. for the ML fit.

Note that the last column in the random effects summary table is the
estimate of the variability expressed as a standard deviation rather
than as a variance.  These are provided because it is usually easier
to visualize standard deviations, which are on the scale of the
response, than it is to visualize the magnitude of a variance.  The
values in this column are a simple re-expression (the square root) of
the estimated variances. Do not confuse them with the standard errors
of the variance estimators, which are not given here.  In
section~\ref{sec:variability} we explain why we do not provide standard
errors of variance estimates.

The line labeled \code{Residual} in this table gives the estimate of
the variance of the residuals (also in gm.$^2$) and its corresponding
standard deviation.  For the REML fit the estimated standard deviation
of the residuals is \Sexpr{round(attr(VarCorr(fm1), "sc"), 2)} gm. and
for the ML fit it is also \Sexpr{round(attr(VarCorr(fm1ML), "sc"), 2)}
gm.  (Generally these estimates do not need to be equal.  They happen
to be equal in this case because of the simple model form and the
balanced data set.)

The last line in the random effects table states the number of
observations to which the model was fit and the number of levels of
any ``grouping factors'' for the random effects.  In this case we have
a single random effects term, \code{(1|Batch)}, in the model formula
and the grouping factor for that term is \code{Batch}.  There will be
a total of six random effects, one for each level of \code{Batch}.

The final part of the printed display gives the estimates and standard
errors  of any fixed-effects parameters in the model.  The only
fixed-effects term in the model formula is the \code{1}, denoting a
constant which, as explained above, is labeled as \code{(Intercept)}.
For both the REML and the ML estimation criterion the estimate of this
parameter is \Sexpr{round(fixef(fm1)[1], 2)} gm. (equality is again a
consequence of the simple model and balanced data set).  The standard
error of the intercept estimate is
\Sexpr{round(coef(summary(fm1))[1,2], 2)} gm. for the REML fit and
\Sexpr{round(coef(summary(fm1ML))[,2], 2)} gm. for the ML fit.

\subsection{A Model For the \texttt{Dyestuff2} Data}
\label{sec:Dyestuff2LMM}

Fitting a similar model to the \code{Dyestuff2} data produces an
estimate $\widehat{\sigma}_1=0$ in both the REML
<<fm2>>=
(fm2 <- lmer(Yield ~ 1 + (1|Batch), Dyestuff2))
@ 
and the ML fits.
<<fm2ML>>=
(fm2ML <- update(fm2, REML = FALSE))
@ 
(Note the use of the \code{update} function to re-fit a model with
different values of some of the arguments.  In a case like this where
the call to fit the original model is not very complicated the use of
\code{update} is not that much simpler than repeating the original
call to \code{lmer} with extra arguments.  For complicated model fits
it can be.)

An estimate of $0$ for $\sigma_1$ does not mean that there is no
variation between the groups.  Indeed Figure~\ref{fig:Dyestuff2dot}
shows that there is some small amount of variability between the
groups.  The estimate, $\widehat{\sigma}_1=0$, simply indicates that
the level of ``between-group'' variability is not sufficient to
warrant incorporating random effects in the model.

The important point to take away from this example is that we must
allow for the estimates of variance components to be zero.  We
describe such a model as being degenerate, in the sense that it
corresponds to a linear model in which we have removed the random
effects associated with \code{Batch}.  Degenerate models can and do
occur in practice.  Even when the final fitted model is not
degenerate, we must allow for such models when determining the
parameter estimates through numerical optimization.

To reiterate, the model \code{fm2} corresponds to the linear model
<<fm2a>>=
summary(fm2a <- lm(Yield ~ 1, Dyestuff2))
@
because the random effects are inert, in the sense that they have a
variance of zero, and can be removed.  

Notice that the estimate of $\sigma$ from the linear model (called the
\code{Residual standard error} in the output corresponds to the
estimate in the REML fit (\code{fm2}) but not that from the ML fit
(\code{fm2ML}). The fact that the REML estimates of variance
components generalize the estimate of the variance used in linear
models, in the sense that they correspond in the degenerate case, is
part of the motivation for the use of the REML criterion for fitting
mixed-effects models.

\subsection{Further assessment of the fitted models}
\label{sec:furtherassess}

The parameter estimates in a statistical model represent our ``best
guess'' at the unknown values of the model parameters and, as such,
are important results in statistical modeling.  However, they are not
the whole story.  Statistical models characterize the variability in
the data and we must assess the effect of this variability on the
parameter estimates and on the precision of predictions made from the
model.

In section~\ref{sec:variability} we introduce a method of assessing
variability in parameter estimates using the ``profiled deviance'' and
in section~\ref{sec:randomeffects} we show methods of characterizing
the conditional distribution of the random effects given the data.
Before we get to these sections, however, we should state in some
detail the probability model for linear mixed-effects and establish
some definitions and notation.  In particular, before we can discuss
profiling the deviance, we should define the deviance and we do that
in the next section.

\section{The Linear Mixed-effects Probability Model}
\label{sec:Probability}

In explaining some of parameter estimates related to the random
effects we have used terms such as ``unconditional distribution'' from
the theory of probability.  Before proceeding further we should
clarify the linear mixed-effects probability model and define several
terms and concepts that will be used throughout the book.

\subsection{Important definitions and results}
\label{sec:definitions}

Because some readers may find the rest of this section uncomfortably
complex we will first highlight a few important definitions and
results.  These results are presented without derivation and with
minimal explanation so that those readers who prefer not to be exposed
to the longer explanations and derivations that follow can read this
subsection then skip the rest of the section.  The equations in this
subsection are not numbered because they are repeated for reference in
later subsections.

As stated earlier, a mixed model incorporates two random variables,
$\bc B$, the $q$-dimensional vector of random effects, and $\bc Y$,
the $n$-dimensional response vector.  In a linear mixed model the
unconditional distribution of $\bc B$ and the conditional distribution
$(\bc Y|\bc B=\vec b)$ are
\begin{displaymath}
  \begin{aligned}
    (\bc Y|\bc B=\vec b)&\sim\mathcal{N}(\vec X\vec\beta+\vec Z\vec
    b,\sigma^2\vec I)\\
    \bc{B}&\sim\mathcal{N}(\vec0,\Sigma_\theta) .
  \end{aligned}
\end{displaymath}
That is, they are both multivariate Gaussian (or ``normal'')
distributions. The \emph{conditional mean} of $\bc Y$, given $\bc
B=\vec b$, is the \emph{linear predictor}, $\vec X\vec\beta+\vec Z\vec
b$, which depends on the $p$-dimensional \emph{fixed-effects
  parameter}, $\vec \beta$, and on $\vec b$.  The \emph{model
  matrices}, $\vec X$ and $\vec Z$, which are of dimension $n\times p$
and $n\times q$, respectively, are determined from the formula for the
model and the values of covariates.  The matrix $\vec Z$ is always
sparse (i.e. most of the elements in the matrix are zero).

The \emph{relative covariance factor}, $\Lambda_\theta$ is a
$q\times q$ matrix, depending on the \emph{variance-component
  parameter}, $\vec\theta$, and generating the symmetric $q\times q$
variance-covariance matrix, $\Sigma_\theta$, according to
\begin{displaymath}
  \Sigma_\theta=\sigma^2\Lambda_\theta\Lambda_\theta\trans .
\end{displaymath}
The \emph{spherical random effects}, $\bc U\sim\mathcal{N}(\vec
0,\sigma^2\vec I_q)$, determine $\bc B$ according to
\begin{displaymath}
  \bc B=\Lambda_\theta\bc U .
\end{displaymath}
The \emph{sparse Cholesky factor}, $\vec L_\theta$, is a lower
triangular $q\times q$ matrix satisfying
\begin{displaymath}
  \vec L_\theta\vec L_\theta\trans=
  \Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec I_q .
\end{displaymath}
where $\vec I_q$ is the $q\times q$ \emph{identity matrix}.  The
\emph{conditional mode}, $\tilde{\vec u}_\theta$, of the spherical random
effects and the conditional estimate, $\widehat{\beta}_\theta$, are the
values that minimize the \emph{penalized residual sum of squares}
(PRSS) and $r^2_\theta$ is the value at the minimum.  That is,
\begin{displaymath}
  r^2_\theta=\min_{u,\beta}\left\{\|\vec y -\vec X\vec\beta -\vec
    Z\Lambda_\theta\vec u\|^2+\|\vec u\|^2\right\} .
\end{displaymath}

The \emph{maximum likelihood estimate}, $\widehat{\theta}$, is the
value that minimizes \emph{profiled deviance}
\begin{displaymath}
  \tilde{d}(\vec\theta|\vec y)
  =\log(|\vec L_\theta|^2)+n\left(1 +
    \log\left(\frac{2\pi r^2_\theta}{n}\right)\right).
\end{displaymath}
where $|\vec L_\theta|$ denotes the \emph{determinant} of $\vec
L_\theta$.  Because $\vec L_\theta$ is triangular, its determinant is
the product of its diagonal elements.

We use numerical optimization of $\tilde{d}(\vec\theta|\vec y)$ to
determine $\widehat{\vec\theta}$. In the process of evaluating
$\tilde{d}((\widehat{\vec\theta}|\vec y)$ we determine
$\widehat{\beta}$, $\tilde{\vec u}_{\widehat{\theta}}$ and
$r^2_{\widehat{\theta}}$, from which we can evaluate
$\widehat{\sigma}=r^2_{\widehat{\theta}}/n$.

The elements of the conditional mode of $\bc B$, evaluated at the
parameter estimates,
\begin{displaymath}
\tilde{b}_{\widehat{\theta}}=\Lambda_{\widehat{\theta}}\tilde{u}_{\widehat{\theta}}
\end{displaymath}
are sometimes called the \emph{best linear unbiased predictors} or
BLUPs of the random effects.

\subsection{Definitions of the Random Variables}
\label{sec:defnRV}

As mentioned in section~\ref{sec:memod}, the mixed-effects probability
model is based on two vector-valued random variables: the
$q$-dimensional vector of random effects, $\bc B$, and the
$n$-dimensional response vector, $\bc Y$.  In our model for the
\code{Dyestuff} data the dimension of the response vector is $n=30$
and the dimension of the random effects vector is $q=6$.  

In all the forms of mixed models that we will consider, the random
effects vector has a multivariate Gaussian (also called ``normal'')
distribution with mean vector $\vec 0$ and with a parameterized,
$q\times q$, symmetric, variance-covariance matrix that we will write
as $\Sigma_\theta$.  The notation indicates that the matrix $\Sigma$
depends on a parameter vector that is written as $\vec\theta$ and
which we call the \emph{variance-component parameters}, even though
some of the elements of $\vec\theta$ may determine covariances and not
variances.

We write this distribution as
\begin{equation}
  \label{eq:unconditionalB}
  \bc{B}\sim\mathcal{N}(\vec0,\Sigma_\theta) .
\end{equation}
Because this distribution does not depend on the value, $\vec y$, of
the random variable, $\bc Y$, we say it is the \emph{unconditional}
distribution of $\bc B$.

To describe the probability model for the response, $\bc Y$, we
provide the conditional distribution $(\bc Y|\bc B=\vec b)$, which is
the distribution of the response vector, $\bc Y$, assuming that the
value of the random effects vector, $\bc B$, is known to be $\vec b$.
(In practice we never know the value $\vec b$ but, for the purpose of
formulating the model, we will assume that we do.)  For all the forms
of mixed models that we will consider, $\vec b$ changes the
conditional distribution of $\bc Y$ by changing the conditional mean,
$\vec\mu_{\bc Y|\bc B=\vec b}$.  Furthermore, the conditional mean
response depends on $\vec b$ and on the fixed-effects parameter vector
$\vec\beta$ only through the \emph{linear predictor}
\begin{equation}
  \label{eq:linpred}
  \vec\gamma = \vec Z\vec b+\vec X\vec\beta
\end{equation}
where $\vec Z$ and $\vec X$ are known \emph{model matrices} of
the appropriate size.

To simplify notation, we will write the conditional mean,
$\vec\mu_{\bc Y|\bc B=\vec b}$, as $\vec\mu$ without the subscript.
Bear in mind that $\vec\mu$ will always represent the $n$-dimension
vector that is the conditional mean of the response random variable,
$\bc Y$, given a particular value, $\vec b$, of the random effects
random variable $\bc B$.

For all the forms of mixed models that will be considered in this book
the fixed-effects parameter, $\vec\beta$, and the random effects
vector, $\vec b$, determine the (conditional) mean, $\vec\mu$ through
the linear predictor, $\vec\gamma$.  In the case of a linear mixed
model $\vec\mu=\vec\gamma$ and the particular form of the conditional
distribution is a ``spherical'' Gaussian.  That is
\begin{equation}
  \label{eq:condmeanY}
  \vec\mu=\vec\gamma=\vec Z\vec b+\vec X\vec\beta
\end{equation}
and
\begin{equation}
  \label{eq:LMMconddist}
  (\bc Y|\bc B=\vec b)\sim\mathcal{N}\left(\vec Z\vec b+\vec
    X\vec\beta,\sigma^2\vec I_n\right) .
\end{equation}

The expression $\vec I_n$ denotes the identity matrix of size $n$.
This is the $n\times n$ matrix whose diagonal elements are all unity
and whose off-diagonal elements are all zero.  The parameter
$\sigma$ is called the \emph{common scale parameter}.  Its square is the
variance of the residual ``noise'' terms that cannot be explained by
other parts of the model.  The name ``spherical'' is applied to
Gaussian distributions of the form (\ref{eq:LMMconddist}) because the
contours of constant probability density are spheres centered at $\vec
Z\vec b+\vec X\vec\beta$ in the $n$-dimensional response space

Because $\vec\mu$ (and, hence, $\vec\gamma$) are $n$-dimensional
vectors, the model matrix $\vec Z$ must be $n\times q$ and the model
matrix $\vec X$ must be $n\times p$, where $p$ is the dimension of the
fixed-effects parameter vector, $\vec\beta$.  For the model fit to the
\code{Dyestuff} data, $p=1$ and $\vec X$ is a $30\times 1$
matrix, all of whose elements are unity.  The fixed-effects term
\code{1} in a model formula generates a column of ones in the
fixed-effects model matrix, $\vec X$.  For the model being considered
here, this column of ones is the only column in $\vec X$.

The form of the random-effects model matrix, $\vec Z$, and the form of
the variance-covariance matrix, $\Sigma_\theta$, and the method by which
$\Sigma_\theta$ is determined from the value of $\vec\theta$ are all based
on the random-effects terms in the model formula.  As stated earlier,
there is one random effects term, \code{(1 | Batch)}, in the formula
for this model.  Random-effects terms are those that contain the
vertical bar, \code{"|"}, character.  The \code{Batch} variable is the
grouping factor for the random effects described by this term.  An
expression for the grouping factor, usually just the name of a
variable, occurs to the right of the vertical bar.  If the expression
on the left of the vertical bar is \code{1}, as it is here, we
describe the term as a \emph{simple, scalar, random-effects term}.
The designation ``scalar'' means there will be exactly one random
effect generated for each level of the grouping factor.  A simple,
scalar term generates a block of indicator columns --- the indicators for
the grouping factor --- in $\vec Z$.  Because there is only one
random-effects term in this model and because that term is a simple,
scalar term, the model matrix $\vec Z$ for this model is the indicator
matrix for the levels of \code{Batch}.

The transpose of this matrix, of dimension $6\times 30$, is stored as
a sparse matrix in the environment of the fitted model as \code{Zt}.
<<fm1Zt>>=
env(fm1)$Zt
@ 
A sparse matrix is one in which most of the elements are known to be
zero.  These elements are represented by a \code{"."} in this display.
The nature of an indicator matrix is such that only one element in
each row of the indicator matrix (corresponding to a column of \code{Zt}
shown above) is non-zero.  Sparse matrix
methods~\citep{davis06:csparse_book} for numerical linear algebra
provide special techniques for storing and manipulating such matrices.
These techniques are the basis of the numerical methods implemented in
\code{lmer}.

Often we will show the structure of sparse matrices as an image
(Figure~\ref{fig:fm1Ztimage}).
\begin{figure}[tbp]
  \centering
<<fm1Ztimage,fig=TRUE,echo=FALSE,height=2.5>>=
print(image(env(fm1)$Zt, sub=NULL))
@   
  \caption[Image of the random-effects model matrix for
  \texttt{fm1}]{Image of the transpose of the random-effects model
    matrix, $\vec Z$, for model \code{fm1}.  The non-zero elements,
    which are all unity, are shown as darkened squares.  The zero
    elements are blank.}
  \label{fig:fm1Ztimage}
\end{figure}
Especially for large sparse matrices, the image conveys its structure
more compactly than does the printed representation.

When the model contains only one random-effects term and that term is
a simple, scalar term, then the variance-covariance matrix
$\Sigma_\theta$ is a non-negative multiple of $\vec I_q$, the
$q\times q$ identity matrix.  Although we will defer until later the
discussion of the exact form of $\Sigma_\theta$ for other
models, we will now introduce a transformation of
$\Sigma_\theta$ that we will use throughout.

\subsection{The relative covariance factor}
\label{sec:relcovfac}

A variance-covariance matrix such as $\Sigma_\theta$ is required to be
symmetric and \emph{positive semi-definite}, which means, in effect,
that $\Sigma_\theta$ has the matrix equivalent of a ``square root''.
Recall that a scalar variance, such as $\sigma^2$ in
(\ref{eq:LMMconddist}), must be non-negative and that the standard
deviation, $\sigma\ge0$, is the square root of the variance.  A
variance-covariance matrix has a similar property.  There must be a
matrix, say $\Lambda_\theta$, such that when it is multiplied by its
transpose it produces the original matrix, $\Sigma_\theta$.  This is
not quite the same ``squaring'' the matrix $\Lambda_\theta$ because it
is multiplied by its transpose, $\Lambda_\theta\trans$, so as to create a
product that is symmetric, as $\Sigma_\theta$ must be.

It turns out that we can simplify many subsequent formulas if we
define $\Lambda_\theta$ to be a multiple of the square-root
factor, which we call the \emph{relative covariance factor}, defined
to satisfy
\begin{equation}
  \label{eq:relcovfac}
  \Sigma_\theta=\sigma^2\Lambda_\theta
  \Lambda_\theta\trans .
\end{equation}
The scalar $\sigma^2$ is the square of the common scale parameter
introduced in (\ref{eq:LMMconddist}).  The name ``relative covariance
factor'' indicates that $\Lambda_\theta$ is the factor of the
variance-covariance matrix, $\Sigma_\theta$, of $\bc B$ relative to
the variance, $\sigma^2$, in the conditional distribution $(\bc Y|\bc
B=\vec b)$.

For the \code{Dyestuff} model, which has only one simple, scalar
random-effects term, $\Sigma_\theta$ and
$\Lambda_\theta$ are both multiples of $\vec I_6$, the identity
matrix of size six and $\theta$ is a one-dimensional parameter.
Setting $\theta=\frac{\sigma_1}{\sigma}$ we can write
\begin{equation}
  \label{eq:lambdatheta}
  \Lambda_\theta = \theta\vec I_6 \text{ and }\Sigma_\theta=\theta^2\vec I_6
\end{equation}
subject to the constraint that $\theta\ge0$.  Notice that the
constraint allows for $\theta$ to be zero, in which case both
$\Lambda_0$ and $\Sigma_0$ are $6\times6$ matrices of zeros.

\subsection{The ``Spherical'' Random Effects Vector}
\label{sec:sphericalRE}

Because we isolate the effect of $\vec b$ on the conditional
distribution, $(\bc Y|\bc B=\vec b)$, in the linear predictor,
$\vec\gamma$, we can use a linear transformation of $\bc B$ in place
of $\bc B$.  Because we wish to allow for degenerate models, we define
the transformation in the other direction, from a $q$-dimensional
``spherical'' Gaussian random variable, $\bc U$, to $\bc B$ as
\begin{equation}
  \label{eq:3}
  \bc B=\Lambda_\theta\bc U,\quad
  \bc U\sim\mathcal{N}(\vec 0,\sigma^2\vec I_q).
\end{equation}
so the transformation is well-defined, even when $\Lambda_\theta$ is
singular, and produces the desired distribution, $\bc
B\sim\mathcal{N}(\vec 0,\Sigma_\theta)$.

The linear predictor, $\vec\gamma$, can be expressed in terms of $\vec
u$ as
\begin{equation}
  \label{eq:4}
  \vec\gamma=\vec Z\Lambda_\theta\vec u + \vec X\vec\beta.
\end{equation}

Because we observe $\vec y$ and do not observe $\vec b$ or $\vec u$,
the conditional distribution of interest, for the purposes of
statistical inference, is $(\bc U|\bc Y=\vec y)$ (or, equivalently,
$(\bc B|\bc Y=\vec y)$).  For a linear mixed model in which both $\bc
Y$ and $\bc U$ have spherical Gaussian distributions, with the same
scale parameter, $\sigma$, we can explicitly derive the conditional
distribution $(\bc U|\bc Y=\vec y)$.  We will state some of the
characteristics of this distribution here and defer the actual
derivation until Chapter~\ref{chap:computational}.

The conditional distribution, $(\bc U|\bc Y=\vec y)$, is a multivariate
Gaussian whose mean, which we will write as $\tilde{\vec u}$, is the
solution to the penalized least squares problem
\begin{equation}
  \label{eq:LMMcondmode}
  \begin{aligned}
  \tilde{\vec u}&=\arg\min_{\vec u}\left\{\|\vec y-\vec\gamma\|^2+
    \|\vec u\|^2\right\}\\
    &=\arg\min_{\vec u}\left\{\|(\vec y-\vec X\vec\beta)-\vec
      Z\Lambda_\theta\vec u\|^2+
    \|\vec u\|^2\right\} .
  \end{aligned}
\end{equation}
(The notation $\arg\min_{\vec u}$ means that $\tilde{\vec u}$ is the
value of $\vec u$ that minimizes the expression on the right.)  We say
this is a \emph{penalized least squares} problem because it requires
determining the value of $\vec u$ that minimizes the sum of a residual
sum of squares, $\|(\vec y-\vec X\vec\beta)-\vec Z\Lambda_\theta\vec
u\|^2$, plus $\|u\|^2$, which we describe as a penalty on the size of
$\vec u$.

One way of expressing a penalized least squares problem like this is
by incorporating the penalty as ``pseudo-data'' in a standard least
squares problem.  We extend the observed response vector, which in
this case is actually $\vec y-\vec X\vec\beta$ because we are
minimizing with respect to $\vec u$ only, with $q$ responses that are
0 and we extend the predictor expression, $\vec Z\Lambda_\theta\vec u$
with $\vec I_q\vec u$.  Writing this as a least squares problem produces
\begin{equation}
  \label{eq:PLSLMM}
  \tilde{\vec u}=\arg\min_{\vec u}\left\|
    \begin{bmatrix}
      \vec y-\vec X\vec\beta\\
      \vec 0
    \end{bmatrix} -
    \begin{bmatrix}
      \vec Z\Lambda_\theta\\
      \vec I_q
    \end{bmatrix}\vec u\right\|^2
\end{equation}
with a solution that satisfies
\begin{equation}
  \label{eq:LMMPLSsol}
  \left(\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec
    I_q\right)\tilde{\vec u}
  =\Lambda_\theta\trans\vec Z\trans\left[\vec y-\vec X\vec\beta\right]
\end{equation}

To evaluate $\tilde{\vec u}$ we form the \emph{sparse Cholesky
  factor}, $\vec L_\theta$, which is a lower triangular $q\times q$
matrix that satisfies
\begin{equation}
  \label{eq:sparseCholesky}
  \vec L_\theta\vec L_\theta\trans=
  \Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec I_q .
\end{equation}
(In chapter~\ref{chap:computational} we will modify this definition slightly by incorporating a
``fill-reducing'' permutation matrix but that is a computational
detail and does not affect the results here.)  Obtaining the Cholesky
factor, $\vec L_\theta$, may not seem to be great progress because we
now need to solve
\begin{equation}
  \label{eq:LMMCholsol}
  \vec L_\theta\vec L_\theta\trans\tilde{\vec u}
  =\Lambda_\theta\trans\vec Z\trans\left[\vec y-\vec X\vec\beta\right]
\end{equation}
for $\tilde{\vec u}$.  However, this is the key step in computational
methods in the \package{lme4} package. The ability to evaluate $\vec
L_\theta$ rapidly for many different values of $\vec\theta$ is what
makes the computational methods in \package{lme4} feasible, even when
applied to very large data sets with complex structure.  Determining
$\tilde{\vec u}$ through (\ref{eq:LMMCholsol}) is a straightforward
process because $\vec L_\theta$ is lower triangular.

After evaluating $\vec L_\theta$ and using that to solve for
$\tilde{\vec u}$, which also produces $r^2_{\beta,\theta}$, the value
of the penalized residual sum of squares (PRSS) at $\tilde{\vec u}$, we can
evaluate the likelihood, which is a function of the parameters,
$\vec\beta$, $\vec\theta$ and $\sigma$, given the observed data, $\vec
y$. On the deviance scale, which is negative twice the (natural)
logarithm of the likelihood, this is
\begin{equation}
  \label{eq:LMMdeviance}
  d(\vec\theta,\vec\beta,\sigma|\vec y)
  =n\log(2\pi\sigma^2)+\log(|\vec L_\theta|^2)+\frac{r^2_{\beta,\theta}}{\sigma^2}.
\end{equation}
%(The notation $|\vec L_\theta|$ denotes the \emph{determinant} of
%$\vec L_\theta$. In the case of a triangular matrix, such as $\vec
%L_\theta$, this is simply the product of the diagonal elements.)
The maximum likelihood estimates of the parameters are those that
minimize the deviance, (\ref{eq:LMMdeviance}).

Equation (\ref{eq:LMMdeviance}) is a remarkably compact expression,
considering that the class of models to which it applies is very large
indeed.  However, we can do better than this if we notice that
$\vec\beta$ enters (\ref{eq:LMMdeviance}) only through the PRSS
$r^2_{\beta,\theta}$, and, for any value of
$\vec\theta$, minimizing this expression with respect to $\vec\beta$
is just another least squares problem.  Let
$\widehat{\vec\beta}_\theta$ be the value of $\vec\beta$ that
minimizes the PRSS simultaneously with respect to
$\vec\beta$ and $\vec u$ and let $r^2_\theta$ be the minimum PRSS.
Furthermore, if we set $\widehat{\sigma^2}_\theta=r^2_\theta/n$,
which is the value of $\sigma^2$ that minimizes the deviance for a
given value of $r^2_\theta$, then the \emph{profiled deviance}, which
is a function of $\vec\theta$ only, is
\begin{equation}
  \label{eq:LMMprofdeviance}
  \tilde{d}(\vec\theta|\vec y)
  =\log(|\vec L_\theta|^2)+n\left[1 +
    \log\left(\frac{2 \pi r^2_\theta}{n}\right)\right].
\end{equation}


\section{Assessing the Variability of the Parameter Estimates}
\label{sec:variability}

The mixed-effects model fit as \code{fm1} or \code{fm1ML} has three
parameters for which we obtained estimates.  These parameters are
$\sigma_1$, the standard deviation of the random effects, $\sigma$,
the standard deviation of the residual or ``per-observation'' noise
term and $\beta_0$, the fixed-effects parameter that is labelled as the
intercept.  A standard error for the estimate $\widehat{\beta_0}$ is
provided but no standard errors are given for the estimates
$\widehat{\sigma_1}$ and $\widehat{\sigma}$.

The reason that we do not simply quote standard errors for the
estimated standard deviations is because they are misleading.  Even
more misleading is the common practice of quoting a standard error for
an estimated variance.  Quoting a parameter estimate and a standard
error for the estimator gives the impression that we can summarize the
precision of the estimate with a series of confidence intervals that
are symmetric about the estimate.  In other words we are assuming that
decreasing a parameter from its estimate by an amount $\delta$ has the
same effect on the quality of the model fit as does increasing the
parameter by $\delta$.  It is not realistic to expect that this is
true of measures of variability, such as standard deviations and variances.

Instead of making the unrealistic assumption that the precision of
these parameter estimates can be summarized by standard errors we
evaluate the quality of the fit for nearby values of the parameter and
form confidence intervals based on the change in the deviance.  This
is called \emph{profiling} the deviance with respect to a parameter.
We plot the profiles, obtained by applying the function \code{profile}
to the fitted model, on the scale of the square root of the change in
the deviance, as shown in Figure~\ref{fig:fm1absprof}.
\begin{figure}[tb]
  \centering
<<fm1absprof,fig=TRUE,echo=FALSE,height=2.5>>=
print(xyplot(pr1, aspect = 0.7, strip=FALSE, strip.left = TRUE))
@   
\caption{Profiled deviance, on the scale $|\zeta|$, the square root of
  the change in the deviance, for each of the parameters in model
  \code{fm1ML}.  The intervals shown are 50\%, 80\%, 90\%, 95\% and
  99\% confidence intervals based on the profile likelihood.}
  \label{fig:fm1absprof}
\end{figure}

Because we will use such plots of the profiled deviance extensively
for assessing the uncertainty in parameter estimates, we will describe
this particular plot qualitatively, but in some detail, here.  Formal
definitions and equations will be given in the next section.

Recall that one of the measures of the quality of a model fit is the
\emph{deviance}, which is negative twice the log-likelihood evaluated
at the parameter estimates.  The maximum likelihood estimates are the
parameter values that minimize the deviance,
$d(\sigma_1,\sigma,\beta_0)$, over all feasible values of the
parameters.

To perform a hypothesis test of the form $H_0:\sigma_1=80$ versus
$H_a:\sigma_1\ne 80$ we could consider the minimum deviance achieved
when $\sigma_1=80$ minus the global minimum deviance.  This difference,
\begin{displaymath}
  \min_{\sigma,\beta_0}d(80,\sigma,\beta_0)-
  d(\widehat{\sigma}_1, \widehat{\sigma}, \widehat{\beta}_0),
\end{displaymath}
is the likelihood ratio test statistic. (The deviance is a multiple of
the logarithm of the likelihood so ratios of likelihoods correspond to
differences in the deviance.)  This test statistic explicitly compares
how well we can fit the model for this specific value of one of the
parameters to the best possible fit of the model.  Note that this
quantity is based on two model fits: the unconstrained fit and a
second fit subject to the constraint $\sigma_1=80$.   We are not
basing our test on how well we think we should be able to fit the
constrained model --- based on the global parameter estimates and
approximate standard errors --- we are actually re-fitting the model
subject to the constraint.

The theory of likelihood ratio tests states that the test statistic
would be compared to the quantiles of the $\chi_k^2$ distribution
where $k$ is the number of constraints, which is 1 in this case.  But
a $\chi_1^2$ distribution is simply the square of the standard
normal distribution, $\mathcal{N}(0,1)$, so we plot its square root,
which we denote as
\begin{equation}
  \label{eq:abszetadef}
  |\zeta(\sigma_1)|=\sqrt{
  \min_{\sigma,\beta_0}d(\sigma_1,\sigma,\beta_0)-
  d(\widehat{\sigma}_1, \widehat{\sigma}, \widehat{\beta}_0)},
\end{equation}
with similar definitions for the other parameters.  (The perceptive
reader will have notice that the second profile plot is on the scale
of $\log(\sigma)$, not $\sigma$.  We will discuss this choice of scale
later.)

The intervals shown on the plots are 50\%, 80\%, 90\%, 95\% and 99\%
confidence intervals based on this likelihood ratio test.  That is,
the bounds on the 95\% confidence intervals are the points where
$|\zeta|=1.960$.  We use the \R{} function \code{profile} to create the
profiled deviance object and \code{xyplot} to display the plot.
<<pr1,eval=FALSE>>=
pr1 <- profile(fm1ML)
@ 
The
\code{confint} extractor is used to obtain the endpoints of a
particular confidence interval.
<<confint1>>=
confint(pr1)
@ 

Often the \emph{signed square root} provides a more meaningful scale
on which to plot the likelihood ratio test statistic, as in
Figure~\ref{fig:fm1prof}.
\begin{figure}[tb]
  \centering
<<fm1prof,fig=TRUE,echo=FALSE,height=3.5>>=
print(xyplot(pr1, aspect = 1.3, absVal = FALSE))
@   
  \caption{Signed square root, $\zeta$, of the likelihood ratio test
    statistic for each of the parameters in model \code{fm1ML}.  The
    vertical lines are the endpoints of 50\%, 80\%, 90\%, 95\% and 99\%
    confidence intervals derived from this test statistic.}
  \label{fig:fm1prof}
\end{figure}
The signed square root assigns a negative sign to $\zeta$ for
parameter values less than the estimate and a positive sign for
parameter values greater than the estimate providing a curve whose
interpretation is similar to that of a normal probability plot of data
or residuals.

\subsubsection{Interpretation of the Shape of the $\zeta$ Plot}

We will use $\phi$ to represent a parameter or a transformation of a
parameter in the model.  For the model we are considering $\phi$ can
be $\sigma_1$ or $\sigma_1^2$ or $\log(\sigma)$ or $\sigma$ or
$\sigma^2$ or $\beta_0$ or one of many other possibilities.  In a plot
of $\zeta(\phi)$ versus $\phi$ the estimate, $\widehat{\phi}$, is the
point where $\zeta = 0$ and the slope of the curve at $\widehat{\phi}$
is the inverse of the standard error of $\phi$.  There is more than
one way of calculating a standard error of a parameter estimate and in
chapter~\ref{cha:Theory} we will clarify that this standard error is
based on a quantity called ``the observed information matrix''.  That
level of detail is not necessary here.  All we need to know is that
the inverse slope, written $s_\phi$, is that standard error of the
parameter estimate, which is a measure of its precision.

If the plot of $\zeta$ versus $\phi$ is close to a straight line over
the region of interest then our $(1-\alpha)$ confidence interval on
$\phi$ will be close to
\begin{equation}
  \label{eq:confint}
  \widehat{\phi}\pm z_{\alpha/2}s_{\phi} 
\end{equation}
where $z_{\alpha/2}$ is the $\alpha/2$ upper quantile of the standard
normal (also called Gaussian) distribution.  For example,
$z_{0.025}=1.960$ so a 95\% confidence interval on the parameter
$\phi$ would be calculated as
\begin{equation}
  \label{eq:confint95}
  \widehat{\phi}\pm 1.960 s_{\phi} 
\end{equation}
or, effectively, the estimate plus-or-minus two standard errors of the
estimate.

We should realize that confidence intervals of the form
(\ref{eq:confint}) are based on an approximation.  Similarly,
performing a hypothesis test of the form $H_0:\phi = \phi_0$
\textrm{vs.} $H_a:\phi\ne\phi_0$ by calculating the observed
``z-statistic''
\begin{equation}
  \label{eq:zobs}
  z_{\text{obs}}=\frac{\widehat{\phi} - \phi_0}{s_\phi}
\end{equation}
and evaluating a p-value as
$2\cdot\mathrm{P}[\mathcal{Z}>|z_{\text{obs}}|]$ is also an
approximation.

Of course, we don't need to perform hypothesis tests of the form
$H_0:\phi=\phi_0$ \emph{vs.} $H_a:\phi\ne\phi_0$ in this way
because can fit the model without any constraints on the parameters
then fit the model subject to the constraint $\phi=\phi_0$ and compare
the two model fits using the likelihood ratio.  Furthermore the
confidence intervals we obtain are based on ``inverting'' such
hypothesis tests.  Because we are fitting the model under both
conditions and comparing the quality of the fits we obtain more
realistic information regarding whether the parameter value $\phi_0$
is reasonable.  We do not need to resort to fitting the model once
only and keeping our fingers crossed while using approximations to
perform statistical inference.

This situation is unfortunately characteristic of modern statistics.
The well-known statistical methods shown in many texts are often based
on unnecessary approximations.  We have the ability to fit models many
times subject to different constraints on the parameters so we can
actually examine the effective variability in the parameter estimates
rather than resorting to approximations.

Returning to consideration of confidence intervals of the form
(\ref{eq:confint}) and hypothesis tests based on (\ref{eq:zobs}),
these will be suitable methods when the profile plot (the plot of
$\zeta$ versus $\phi$) is reasonably straight. In those cases, of
course, the methods based on $\zeta$ will produce confidence intervals
or p-values for hypothesis tests of the expected form.  The important
cases are when the profile plot is not reasonably straight over the
region of interest.  In those cases the confidence intervals are not
symmetric or not determined from quantiles of the standard normal nor
are p-values based on the standard normal distribution.

As shown above, we can create confidence intervals on parameters or
transformed parameter values using the profile and the \code{confint}
extractor function so we don't really need to examine the profile
plot.  However, the profile plots do provide us with a valuable visual
evaluation of the precision of the parameter estimates and we will
use them frequently.

We can read a profile plot similar to the way that we read a
quantile-quantile plot, such as a normal probability plot, of observed
data or of residuals from a fitted model.  That is, we evaluate a
profile plot for skewness and for over-dispersion.  We will
speak of these properties as applying to the distribution of the
parameter estimator.  Technically that is not quite correct (the
profile plot indicates the sensitivity of the model fit to the
parameter values) but I find it a convenient way of discussing
patterns in the profile plots.

With this in mind we can examine the patterns in
Figure~\ref{fig:fm1prof}.  The profile plot for $\log(\sigma)$ is
quite straight indicating that the local approximation based on a
standard error is quite good.  Notice, however, that this pattern is
with respect to $\log(\sigma)$, not $\sigma$ or, even worse, $\sigma^2$
see that the sigmoidal (i.e. like an
elongated ``S'' curve) pattern for the profile plot of $\beta_0$ in Figure~\ref{fig:fm1prof} the
parameter $\beta_0$ is very close to symmetric about the estimate
$\widehat{\beta_0}$ but over-dispersed relative to a normal distribution.

\subsubsection{Precision of the Variance Components}
\label{sec:precvarcomp}

When we summarize our knowledge of a parameter in a fitted model by
giving its estimate and a standard error of this estimate we are
assuming that the quality of fit for different values of this
parameter, as measured by the signed square root, $\eta$, say, will be
symmetric about the estimate.  Furthermore, if we are to use a z
test'' or confidence intervals based on quantiles of the standard
normal, then we are assuming that the plot of $\zeta$ will be
reasonably close to a straight line.  In that case the standard error
of the parameter corresponds to the inverse of the slope of the line.

We can see that the only panel in Figure~\ref{fig:fm1prof} that is at
all close to a straight line is the middle panel, which shows $\zeta$
versus $\log(\sigma)$, not $\zeta$ versus $\sigma$ or, even worse,
$\zeta$ versus $\sigma^2$, as shown in Figure~\ref{fig:sigmaprof}.
\begin{figure}[tb]
  \centering
<<sigmaprof,fig=TRUE,echo=FALSE>>=
zeta <- sqrt(qchisq(c(0.5,0.8,0.9,0.95,0.99), 1))
zeta <- c(-rev(zeta), 0, zeta)
spl <- attr(pr1, "forward")[[2]]
endpts <- predict(attr(pr1, "backward")[[2]], zeta)$y

fr <- data.frame(zeta = rep.int(zeta, 2),
                 endpts = c(exp(endpts), exp(2*endpts)),
                 pnm = gl(2, length(zeta)))
levels(fr$pnm) <- c(expression(sigma), expression(sigma^2))
print(xyplot(zeta ~ endpts|pnm, fr, type = "h",
             scales = list(x = list(relation = "free")),
             xlab = NULL, ylab = expression(zeta), aspect = 1.3,
             strip = strip.custom(
             factor.levels = expression(sigma, sigma^2)),
             panel = function(...) {
                 panel.grid(h = -1, v = -1)
                 panel.xyplot(...)
                 lims <- log(current.panel.limits()$xlim)
                 if (panel.number() == 2) lims <- lims/2
                 pr <- predict(spl, seq(lims[1], lims[2], len = 101))
                 panel.lines(exp(pr$x * panel.number()), pr$y)
             }))
@   
\caption{Signed square root, $\zeta$, of the likelihood ratio test
  statistic as a function of $\sigma$ and as a function of $\sigma^2$.
  The vertical lines are the endpoints of 50\%, 80\%, 90\%, 95\% and
  99\% confidence intervals.}
  \label{fig:sigmaprof}
\end{figure}

\subsection{Profile Pairs Plots}
\label{sec:Profpairs}

We can use the information from the profiled deviance to produce
\begin{figure}[tb]
  \centering
<<fm1profpair,fig=TRUE,echo=FALSE,height=8>>=
print(splom(pr1))
@   
\caption{Profile pairs plot for the parameters in model \code{fm1}.
  The contour lines correspond to marginal 50\%, 80\%, 90\%, 95\% and
  99\% marginal confidence regions based on the likelihood ratio.
  Panels below the diagonal represent the $(\zeta_i,\zeta_j)$
  parameters; those above the diagonal represent the original
  parameters.}
  \label{fig:fm1profpair}
\end{figure}
approximate contours of the profiled deviance with respect to pairs of
parameters.  Such a plot is shown in Figure~\ref{fig:fm1profpair}.
Because we will use such plots throughout this book to examine the
pairwise dependence of parameters, we will describe this plot in some detail.

The labels along the diagonal indicate the parameter shown on a
particular axis.  Each parameter is represented on two scales, the
original scale and the transformation determined by $\zeta$.  Panels
above the diagonal are on the original scale; those below the diagonal
are on the $(\zeta_i,\zeta_j)$ scale.  There are two lines drawn on
each panel.  These lines intersect at the estimated parameter values.

