\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=8,strip.white=TRUE}
\SweaveOpts{keep.source=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Long,include=TRUE}
\setkeys{Gin}{width=\textwidth}

<<preliminaries,echo=FALSE,print=FALSE,results=hide>>=
options(width=80, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
#library(splines)
#library(lattice)
#library(Matrix)
#library(lme4a)
@

\chapter{Computational Methods for Linear Mixed Models}
\label{chap:computational}
\abstract*{
  The \package{lme4} package provides R functions to fit and analyze
  several different types of mixed-effects models, including linear
  mixed models, generalized linear mixed models and nonlinear mixed
  models.  In this vignette we describe the formulation of these
  models and the computational approach used to evaluate or
  approximate the log-likelihood of a model/data/parameter value
  combination.}

In this chapter we describe some of the details of the computational
methods for fitting linear mixed models, as implemented in the
\package{lme4} package, and the theoretical development behind these
methods.  We also provide the basis for later generalizations to
models for non-Gaussian responses and to models in which the
relationship between the conditional mean, $\vec\mu$, and the linear
predictor, $\vec\gamma=\vec X\vec\beta+\vec Z\vec b=
\vec Z\Lambda_\theta\vec u+\vec X\vec\beta$ is a nonlinear relationship.

This material is directed at those readers who wish to follow the
theory and methodology of linear mixed models and how both can be
extended to other forms of mixed models.  Readers who are less
interested in the ``how'' and the ``why'' of fitting mixed models than
in the results themselves should not feel obligated to master these
details.

We begin by reviewing the definition of linear mixed-effects models
and some of the basics of the computational methods, as given in
Sect.~\ref{sec:memod}.

\section{Definitions and Basic Results}
\label{sec:defnLMM}

As described in Sect.~\ref{sec:memod}, a linear mixed-effects model is
based on two vector-valued random variables: the $q$-dimensional
vector of random effects, $\bc B$, and the $n$-dimensional response
vector, $\bc Y$.  Equation (\ref{eq:LMMdist}) defines the unconditional distribution of $\bc B$ and the
conditional distribution of $\bc Y$, given $\bc B=\vec b$, as
multivariate Gaussian distributions of the form
\begin{displaymath}
  \begin{aligned}
    (\bc Y|\bc B=\vec b)&\sim\mathcal{N}(\vec X\vec\beta+\vec Z\vec
    b,\sigma^2\vec I)\\
    \bc{B}&\sim\mathcal{N}(\vec0,\Sigma_\theta) .
  \end{aligned}
\end{displaymath}

The $q\times q$, symmetric, variance-covariance matrix,
$\mathrm{Var}(\bc B)=\Sigma_\theta$, depends on the
\emph{variance-component parameter vector}, $\vec\theta$, and is
\emph{positive semidefinite}, which means that
\begin{equation}
  \label{eq:posSemiDef}
  \vec b\trans\Sigma_\theta\vec b\ge0,\quad\forall\,\vec b\ne\vec 0 .
\end{equation}
(The symbol $\forall$ denotes ``for all''.)  The fact that
$\Sigma_\theta$ is positive semidefinite does not guarantee that
$\Sigma_\theta^{-1}$ exists.  We would need a stronger property, $\vec
b\trans\Sigma_\theta\vec b>0,\,\forall\,\vec b\ne\vec 0$, called
positive definiteness, to ensure that $\Sigma_\theta^{-1}$ exists.

Many computational formulas for linear mixed models are written in
terms of $\Sigma_\theta^{-1}$.  Such formulas will become unstable as
$\Sigma_\theta$ approaches singularity. And it can do so.  It is a
fact that singular (i.e. non-invertible) $\Sigma_\theta$ can and do
occur in practice, as we have seen in some of the examples in earlier
chapters.  Moreover, during the course of the numerical optimization
by which the parameter estimates are determined, it is frequently the
case that the deviance or the REML criterion will need to be evaluated
at values of $\vec\theta$ that produce a singular $\Sigma_\theta$.
Because of this we will take care to use computational methods that
can be applied even when $\Sigma_\theta$ is singular and are stable as
$\Sigma_\theta$ approaches singularity.

As defined in (\ref{eq:relcovfac}) a relative covariance factor,
$\Lambda_\theta$, is any matrix that satisfies
\begin{displaymath}
  \Sigma_\theta=\sigma^2\Lambda_\theta\Lambda_\theta\trans .
\end{displaymath}
According to this definition, $\Sigma$ depends on both $\sigma$ and
$\theta$ and we should write it as $\Sigma_{\sigma,\theta}$. However,
we will blur that distinction and continue to write $\text{Var}(\bc
B)=\Sigma_\theta$.  Another technicality is that the \emph{common
  scale parameter}, $\sigma$, can, in theory, be zero.  We will show
that in practice the only way for its estimate, $\widehat{\sigma}$, to
be zero is for the fitted values from the fixed-effects only, $\vec
X\widehat{\vec\beta}$, to be exactly equal to the observed data.  This
occurs only with data that have been (incorrectly) simulated without
error.  In practice we can safely assume that $\sigma>0$.  However,
$\Lambda_\theta$, like $\Sigma_\theta$, can be singular.

Our computational methods are based on $\Lambda_\theta$ and do not
require evaluation of $\Sigma_\theta$.  In fact, $\Sigma_\theta$ is
explicitly evaluated only at the converged parameter estimates.

The spherical random effects, $\bc U\sim\mathcal{N}(\vec
0,\sigma^2\vec I_q)$, determine $\bc B$ as
\begin{equation}
  \label{eq:sphericalRE}
  \bc B=\Lambda_\theta\bc U .
\end{equation}
Although it may seem more intuitive to write $\bc U$ as a linear
transformation of $\bc B$, we cannot do that when $\Lambda_\theta$ is
singular, which is why (\ref{eq:sphericalRE}) is in the form shown.

We can easily verify that (\ref{eq:sphericalRE}) provides the desired
distribution for $\bc B$.  As a linear transformation of a
multivariate Gaussian random variable, $\bc B$ will also be
multivariate Gaussian.  Its mean and variance-covariance matrix are
straightforward to evaluate,
\begin{align}
  \mathrm{E}[\bc B]& = \Lambda_\theta\mathrm{E}[\bc U]=\Lambda_\theta\vec0=\vec0\\
  \mathrm{Var}(\bc B)&
  \begin{aligned}[t]
    &=\mathrm{E}\left[(\bc B-\mathrm{E}[\bc B])
      (\bc B-\mathrm{E}[\bc B])\trans\right]
    =\mathrm{E}\left[\bc B\bc B\trans\right]\\
    &=\mathrm{E}\left[\Lambda_\theta\,\bc U\bc U\trans\Lambda_\theta\trans\right]
    =\Lambda_\theta\,\mathrm{E}[\bc U\bc U\trans]\Lambda_\theta\trans
    =\Lambda_\theta\,\mathrm{Var}(\bc U)\Lambda_\theta\trans\\
    &=\Lambda_\theta\,\sigma^2\vec I_q\,\Lambda_\theta\trans
    =\sigma^2\Lambda_\theta\Lambda_\theta\trans
    =\Sigma_\theta
  \end{aligned},
\end{align}
and have the desired form.

Just as we concentrate on how $\vec\theta$ determines
$\Lambda_\theta$, not $\Sigma_\theta$, we will concentrate on properties of
$\bc U$ rather than $\bc B$.  In particular, we now define the
model according to the distributions
\begin{equation}
  \label{eq:condYgivenU}
  \begin{aligned}
  (\bc Y|\bc U=\vec u)&\sim\mathcal{N}(\vec Z\Lambda_\theta\vec
  u+\vec X\beta,\sigma^2\vec I_n)\\
  \bc U&\sim\mathcal{N}(\vec0,\sigma^2\vec I_q) .
  \end{aligned}
\end{equation}

To allow for extensions to other types of mixed models we distinguish
between the \emph{linear predictor}
\begin{equation}
  \label{eq:linearpred}
  \vec\gamma = \vec Z\Lambda_\theta\vec u+\vec X\beta
\end{equation}
and the \emph{conditional mean} of $\bc Y$, given $\bc U=\vec u$,
which is
\begin{equation}
  \label{eq:conditionalMean}
  \vec\mu = \mathrm{E}\left[\bc Y|\bc U=\vec u\right] .
\end{equation}
For a linear mixed model $\vec\mu=\vec\gamma$.  In other forms of
mixed models the conditional mean, $\vec\mu$, can be a nonlinear
function of the linear predictor, $\vec\gamma$.  For some models the
dimension of $\vec\gamma$ is a multiple of $n$, the dimension of
$\vec\mu$ and $\vec y$, but for a linear mixed model the dimension of
$\vec\gamma$ must be $n$.  Hence, the model matrices $\vec Z$ must be
$n\times q$ and $\vec X$ must be $n\times p$.

%% \bc inside \section{...} + \usepackage{hyperref} fails -> using \mathcal here
\section{The Conditional Distribution $(\mathcal{U}|\mathcal{Y}=\vec y)$}
\label{sec:conddistUgivenY}

In this section it will help to be able to distinguish between the
observed response vector and an arbitrary value of $\bc Y$.  For this
section only we will write the observed data vector as $\vec
y_{\text{obs}}$, whereas $\vec y$ without the subscript will refer to an
arbitrary value of the random variable $\bc Y$.

The likelihood of the parameters, $\vec\theta$, $\vec\beta$, and
$\sigma$, given the observed data, $\vec y_{\text{obs}}$, is the
probability density of $\bc Y$, evaluated at $\vec y_{\text{obs}}$.
Although the numerical values of the probability density and the
likelihood are identical, the interpretations of these functions are
different.  In the density we consider the parameters to be fixed
and the value of $\vec y$ as varying.  In the likelihood we consider
$\vec y$ to be fixed at $\vec y_{\text{obs}}$ and the
parameters, $\vec\theta$, $\vec\beta$ and $\sigma$ as varying.

The natural approach for evaluating the likelihood is to determine the
marginal distribution of $\bc Y$, which in this case amounts to
determining the marginal density of $\bc Y$, and evaluate that density
at $\vec y_{\text{obs}}$.  To follow this course we would first
determine the joint density of $\bc U$ and $\bc Y$, written $f_{\bc
  U,\bc Y}(\vec u,\vec y)$, then integrate this density with respect
$\vec u$ to create the marginal density, $f_{\bc Y}(\vec y)$, and
finally evaluate this marginal density at $\vec y_{\text{obs}}$.

To allow for later generalizations we will change the order of these
steps slightly.  We evaluate the joint density function, $f_{\bc U,\bc
  Y}(\vec u,\vec y)$, at $\vec y_{\text{obs}}$, producing the
\emph{unnormalized conditional density}, $h(\vec u)$.  We say that $h$
is ``unnormalized'' because the conditional density is a multiple of $h$
\begin{equation}
  \label{eq:conddenUgivenY}
  f_{\bc U|\bc Y}(\vec u|\vec y_{\text{obs}})=\frac
  {h(\vec u)}{\int_{\mathbb{R}^q}h(\vec u)\,d\vec u}  .
\end{equation}
In some theoretical developments the normalizing constant, which is
the integral in the denominator of an
expression like (\ref{eq:conddenUgivenY}), is not of interest. Here
it is of interest because the normalizing constant is
exactly the likelihood that we wish to evaluate,
\begin{equation}
  \label{eq:LMMlikelihood}
  L(\vec\theta,\vec\beta,\sigma|\vec y_{\text{obs}}) =
  \int_{\mathbb{R}^q}h(\vec u)\,d\vec u .
\end{equation}

For a linear mixed model, where all the distributions of interest are
multivariate Gaussian and the conditional mean, $\vec\mu$, is a linear
function of both $\vec u$ and $\vec\beta$, the distinction between
evaluating the joint density at $\vec y_{\text{obs}}$ to produce
$h(\vec u)$ then integrating with respect to $\vec u$, as opposed to
first integrating the joint density then evaluating at $\vec
y_{\text{obs}}$ is not terribly important.  For other mixed models
this distinction can be important.  In particular, generalized linear
mixed models, described in Sect.~\ref{sec:GLMM}, are often used to
model a discrete response, such as a binary response or a count,
leading to a joint distribution for $\bc Y$ and $\bc U$ that is
discrete with respect to one variable, $\vec y$, and continuous with
respect to the other, $\vec u$.  In such cases there isn't a joint
density for $\bc Y$ and $\bc U$.  The necessary distribution theory
for general $\vec y$ and $\vec u$ is well-defined but somewhat awkward
to describe.  It is much easier to realize that we are only interested
in the observed response vector, $\vec y_{\text{obs}}$, not some
arbitrary value of $\vec y$, so we can concentrate on the conditional
distribution of $\bc U$ given $\bc Y=\vec y_{\text{obs}}$.  For all
the mixed models we will consider, the conditional distribution, $(\bc
U|\bc Y=\vec y_{\text{obs}})$, is continuous and both the conditional
density, $f_{\bc U|\bc Y}(\vec u|\vec y_{\text{obs}})$ and its
unnormalized form, $h(\vec u)$, are well-defined.

\section{Integrating $h(\vec u)$ in the Linear Mixed Model}
\label{sec:IntegratingH}

The integral defining the likelihood in (\ref{eq:LMMlikelihood}) has a
closed form in the case of a linear mixed model but not for some of
the more general forms of mixed models.  To motivate methods for
approximating the likelihood in more general situations, we describe
in some detail how the integral can be evaluated using the sparse
Cholesky factor, $\vec L_\theta$, and the conditional mode,
\begin{equation}
  \label{eq:condMode}
  \tilde{\vec u}=\arg\max_{\vec u} f_{\bc U|\bc Y}(\vec u|\vec y)=
  \arg\max_{\vec u} h(\vec u) = \arg\max_{\vec u}
  f_{\bc Y|\bc U}(\vec y|\vec u)\,f_{\bc U}(\vec u)
\end{equation}
The notation $\arg\max_{\vec u}$ means that $\tilde{\vec u}$ is the
value of $\vec u$ that maximizes the expression on the right.

In general, the \emph{mode} of a continuous distribution is the value
of the random variable that maximizes the density.  The value
$\tilde{\vec u}$ is called the conditional mode of $\vec u$, given
$\bc Y=\vec y_{\text{obs}}$, because $\tilde{\vec u}$ maximizes the
conditional density of $\bc U$ given $\bc Y=\vec y_{\text{obs}}$.  The
location of the maximum can be determined by maximizing the
unnormalized conditional density because $h(\vec u)$ is just a
constant multiple of $f_{\bc U|\bc Y}(\vec y|\vec u)$.  The last part
of (\ref{eq:condMode}) is simply a re-expression of $h(\vec u)$ as the
product of $f_{\bc Y|\bc U}(\vec y|\vec u)$ and $f_{\bc U}(\vec u)$.
For a linear mixed model these densities are
\begin{align}
  \label{eq:densYgivenUandU}
  f_{\bc Y|\bc U}(\vec y|\vec u)&=
  \frac{1}{\left(2\pi\sigma^2\right)^{n/2}}
  \exp\left(-\frac{\left\|\vec y-\vec X\vec\beta-\vec Z\Lambda_\theta\vec u\right\|^2}{2\sigma^2}\right)\\
  f_{\bc U}(\vec u)&=
  \frac{1}{\left(2\pi\sigma^2\right)^{q/2}}\exp\left(-\frac{\|\vec u\|^2}
    {2\sigma^2}\right) .
\end{align}

We can simplify the task of maximizing the product of these densities
by transforming to the deviance scale (negative twice the logarithm of
the density).
\begin{equation}
  \label{eq:devh}
  -2\log\left(h(\vec u)\right)=(n+q)\log(2\pi\sigma^2)
  +\frac{\left\|\vec y-\vec X\vec\beta-\vec Z\Lambda_\theta\vec
      u\right\|^2+\|\vec u\|^2}{\sigma^2} .
\end{equation}
Because (\ref{eq:devh}) describes the negative log density,
$\tilde{\vec u}$ will be the value of $\vec u$ that minimizes the
expression on the right of (\ref{eq:devh}).

The only part of the expression in (\ref{eq:devh}) that depends on
$\vec u$ is the numerator of the second term.  Thus
\begin{equation}
  \label{eq:PLSsol}
  \tilde{\vec u}=\arg\min_{\vec u} \left\|
    \vec y-\vec X\vec\beta-\vec Z\Lambda_\theta\vec u\right\|^2+
  \|\vec u\|^2.
\end{equation}
The expression to be minimized, called the \emph{objective function},
is described as a \emph{penalized residual sum of squares} (PRSS) and
the minimizer, $\tilde{\vec u}$, is called the \emph{penalized least
  squares} (PLS) solution.  They are given these names because the
first term in the objective, $\left\| \vec y-\vec X\vec\beta-\vec
  Z\Lambda_\theta\vec u\right\|^2$, is a sum of squared residuals, and
the second term, $\|\vec u\|^2$, is a penalty on the length, $\|\vec
u\|$, of $\vec u$.  Larger values of $\vec u$ (in the sense of greater
lengths as vectors) incur a higher penalty.

The PRSS criterion determining the conditional mode balances fidelity
to the observed data (i.e. producing a small residual sum of squares)
against simplicity of the model (small $\|\vec u\|$).  We refer to
this type of criterion as a smoothing objective, in the sense that it
seeks to smooth out the fitted response by reducing model complexity
while still retaining reasonable fidelity to the observed data.

For the purpose of evaluating the likelihood we will regard the PRSS
criterion as a function of the parameters, given the data, and write
its minimum value as
\begin{equation}
  \label{eq:r2thetabeta}
  r^2_{\theta,\beta}=\min_{\vec u} \left\|
    \vec y-\vec X\vec\beta-\vec Z\Lambda_\theta\vec u\right\|^2+ \|\vec u\|^2.
\end{equation}
Notice that $\vec\beta$ only enters the right hand side of
(\ref{eq:r2thetabeta}) through the linear predictor expression.  We
will see that $\tilde{\vec u}$ can be determined by a direct
(i.e. non-iterative) calculation and, in fact, we can minimize the
PRSS criterion with respect to $\vec u$ and $\vec\beta$ simultaneously
without iterating.  We write this minimum value as
\begin{equation}
  \label{eq:r2theta}
  r^2_\theta=\min_{\vec u,\vec\beta} \left\|
    \vec y-\vec X\vec\beta-\vec Z\Lambda_\theta\vec u\right\|^2+ \|\vec u\|^2.
\end{equation}
The value of $\vec\beta$ at the minimum is called the conditional
estimate of $\vec\beta$ given $\vec\theta$, written
$\widehat{\vec\beta}_\theta$.

\section{Determining the PLS Solutions, $\tilde{\vec u}$ and
  $\widehat{\vec\beta}_\theta$}
\label{sec:PLSsol}

One way of expressing a penalized least squares problem like
(\ref{eq:r2thetabeta}) is by incorporating the penalty as
``pseudo-data'' in a standard least squares problem.  We extend the
``response vector'', which is $\vec y-\vec X\vec\beta$ when we
minimize with respect to $\vec u$ only, with $q$ responses that are 0
and we extend the predictor expression, $\vec Z\Lambda_\theta\vec u$
with $\vec I_q\vec u$.  Writing this as a least squares problem
produces
\begin{equation}
  \label{eq:PLSLMM}
  \tilde{\vec u}=\arg\min_{\vec u}\left\|
    \begin{bmatrix}
      \vec y-\vec X\vec\beta\\
      \vec 0
    \end{bmatrix} -
    \begin{bmatrix}
      \vec Z\Lambda_\theta\\
      \vec I_q
    \end{bmatrix}\vec u\right\|^2
\end{equation}
with a solution that satisfies
\begin{equation}
  \label{eq:LMMPLSsol}
  \left(\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec
    I_q\right)\tilde{\vec u}
  =\Lambda_\theta\trans\vec Z\trans\left[\vec y-\vec X\vec\beta\right]
\end{equation}

To evaluate $\tilde{\vec u}$ we form the \emph{sparse Cholesky
  factor}, $\vec L_\theta$, which is a lower triangular $q\times q$
matrix that satisfies
\begin{equation}
  \label{eq:sparseCholesky}
  \vec L_\theta\vec L_\theta\trans=
  \Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec I_q .
\end{equation}

\subsection{The Fill-reducing Permutation, $\vec P$}
\label{sec:fill-reducingP}

In earlier chapters we have seen that often the random effects vector
is re-ordered before $\vec L_\theta$ is created.  The re-ordering or
permutation of the elements of $\vec u$ and, correspondingly, the
columns of the model matrix, $\vec Z\Lambda_\theta$, does not affect
the theory of linear mixed models but can have a profound effect on
the time and storage required to evaluate $\vec L_\theta$ in large
problems.  We write the effect of the permutation as multiplication by
a $q\times q$ \emph{permutation matrix}, $\vec P$, although in
practice we perform the permutation without ever constructing $\vec
P$.  That is, the matrix $\vec P$ is only a notational convenience only.  The
permutation is determined from the structure of
$\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec I_q$ for
some initial value of $\vec\theta$ in the optimization of the profiled
deviance.  The particular value of $\vec\theta$ used to determine the
fill-reducing permutation does not affect the result as long as
$\vec\theta$ is not on the boundary of the parameter space (i.e. the
values of $\vec\theta$ the generate singular $\Lambda_\theta$
matrices).

Therefore, we redefine the sparse Cholesky factor, $\vec L_\theta$, to
be the sparse lower triangular $q\times q$ matrix that satisfies
\begin{equation}
  \label{eq:sparseCholeskyP}
  \vec L_\theta\vec L_\theta\trans=
   \vec P\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta\vec P\trans
   +\vec I_q .
\end{equation}

Many sparse matrix methods, including the sparse Cholesky
decomposition, are performed in two stages: the \emph{symbolic phase}
in which the locations of the non-zeros in the result are determined
and the \emph{numeric phase} in which the numeric values at these
positions are evaluated.  The symbolic phase for the decomposition
(\ref{eq:sparseCholeskyP}), which includes determining the
permutation, $\vec P$, need only be done once.  Evaluation of $\vec L_\theta$
for subsequent values of $\vec\theta$ requires only the numeric phase,
which is often much faster than the symbolic phase.

The permutation, $\vec P$, serves two purposes.  The first and most
important purpose is to reduce the number of non-zeros in the factor,
$\vec L_\theta$.  The factor is potentially non-zero at every non-zero
location in the lower triangle of the matrix being decomposed.
However, as we saw in Fig.~\ref{fig:fm2LambdaLimage} of
Sect.~\ref{sec:PenicillinModel}, there may be positions in the factor
that get filled-in even though they are known to be zero in the matrix
being decomposed.  The \emph{fill-reducing permutation} is chosen
according to certain heuristics to reduce the amount of fill-in.  We
use the approximate minimal degree (AMD) method described in
\citet{Davis:1996}. After the fill-reducing permutation is determined,
a ``post-ordering'' is applied.  This has the effect of concentrating
the non-zeros near the diagonal of the factor. See
\citet{davis06:csparse_book} for details.

The transpose of a permutation matrix like $\vec P$ is its inverse.
That is $\vec P\trans\vec P=\vec P\vec P\trans=\vec I_q$.
Applying the permutation corresponds to multiplying a vector or matrix
by $\vec P$.  Applying the inverse permutation corresponds to
multiplication by $\vec P\trans$.

\subsection{Solving for $\tilde{\vec u}$}
\label{sec:solvingtildeu}

Obtaining the Cholesky factor, $\vec L_\theta$, may not seem to be
great progress toward determining $\tilde{\vec u}$ because we still must solve
\begin{equation}
  \label{eq:LMMCholsol}
  \vec L_\theta\vec L_\theta\trans\tilde{\vec u}
  =\vec P\Lambda_\theta\trans\vec Z\trans\left[\vec y-\vec X\vec\beta\right]
\end{equation}
for $\tilde{\vec u}$.  However, this is the key step in computational
methods in the \package{lme4} package. The ability to evaluate $\vec
L_\theta$ rapidly for many different values of $\vec\theta$ is what
makes the computational methods in \package{lme4} feasible, even when
applied to very large data sets with complex structure.  Once we
evaluate $\vec L_\theta$ it is straightforward to solve
(\ref{eq:LMMCholsol}) for $\tilde{\vec u}$ because $\vec L_\theta$ is
triangular.

After evaluating $\vec L_\theta$ and using that to solve for
$\tilde{\vec u}$, which also produces $r^2_{\beta,\theta}$, we can
write the PRSS for a general $\vec u$ as
\begin{equation}
  \label{eq:PRSSwithL}
\left\|
    \vec y-\vec X\vec\beta-\vec Z\Lambda_\theta\vec u\right\|^2+ \|\vec u\|^2=
  r^2_{\theta,\beta}+\|\vec L_\theta\trans(\vec u-\tilde{\vec u})\|^2
\end{equation}
which finally allows us to evaluate the likelihood.  We plug the right
hand side of (\ref{eq:PRSSwithL}) into the definition of $h(\vec u)$
and apply the change of variable
\begin{equation}
  \label{eq:changeVar}
  \vec z=\frac{\vec L_\theta\trans(\vec u-\tilde{\vec u})}{\sigma} .
\end{equation}
The determinant of the Jacobian of this transformation,
\begin{equation}
  \label{eq:tranJac}
  \left|\frac{d\vec z}{d\vec u}\right|=
  \left|\frac{\vec L_\theta\trans}{\sigma}\right|=
  \frac{|\vec L_\theta|}{\sigma^q}
\end{equation}
is required for the change of variable in the integral.  We use the
letter $\vec z$ for the transformed value because we will rearrange
the integral to have the form of the integral of the density of the
standard multivariate normal distribution.

Putting all this together gives
\begin{equation}
  \label{eq:hintegral}
  \begin{aligned}
    L(\theta,\beta,\sigma)&=\int_{\mathbb{R}^q}h(\vec u)\,d\vec u\\
    &=\int_{\mathbb{R}^q}\frac{1}{(2\pi\sigma^2)^{(n+q)/2}}
    \exp\left(-\frac{r^2_{\theta,\beta}+\|\vec L_\theta\trans(\vec
        u-\tilde{\vec u})\|^2}{2\sigma^2}\right)d\vec u\\
    &=\frac{\exp\left(-\frac{r^2_{\theta,\beta}}{2\sigma^2}\right)}
    {(2\pi\sigma^2)^{n/2}}\int_{\mathbb{R}^q}\frac{1}{(2\pi)^{q/2}}
    \exp\left(-\frac{\|\vec L_\theta\trans(\vec u-\tilde{\vec
          u})\|^2}{2\sigma^2}\right)
    \frac{|\vec L_\theta|}{|\vec L_\theta|}
    \frac{d\vec
      u}{\sigma^q}\\
    &=\frac{\exp\left(-\frac{r^2_{\theta,\beta}}{2\sigma^2}\right)}
    {(2\pi\sigma^2)^{n/2}|\vec L_\theta|}
    \int_{\mathbb{R}^q}\frac{e^{-\|\vec
        z\|^2/2}}{(2\pi)^{q/2}}\,d\vec z\\
    &=\frac{\exp\left(-\frac{r^2_{\theta,\beta}}{2\sigma^2}\right)}
    {(2\pi\sigma^2)^{n/2}|\vec L_\theta|} .
  \end{aligned}
\end{equation}

The deviance, which is negative twice the (natural)
logarithm of the likelihood, becomes (\ref{eq:LMMdeviance})
\begin{displaymath}
  d(\vec\theta,\vec\beta,\sigma|\vec y)
  =n\log(2\pi\sigma^2)+\log(|\vec L_\theta|^2)+
  \frac{r^2_{\beta,\theta}}{\sigma^2}.
\end{displaymath}
The maximum likelihood estimates of the parameters are those that
minimize this deviance.  Note that evaluating the determinant, $|\vec
L_\theta|$, which may seem formidable, is not. Because $\vec L_\theta$
is triangular, its determinant is simply the product of its diagonal
elements.

Equation (\ref{eq:LMMdeviance}) is a remarkably compact expression,
considering that the class of models to which it applies is very large
indeed.  However, we can do better than this if we notice that
$\vec\beta$ affects (\ref{eq:LMMdeviance}) only through
$r^2_{\beta,\theta}$, and, for any value of
$\vec\theta$, minimizing this expression with respect to $\vec\beta$
is just another least squares problem.  Let
$\widehat{\vec\beta}_\theta$ be the value of $\vec\beta$ that
minimizes the PRSS simultaneously with respect to
$\vec\beta$ and $\vec u$ and let $r^2_\theta$ be the minimum PRSS.
Furthermore, if we set $\widehat{\sigma^2}_\theta=r^2_\theta/n$,
which is the value of $\sigma^2$ that minimizes the deviance for a
given value of $r^2_\theta$, then the \emph{profiled deviance}, which
is a function of $\vec\theta$ only, is
\begin{equation}
  \label{eq:LMMprofdeviance}
  \tilde{d}(\vec\theta|\vec y)
  =\log(|\vec L_\theta|^2)+n\left[1 +
    \log\left(\frac{2 \pi r^2_\theta}{n}\right)\right].
\end{equation}

The profiled deviance (\ref{eq:LMMprofdeviance}) is optimized
numerically with respect to $\vec\theta$ to determine the MLE,
$\widehat{\vec\theta}$.  The MLEs for the other parameters,
$\widehat{\vec\beta}$ and $\widehat{\sigma}$, are the conditional
estimates evaluated at $\widehat{\vec\theta}$.

\subsection{Determining $r^2_\theta$ and $\widehat{\vec\beta}_\theta$}
\label{sec:betahat}

To determine $\tilde{\vec u}$ and $\widehat{\vec\beta}_\theta$
simultaneously we rearrange the terms in (\ref{eq:PLSLMM}) as
\begin{equation}
  \label{eq:PLSLMM1}
  \begin{bmatrix}
    \tilde{\vec u}\\
    \widehat{\vec\beta}_\theta
  \end{bmatrix}
  =\arg\min_{\vec u,\vec\beta}
  \left\|
    \begin{bmatrix}
      \vec y\\
      \vec 0
    \end{bmatrix} -
    \begin{bmatrix}
      \vec Z\Lambda_\theta\vec P\trans & \vec X\\
      \vec I_q &\vec 0
    \end{bmatrix}
    \begin{bmatrix}
      \vec P\vec u\\
      \vec\beta
    \end{bmatrix}
  \right\|^2 .
\end{equation}
The PLS values, $\vec P\tilde{\vec u}$ and $\widehat{\vec\beta}_\theta$, are
the solutions to
\begin{equation}
  \label{eq:bigPLS}
  \begin{bmatrix}
    \vec P\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta\vec
    P\trans+\vec I_q &
    \vec P\Lambda_\theta\trans\vec Z\trans\vec X\\
    \vec X\trans\vec Z\Lambda_\theta\vec P\trans & \vec X\trans\vec X
  \end{bmatrix}
  \begin{bmatrix}
    \vec P\tilde{\vec u}\\
    \widehat{\vec\beta}_\theta
  \end{bmatrix}=
  \begin{bmatrix}
    \vec P\Lambda_\theta\trans\vec Z\trans\vec y\\
    \vec X\trans\vec y .
  \end{bmatrix}
\end{equation}
To evaluate these solutions we decompose the system
matrix as
\begin{equation}
  \label{eq:bigdecomp}
  \begin{bmatrix}
    \vec P\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta\vec
    P\trans+\vec I_q &
    \vec P\Lambda_\theta\trans\vec Z\trans\vec X\\
    \vec X\trans\vec Z\Lambda_\theta\vec P\trans & \vec X\trans\vec X
  \end{bmatrix}
  =
  \begin{bmatrix}
    \vec L_\theta & \vec 0\\
    \vec R_{ZX}\trans & \vec R_X\trans
  \end{bmatrix}
  \begin{bmatrix}
    \vec L_\theta\trans & \vec R_{ZX}\\
    \vec 0 & \vec R_X
  \end{bmatrix}
\end{equation}
where, as before, $\vec L_\theta$, the sparse Cholesky factor, is the
sparse lower triangular $q\times q$ matrix satisfying
(\ref{eq:sparseCholeskyP}). The other two matrices in
(\ref{eq:bigdecomp}): $\vec R_{ZX}$, which is a general $q\times p$
matrix, and $\vec R_X$, which is an upper triangular $p\times p$
matrix, satisfy
\begin{equation}
  \label{eq:RZXdef}
  \vec L_\theta\vec R_{ZX}=\vec P\Lambda_\theta\trans\vec Z\trans\vec X
\end{equation}
and
\begin{equation}
  \label{eq:RXdef}
  \vec R_X\trans\vec R_X=\vec X\trans\vec X-\vec R_{ZX}\trans\vec R_{ZX}
\end{equation}

Those who are familiar with standard ways of writing a Cholesky
decomposition as either $\vec L\vec L\trans$ or $\vec R\trans\vec R$
($\vec L$ is the factor as it appears on the left and $\vec R$ is as
it appears on the right) will notice a notational inconsistency in
(\ref{eq:bigdecomp}).  One Cholesky factor is defined as the lower
triangular fractor on the left and the other is defined as the upper
triangular factor on the right.  It happens that in \R{} the Cholesky
factor of a dense positive-definite matrix is returned as the right
factor, whereas the sparse Cholesky factor is returned as the left
factor.

The extended decomposition (\ref{eq:bigdecomp}) not only provides for
the evaluation of the profiled deviance function,
$\tilde{d}(\vec\theta)$, (\ref{eq:LMMprofdeviance}) but also allows
use to define and evaluate the profiled REML criterion.

\section{The REML Criterion}
\label{sec:REML}

In practice the so-called REML estimates of variance components are
often preferred to the maximum likelihood estimates.  (``REML'' can be
considered to be an acronym for ``restricted'' or ``residual'' maximum
likelihood, although neither term is completely accurate because these
estimates do not maximize a likelihood.) We can motivate the use of
the REML criterion by considering a linear regression model,
\begin{equation}
  \label{eq:20}
  \bc Y\sim\mathcal{N}(\vec X\vec\beta,\sigma^2\vec I_n),
\end{equation}
in which we typically estimate $\sigma^2$ by
\begin{equation}
  \label{eq:21}
  \widehat{\sigma^2_R}=\frac{\|\vec y-\vec X\widehat{\vec\beta}\|^2}{n-p}
\end{equation}
even though the maximum likelihood estimate of $\sigma^2$ is
\begin{equation}
  \label{eq:22}
  \widehat{\sigma^2_{L}}=\frac{\|\vec y-\vec
    X\widehat{\vec\beta}\|^2}{n} .
\end{equation}

The argument for preferring $\widehat{\sigma^2_R}$ to
$\widehat{\sigma^2_{L}}$ as an estimate of $\sigma^2$ is that the
numerator in both estimates is the sum of squared residuals at
$\widehat{\vec\beta}$ and, although the residual vector $\vec y-\vec
X\vec\beta$ is an $n$-dimensional vector, the residual at
$\widehat{\vec\theta}$ satisfies $p$ linearly independent constraints,
$\vec X\trans(\vec y-\vec X\widehat{\vec\beta})=\vec 0$. That is, the
residual at $\widehat{\vec\theta}$ is the projection of the observed
response vector, $\vec y$, into an $(n-p)$-dimensional linear subspace
of the $n$-dimensional response space.  The estimate
$\widehat{\sigma^2_R}$ takes into account the fact that $\sigma^2$ is
estimated from residuals that have only $n-p$ \emph{degrees of
  freedom}.

Another argument often put forward is that $\widehat{\sigma^2_R}$ is
an \emph{unbiased} estimate of $\sigma^2$, in the sense that the
expected value of the estimator is equal to the value of the
parameter.  However, determining the expected value of an estimator
involves integrating with respect to the density of the estimator and
we have seen that densities of estimators of variances will be skewed,
often highly skewed.  It is not clear why we should be interested in
the expected value of a highly skewed estimator.  If we were to
transform to a more symmetric scale, such as the estimator of the
standard deviation or the estimator of the logarithm of the standard
deviation, the REML estimator would no longer be unbiased.
Furthermore, this property of unbiasedness of variance estimators does
not generalize from the linear regression model to linear mixed
models.  This is all to say that the distinction between REML and ML
estimates of variances and variance components is probably less
important that many people believe.

Nevertheless it is worthwhile seeing how the computational techniques
described in this chapter apply to the REML criterion because the REML
parameter estimates $\widehat{\vec\theta}_R$ and
$\widehat{\sigma_R^2}$ for a linear mixed model have the property that
they would specialize to $\widehat{\sigma^2_R}$ from (\ref{eq:21}) for
a linear regression model, as seen in Sect.~\ref{sec:Dyestuff2LMM}.

Although not usually derived in this way, the REML criterion can be
expressed as
\begin{equation}
  \label{eq:23}
  d_R(\vec\theta,\sigma|\vec y)=-2\log
  \int_{\mathbb{R}^p}L(\vec\theta,\vec\beta,\sigma|\vec y)\,d\vec\beta
\end{equation}
on the deviance scale.  The REML estimates $\widehat{\vec\theta}_R$ and
$\widehat{\sigma_R^2}$ minimize $d_R(\vec\theta,\sigma|\vec y)$.

To evaluate this integral we form an expansion, similar to
(\ref{eq:PRSSwithL}), of $r^2_{\theta,\beta}$ about
$\widehat{\vec\beta}_\theta$
\begin{equation}
  \label{eq:rsqbetathetaexp}
  r^2_{\theta,\beta}=r^2_\theta+\|\vec R_X(\vec\beta-\widehat{\vec\beta})\|^2 .
\end{equation}
In the same way that (\ref{eq:PRSSwithL}) was used to simplify the
integral in (\ref{eq:hintegral}), we can derive
\begin{equation}
  \label{eq:betaintegral}
  \int_{\mathbb{R}^p}\frac{\exp\left(-\frac{r^2_{\theta,\beta}}{2\sigma^2}\right)}
  {(2\pi\sigma^2)^{n/2}|\vec L_\theta|} \,d\vec\beta=
  \frac{\exp\left(-\frac{r^2_\theta}{2\sigma^2}\right)}
  {(2\pi\sigma^2)^{(n-p)/2}|\vec L_\theta||\vec R_X|}
\end{equation}
corresponding to a REML criterion on the deviance scale of
\begin{equation}
  \label{eq:REMLdev}
  d_R(\vec\theta,\sigma|\vec y)=(n-p)\log(2\pi\sigma^2)+
  \log\left(|\vec L_\theta|^2|\vec
    R_X|^2\right)+\frac{r^2_\theta}{\sigma^2} .
\end{equation}
Plugging in the conditional REML estimate,
$\widehat{\sigma^2}_R=r^2_\theta/(n-p)$ provides the profiled REML
criterion
\begin{equation}
  \label{eq:24}
  \tilde{d}_R(\vec\theta|\vec y)=
  \log\left(|\vec L_\theta|^2|\vec R_X|^2\right)+(n-p)
  \left[1+\log\left(\frac{2\pi r^2_\theta}{n-p}\right)\right]
\end{equation}

The REML estimate of $\vec\theta$ is
\begin{equation}
  \label{eq:31}
  \widehat{\vec\theta}_R=\arg\min_{\vec\theta}\tilde{d}_R(\vec\theta|\vec y) ,
\end{equation}
and the REML estimate of $\sigma^2$ is the conditional REML estimate of
$\sigma^2$ at $\widehat{\vec\theta}_R$,
\begin{equation}
  \label{eq:REMLsigmasq}
  \widehat{\sigma^2_R}=\frac{r^2_{\widehat\theta_R}}{n-p} .
\end{equation}
It is not entirely clear how one would define a ``REML estimate'' of
$\vec\beta$ because the REML criterion, $d_R(\vec\theta,\sigma|\vec
y)$, defined in (\ref{eq:REMLdev}), does not depend on $\vec\beta$.
However, it is customary (and not unreasonable) to use
$\widehat{\vec\beta}_R=\tilde{\vec\beta}(\widehat{\vec\theta}_R)$ as
the REML estimate of $\vec\beta$.

\section{Generalizing to Other Forms of Mixed Models}
\label{sec:general}

The \package{lme4} package provides \code{R} functions to fit and analyze
linear mixed models, generalized linear mixed models and nonlinear
mixed models.  These models are called \emph{mixed-effects models} or,
more simply, \emph{mixed models} because they incorporate both
\emph{fixed-effects} parameters, which apply to an entire population
or to certain well-defined and repeatable subsets of a population, and
\emph{random effects}, which apply to the particular experimental
units or observational units in the study.  Such models are also
called \emph{multilevel} models because the random effects represent
levels of variation in addition to the per-observation noise term that
is incorporated in common statistical models such as linear
regression models, generalized linear models and nonlinear regression
models.

We begin by describing common properties of these mixed models and the
general computational approach used in the \package{lme4} package. The
estimates of the parameters in a mixed model are determined as the
values that optimize an objective function --- either the likelihood
of the parameters given the observed data, for maximum likelihood
(ML) estimates, or a related objective function called the REML
criterion.  Because this objective function must be evaluated at many
different values of the model parameters during the optimization
process, we focus on the evaluation of the objective function and a
critical computation in this evalution --- determining the solution to a
penalized, weighted least squares (PWLS) problem.

The dimension of the solution of the PWLS problem can be very large,
perhaps in the millions.  Furthermore, such problems must be solved
repeatedly during the optimization process to determine parameter
estimates.  The whole approach would be infeasible were it not for the
fact that the matrices determining the PWLS problem are sparse and we
can use sparse matrix storage formats and sparse matrix computations
\citep{davis06:csparse_book}. In particular, the whole computational
approach hinges on the extraordinarily efficient methods for
determining the Cholesky decomposition of sparse, symmetric,
positive-definite matrices embodied in the CHOLMOD library of C
functions \citep{Cholmod}.

\section{Chapter Summary}
\label{sec:lmmsummary}

A linear mixed model is characterized by the conditional distribution
\begin{equation}
  \label{eq:lmmcond}
  (\bc Y|\bc U=\vec u)\sim\mathcal{N}(\vec\gamma(\vec
  u,\vec\theta,\vec\beta),\sigma^2\vec I_n)\text{ where }
  \vec\gamma(\vec u,\vec\theta,\vec\beta)=\vec Z\Lambda(\vec\theta)\vec
  u+\vec X\vec\beta
\end{equation}
and the unconditional distribution
$\bc U\sim\mathcal{N}(\vec 0,\sigma^2\vec I_q)$.  The discrepancy
function,
\begin{displaymath}
  d(\vec u|\vec y,\vec\theta,\vec\beta)=
  \left\|\vec y-\vec\gamma(\vec u,\vec\theta,\vec\beta)\right\|^2+\|\vec u\|^2,
\end{displaymath}
is minimized at the conditional mode, $\tilde{\vec u}(\vec\theta)$, and
the conditional estimate, $\tilde{\vec\beta}(\vec\theta)$, which are the
solutions to the sparse, positive-definite linear system
\begin{displaymath}
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec Z\Lambda(\theta)
    +\vec I_q&\Lambda\trans(\vec\theta)\vec Z\trans\vec X\\
    \vec X\trans\vec Z\Lambda(\theta) &\vec X\trans\vec X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\\tilde{\vec\beta}(\vec\theta)
  \end{bmatrix} =
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec y\\
    \vec X\trans\vec y
  \end{bmatrix} .
\end{displaymath}
In the process of solving this system we create the sparse left
Cholesky factor, $L_{\vec Z}(\vec\theta)$, which is a lower triangular
sparse matrix satisfying
\begin{displaymath}
  \vec L_{\vec Z}(\vec\theta)\vec L_{\vec Z}(\vec\theta)\trans=\vec P_{\vec
    Z}\left(\Lambda\trans(\vec\theta)\vec Z\trans\vec
    Z\Lambda(\theta)+\vec I_q\right)\vec P_{\vec Z}\trans
\end{displaymath}
where $\vec P_{\vec Z}$ is a permutation matrix representing a
fill-reducing permutation formed from the pattern of nonzeros in $\vec
Z\Lambda(\vec\theta)$ for any $\vec\theta$ not on the boundary of the
parameter region.  (The values of the nonzeros depend on $\vec\theta$
but the pattern doesn't.)

The profiled log-likelihood, $\tilde{\ell}(\vec\theta|\vec y)$, is
\begin{displaymath}
    -2\tilde{\ell}(\vec\theta|\vec y)=
  \log(|\vec L_{\vec Z}(\vec\theta)|^2)+
  n\left(1+\log\left(\frac{2\pi\tilde{d}(\vec y,\vec\theta)}{n}\right)\right)
\end{displaymath}
where $\tilde{d}(\vec y,\vec\theta)=d(\tilde{\vec u}(\vec\theta)|\vec
y,\tilde{\vec\beta}(\vec\theta),\vec\theta)$.
