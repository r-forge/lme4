\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=8,strip.white=TRUE}
\SweaveOpts{prefix=TRUE,prefix.string=figs/Long,include=TRUE}
\setkeys{Gin}{width=\textwidth}

<<preliminaries,echo=FALSE,print=FALSE,results=hide>>=
options(width=80, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
#library(splines)
#library(lattice)
#library(Matrix)
#library(lme4a)
@

\chapter{Computational methods}
\label{chap:computational}
\abstract*{
  The \package{lme4} package provides R functions to fit and analyze
  several different types of mixed-effects models, including linear
  mixed models, generalized linear mixed models and nonlinear mixed
  models.  In this vignette we describe the formulation of these
  models and the computational approach used to evaluate or
  approximate the log-likelihood of a model/data/parameter value
  combination.}

In this chapter we describe some of the details of the theoretical
development and the computational methods for fitting mixed-effects
models as implemented in the \package{lme4} package.  We also provide
the basis for later generalizations to models for non-Gaussian
responses and to models in which the relationship between the
conditional mean, $\vec\mu$, and the linear predictor, $\vec
X\vec\beta+\vec Z\vec b$ is a nonlinear relationship.

This material is directed at those readers who wish to follow the
theory and methodology and especially how both can be extended to
other types of models.  Readers who are less interested in the ``how''
and the ``why'' of fitting mixed models than in the results themselves
should not feel obligated to master these details.

\section{Methods for linear mixed models}
\label{sec:LMMtheory}

We begin by reviewing the definition of linear mixed-effects models
and some of the basics of the computational methods, as given in
Sect.~\ref{sec:memod}.

\subsection{Definitions and basic results.}
\label{sec:defnLMM}

As described in Sect.~\ref{sec:memod}, a linear mixed-effects
model is based on two vector-valued random variables: the
$q$-dimensional vector of random effects, $\bc B$, and the
$n$-dimensional response vector, $\bc Y$.  The unconditional
distribution of $\bc B$ and the conditional distribution of $\bc Y$,
given $\bc B=\vec b$, are described in (\ref{eq:LMMdist}) as
multivariate Gaussian distributions of the form
\begin{equation}
  \label{eq:LMMdist}
  \begin{aligned}
    (\bc Y|\bc B=\vec b)&\sim\mathcal{N}(\vec X\vec\beta+\vec Z\vec
    b,\sigma^2\vec I)\\
    \bc{B}&\sim\mathcal{N}(\vec0,\Sigma_\theta) .
  \end{aligned}
\end{equation}

The $q\times q$, symmetric, variance-covariance matrix,
$\mathrm{Var}(\bc B)=\Sigma_\theta$, depends on the
\emph{variance-component parameter vector}, $\vec\theta$, and is
\emph{positive semidefinite}, which means that
\begin{equation}
  \label{eq:posSemiDef}
  \vec b\trans\Sigma_\theta\vec b\ge0\quad\forall\vec b\ne\vec 0 .
\end{equation}
This property does not guarantee that $\Sigma_\theta^{-1}$ exists.
Indeed, singular (non-invertible) $\Sigma_\theta$ can and do occur in
practice, as we have seen in some of the examples in earlier
chapters.  Moreover, during the course of the numerical optimization
by which the parameter estimates are determined it is frequently the
case that the profiled deviance or the profiled REML criterion will be
evaluated at values of $\vec\theta$ that produce singular $\Sigma_\theta$.

The scalar parameter, $\sigma>0$, which is the standard deviation of
the residual or per-observation noise terms, is called the
\emph{common scale parameter}.  Equation~(\ref{eq:relcovfac}) defines the
relative covariance factor, $\Lambda_\theta$, as any matrix that satisfies
\begin{equation}
  \label{eq:relcovfac}
  \Sigma_\theta=\sigma^2\Lambda_\theta\Lambda_\theta\trans .
\end{equation}
(Technically, we should write the variance-covariance matrix as
$\Sigma_{\sigma,\theta}$ and not just $\Sigma_\theta$, but we will
blur that distinction and continue to use $\Sigma_\theta$.)  
The matrix $\Lambda_\theta$ can be singular.

Our computational methods employ $\Lambda_\theta$ directly.  That is,
the effect of the parameter $\vec\theta$ is only through the way it
determines $\Lambda_\theta$.  The matrix $\Sigma$ in explicitly
evaluated at the converged parameter estimates only.

The spherical random effects, $\bc U\sim\mathcal{N}(\vec
0,\sigma^2\vec I_q)$, determine $\bc B$ as
\begin{equation}
  \label{eq:sphericalRE}
  \bc B=\Lambda_\theta\bc U .
\end{equation}
(Although it may seem more reasonable to write $\bc U$ as a linear
transformation of $\bc B$, we cannot do that when $\Lambda_\theta$ is
singular so we must define the transformation this way.)

We can easily verify that (\ref{eq:sphericalRE}) provides the desired
distribution for $\bc B$.  As a linear transformation of a
multivariate Gaussian random variable, $\bc B$ will be multivariate
Gaussian with mean
\begin{displaymath}
  \mathrm{E}[\bc B] = \Lambda_\theta\mathrm{E}[\bc U]=\Lambda_\theta\vec0=\vec0
\end{displaymath}
and variance-covariance matrix
\begin{displaymath}
  \begin{aligned}
    \mathrm{Var}(\bc B)&=\mathrm{E}\left[(\bc B-\mathrm{E}[\bc B])
      (\bc B-\mathrm{E}[\bc B])\trans\right]\\
    &=\mathrm{E}\left[\bc B\bc B\trans\right]\\
    &=\mathrm{E}\left[\Lambda_\theta\,\bc U\bc U\trans\Lambda_\theta\right]\\
    &=\Lambda_\theta\,\mathrm{E}[\bc U\bc U\trans]\Lambda_\theta\trans\\
    &=\Lambda_\theta\,\mathrm{Var}(\bc U)\Lambda_\theta\\
    &=\Lambda_\theta\,\sigma^2\vec I_q\,\Lambda_\theta\\
    &=\sigma^2\Lambda_\theta\Lambda_\theta\trans\\
    &=\Sigma_\theta .
  \end{aligned}
\end{displaymath}

Just as we concentrate on how $\vec\theta$ determines
$\Lambda_\theta$, not $\Sigma$, we will concentrate on properties of
$\bc U$ rather than on $\bc B$.  In particular, we now define the
model according to the distributions
\begin{equation}
  \label{eq:condYgivenU}
  \begin{aligned}
  (\bc Y|\bc U=\vec u)&\sim\mathcal{N}(\vec Z\vec\Lambda_\theta\vec
  u+\vec X\beta,\sigma^2\vec I_n)\\
  \bc U&\sim\mathcal{N}(\vec0,\sigma^2\vec I_q) .
  \end{aligned}
\end{equation}

To allow for extensions to other types of mixed models we distinguish
between the \emph{linear predictor}
\begin{equation}
  \label{eq:linearpred}
  \vec\gamma = \vec Z\vec\Lambda_\theta\vec u+\vec X\beta
\end{equation}
and the \emph{conditional mean} of $\bc Y$, given $\bc U=\vec u$,
which is
\begin{equation}
  \label{eq:conditionalMean}
  \vec\mu = \mathrm{E}\left[\bc Y|\bc U=\vec u\right] .
\end{equation}
In a linear mixed model $\vec\mu=\vec\gamma$ and, because $\vec\mu$ is
$n$-dimensional, the model matrices $\vec Z$ and $\vec X$ must be
$n\times q$ and $n\times p$, respectively.

\subsection{The conditional distribution $(\bc U|\bc Y=\vec y)$}
\label{sec:conddistUgivenY}

In this section it will help to be able to distinguish between the
observed response vector and an arbitrary value of $\bc Y$.  For this
section only we will write the observed data vector as $\vec
y_{\text{obs}}$ whereas $\vec y$ without the subscript will refer to an
arbitrary value of the random variable $\bc Y$.
The likelihood of the parameters, $\vec\theta$, $\vec\beta$, and
$\sigma$, given the observed data, $\vec y_{\text{obs}}`w$, is defined as the
probability density of $\bc Y$, which is a function of the parameters,
evaluated at $\vec y_{\text{obs}}$.  

The natural approach to evaluating the likelihood is to determine the
marginal (or unconditional) distribution of $\bc Y$, which in this
case amounts to determining the marginal density of $\bc Y$, and
evaluate that density at $\vec y_{\text{obs}}$.  To follow this course
we would first determine the joint density of $\bc U$ and $\bc Y$,
written $f_{\bc u,\bc y}(\vec u,\vec y)$, then integrate this density
with respect $\bc u$ to create the marginal density, $f_{\bc Y}(\vec
y)$, then evaluate the marginal density at $\bc y_{\text{obs}}$.  To
allow for later generalizations we will change the order of these
steps slightly.  We evaluate the joint density function, $f_{\bc U,\bc
  Y}(\vec u,\vec y)$, at $\vec y_{\text{obs}}$, producing the
\emph{unnormalized conditional density}, $h(\vec u)$.  We say that $h$
is ``unnormalized'' because it is a multiple of the conditional
density,
\begin{equation}
  \label{eq:conddenUgivenY}
  f_{\bc U|\bc Y}(\vec u|\vec y_{\text{obs}})=\frac
  {h(\vec u)}{\int_{\mathbb{R}^q}h(\vec u)\,d\vec u}  .
\end{equation}
In some theoretical developments the normalizing constant, which is
the integral in the denominator of an
expression like (\ref{eq:conddenUgivenY}), is not of interest.  In
this case it is of interest because the normalizing constant is
exactly the likelihood that we wish to evaluate,
\begin{equation}
  \label{eq:LMMlikelihood}
  L(\vec\theta,\vec\beta,\sigma|\vec y_{\text{obs}}) = 
  \int_{\mathbb{R}^q}h(\vec u)\,d\vec u .
\end{equation}

For a linear mixed model, where all the distributions of interest are
multivariate Gaussian and the conditional mean, $\vec\mu$, is a linear
function of both $\vec u$ and $\vec\beta$, the distinction between
evaluating the joint density at $\vec y_{\text{obs}}$ to produce
$h(\vec u)$ then integrating with respect to $\vec u$, as opposed to
first integrating the joint density then evaluating at $\vec
y_{\text{obs}}$ is not terribly important.  For other mixed models
this distinction can be important.  In particular, generalized linear
mixed models, described in Sect.~\ref{sec:GLMM}, can be used to model
a discrete response, such as a binary response or a count, leading to
a joint distribution for $\bc Y$ and $\bc U$ that is discrete in one
direction and continuous in the other.  But even in cases like this
the conditional distribution $(\bc U|\bc Y=\vec y_{\text{obs}}$ is
continuous and both the conditional density, $f_{\bc U|\bc Y}(\vec
u|\vec y_{\text{obs}})$ and it unnormalized form, $h(\vec u)$, are
well-defined.

\subsection{Integrating $h(\vec u)$ in the linear mixed model}
\label{sec:IntegratingH}

The integral defining the likelihood in (\ref{eq:LMMlikelihood}) has a
closed form in the case of a linear mixed model but not for some of
the more general forms of mixed models.  To motivate methods for
approximating the likelihood in more general situations, we describe
in some detail how the integral can be evaluated using the sparse
Cholesky factor, $\vec L_\theta$, and the conditional mode,
\begin{equation}
  \label{eq:condMode}
  \tilde{\vec u} 
\end{equation}

Alternatively, we can consider the conditional distribution $(\bc
U|\bc Y=\vec y)$, which is described by its density function
$f_{\bc U|\bc Y}(\vec u|\vec y)$.  We can evaluate this density
function, up to a scale factor, as the product of the unconditional
density of $\bc U$, written $f_{\bc U}(\vec u)$, and the conditional
density of $(\bc Y|\bc U=\vec u)$.  That is, if we let $h(\vec u)$


parameters, given the data, is
\begin{equation}
  \label{eq:likelihood}
  L(\vec\theta,\vec\beta,\sigma|\vec y)=
  \int_\mathbb{R}^q f_{\bc U|\bc Y}(\vec u|\vec y)\,d\vec u
\end{equation}


In all the forms of mixed models that we will consider, the random
effects vector has a multivariate Gaussian (also called ``normal'')
distribution with mean vector $\vec 0$ and with a   The notation indicates that the matrix $\Sigma$
depends on a parameter vector that is written as $\vec\theta$ and
which we call the \emph{variance-component parameters}, even though
some of the elements of $\vec\theta$ may determine covariances and not
variances.

We write this distribution as
\begin{equation}
  \label{eq:unconditionalB}
  \bc{B}\sim\mathcal{N}(\vec0,\Sigma_\theta) .
\end{equation}
Because this distribution does not depend on the value, $\vec y$, of
the random variable, $\bc Y$, we say it is the \emph{unconditional}
distribution of $\bc B$.

To describe the probability model for the response, $\bc Y$, we
provide the conditional distribution $(\bc Y|\bc B=\vec b)$, which is
the distribution of the response vector, $\bc Y$, assuming that the
value of the random effects vector, $\bc B$, is known to be $\vec b$.
(In practice we never know the value $\vec b$ but, for the purpose of
formulating the model, we will assume that we do.)  For all the forms
of mixed models that we will consider, $\vec b$ changes the
conditional distribution of $\bc Y$ by changing the conditional mean,
$\vec\mu_{\bc Y|\bc B=\vec b}$.  Furthermore, the conditional mean
response depends on $\vec b$ and on the fixed-effects parameter vector
$\vec\beta$ only through the \emph{linear predictor}
\begin{equation}
  \label{eq:linpred}
  \vec\gamma = \vec Z\vec b+\vec X\vec\beta
\end{equation}
where $\vec Z$ and $\vec X$ are known \emph{model matrices} of
the appropriate size.

To simplify notation, we will write the conditional mean,
$\vec\mu_{\bc Y|\bc B=\vec b}$, as $\vec\mu$ without the subscript.
Bear in mind that $\vec\mu$ will always represent the $n$-dimension
vector that is the conditional mean of the response random variable,
$\bc Y$, given a particular value, $\vec b$, of the random effects
random variable $\bc B$.

For all the forms of mixed models that will be considered in this book
the fixed-effects parameter, $\vec\beta$, and the random effects
vector, $\vec b$, determine the (conditional) mean, $\vec\mu$ through
the linear predictor, $\vec\gamma$.  In the case of a linear mixed
model $\vec\mu=\vec\gamma$ and the particular form of the conditional
distribution is a ``spherical'' Gaussian.  That is
\begin{equation}
  \label{eq:condmeanY}
  \vec\mu=\vec\gamma=\vec Z\vec b+\vec X\vec\beta
\end{equation}
and
\begin{equation}
  \label{eq:LMMconddist}
  (\bc Y|\bc B=\vec b)\sim\mathcal{N}\left(\vec Z\vec b+\vec
    X\vec\beta,\sigma^2\vec I_n\right) .
\end{equation}

The expression $\vec I_n$ denotes the identity matrix of size $n$.
This is the $n\times n$ matrix whose diagonal elements are all unity
and whose off-diagonal elements are all zero.  The parameter
$\sigma$ is called the \emph{common scale parameter}.  Its square is the
variance of the residual ``noise'' terms that cannot be explained by
other parts of the model.  The name ``spherical'' is applied to
Gaussian distributions of the form (\ref{eq:LMMconddist}) because the
contours of constant probability density are spheres centered at $\vec
Z\vec b+\vec X\vec\beta$ in the $n$-dimensional response space

Because $\vec\mu$ (and, hence, $\vec\gamma$) are $n$-dimensional
vectors, the model matrix $\vec Z$ must be $n\times q$ and the model
matrix $\vec X$ must be $n\times p$, where $p$ is the dimension of the
fixed-effects parameter vector, $\vec\beta$.  For the model fit to the
\code{Dyestuff} data, $p=1$ and $\vec X$ is a $30\times 1$
matrix, all of whose elements are unity.  The fixed-effects term
\code{1} in a model formula generates a column of ones in the
fixed-effects model matrix, $\vec X$.  For the model being considered
here, this column of ones is the only column in $\vec X$.

The form of the random-effects model matrix, $\vec Z$, and the form of
the variance-covariance matrix, $\Sigma_\theta$, and the method by which
$\Sigma_\theta$ is determined from the value of $\vec\theta$ are all based
on the random-effects terms in the model formula.  As stated earlier,
there is one random effects term, \code{(1 | Batch)}, in the formula
for this model.  Random-effects terms are those that contain the
vertical bar, \code{"|"}, character.  The \code{Batch} variable is the
grouping factor for the random effects described by this term.  An
expression for the grouping factor, usually just the name of a
variable, occurs to the right of the vertical bar.  If the expression
on the left of the vertical bar is \code{1}, as it is here, we
describe the term as a \emph{simple, scalar, random-effects term}.
The designation ``scalar'' means there will be exactly one random
effect generated for each level of the grouping factor.  A simple,
scalar term generates a block of indicator columns --- the indicators for
the grouping factor --- in $\vec Z$.  Because there is only one
random-effects term in this model and because that term is a simple,
scalar term, the model matrix $\vec Z$ for this model is the indicator
matrix for the levels of \code{Batch}.

% A sparse matrix is one in which most of the elements are known to be
% zero.  These elements are represented by a \code{"."} in this display.
% The nature of an indicator matrix is such that only one element in
% each row of the indicator matrix (corresponding to a column of \code{Zt}
% shown above) is non-zero.  Sparse matrix
% methods~\citep{davis06:csparse_book} for numerical linear algebra
% provide special techniques for storing and manipulating such matrices.
% These techniques are the basis of the numerical methods implemented in
% \code{lmer}.

When the model contains only one random-effects term and that term is
a simple, scalar term, then the variance-covariance matrix
$\Sigma_\theta$ is a non-negative multiple of $\vec I_q$, the
$q\times q$ identity matrix.  Although we will defer until later the
discussion of the exact form of $\Sigma_\theta$ for other
models, we will now introduce a transformation of
$\Sigma_\theta$ that we will use throughout.

\subsection{The relative covariance factor}
\label{sec:relcovfac}

A variance-covariance matrix such as $\Sigma_\theta$ is required to be
symmetric and \emph{positive semi-definite}, which means, in effect,
that $\Sigma_\theta$ has the matrix equivalent of a ``square root''.
Recall that a scalar variance, such as $\sigma^2$ in
(\ref{eq:LMMconddist}), must be non-negative and that the standard
deviation, $\sigma\ge0$, is the square root of the variance.  A
variance-covariance matrix has a similar property.  There must be a
matrix, say $\Lambda_\theta$, such that when it is multiplied by its
transpose it produces the original matrix, $\Sigma_\theta$.  This is
not quite the same ``squaring'' the matrix $\Lambda_\theta$ because it
is multiplied by its transpose, $\Lambda_\theta\trans$, so as to create a
product that is symmetric, as $\Sigma_\theta$ must be.

It turns out that we can simplify many subsequent formulas if we
define $\Lambda_\theta$ to be a multiple of the square-root
factor, which we call the \emph{relative covariance factor}, defined
to satisfy
\begin{equation}
  \label{eq:relcovfac}
  \Sigma_\theta=\sigma^2\Lambda_\theta
  \Lambda_\theta\trans .
\end{equation}
The scalar $\sigma^2$ is the square of the common scale parameter
introduced in (\ref{eq:LMMconddist}).  The name ``relative covariance
factor'' indicates that $\Lambda_\theta$ is the factor of the
variance-covariance matrix, $\Sigma_\theta$, of $\bc B$ relative to
the variance, $\sigma^2$, in the conditional distribution $(\bc Y|\bc
B=\vec b)$.

For the \code{Dyestuff} model, which has only one simple, scalar
random-effects term, $\Sigma_\theta$ and
$\Lambda_\theta$ are both multiples of $\vec I_6$, the identity
matrix of size six and $\theta$ is a one-dimensional parameter.
Setting $\theta=\frac{\sigma_1}{\sigma}$ we can write
\begin{equation}
  \label{eq:lambdatheta}
  \Lambda_\theta = \theta\vec I_6 \text{ and }\Sigma_\theta=\theta^2\vec I_6
\end{equation}
subject to the constraint that $\theta\ge0$.  Notice that the
constraint allows for $\theta$ to be zero, in which case both
$\Lambda_0$ and $\Sigma_0$ are $6\times6$ matrices of zeros.

\subsection{The ``Spherical'' Random Effects Vector}
\label{sec:sphericalRE}

Because we isolate the effect of $\vec b$ on the conditional
distribution, $(\bc Y|\bc B=\vec b)$, in the linear predictor,
$\vec\gamma$, we can use a linear transformation of $\bc B$ in place
of $\bc B$.  Because we wish to allow for degenerate models, we define
the transformation in the other direction, from a $q$-dimensional
``spherical'' Gaussian random variable, $\bc U$, to $\bc B$ as
\begin{equation}
  \label{eq:3}
  \bc B=\Lambda_\theta\bc U,\quad
  \bc U\sim\mathcal{N}(\vec 0,\sigma^2\vec I_q).
\end{equation}
so the transformation is well-defined, even when $\Lambda_\theta$ is
singular, and produces the desired distribution, $\bc
B\sim\mathcal{N}(\vec 0,\Sigma_\theta)$.

The linear predictor, $\vec\gamma$, can be expressed in terms of $\vec
u$ as
\begin{equation}
  \label{eq:4}
  \vec\gamma=\vec Z\Lambda_\theta\vec u + \vec X\vec\beta.
\end{equation}

Because we observe $\vec y$ and do not observe $\vec b$ or $\vec u$,
the conditional distribution of interest, for the purposes of
statistical inference, is $(\bc U|\bc Y=\vec y)$ (or, equivalently,
$(\bc B|\bc Y=\vec y)$).  For a linear mixed model in which both $\bc
Y$ and $\bc U$ have spherical Gaussian distributions, with the same
scale parameter, $\sigma$, we can explicitly derive the conditional
distribution $(\bc U|\bc Y=\vec y)$.  We will state some of the
characteristics of this distribution here and defer the actual
derivation until Chapter~\ref{chap:computational}.

The conditional distribution, $(\bc U|\bc Y=\vec y)$, is a multivariate
Gaussian whose mean, which we will write as $\tilde{\vec u}$, is the
solution to the penalized least squares problem
\begin{equation}
  \label{eq:LMMcondmode}
  \begin{aligned}
  \tilde{\vec u}&=\arg\min_{\vec u}\left\{\|\vec y-\vec\gamma\|^2+
    \|\vec u\|^2\right\}\\
    &=\arg\min_{\vec u}\left\{\|(\vec y-\vec X\vec\beta)-\vec
      Z\Lambda_\theta\vec u\|^2+
    \|\vec u\|^2\right\} .
  \end{aligned}
\end{equation}
(The notation $\arg\min_{\vec u}$ means that $\tilde{\vec u}$ is the
value of $\vec u$ that minimizes the expression on the right.)  We say
this is a \emph{penalized least squares} problem because it requires
determining the value of $\vec u$ that minimizes the sum of a residual
sum of squares, $\|(\vec y-\vec X\vec\beta)-\vec Z\Lambda_\theta\vec
u\|^2$, plus $\|u\|^2$, which we describe as a penalty on the size of
$\vec u$.

One way of expressing a penalized least squares problem like this is
by incorporating the penalty as ``pseudo-data'' in a standard least
squares problem.  We extend the observed response vector, which in
this case is actually $\vec y-\vec X\vec\beta$ because we are
minimizing with respect to $\vec u$ only, with $q$ responses that are
0 and we extend the predictor expression, $\vec Z\Lambda_\theta\vec u$
with $\vec I_q\vec u$.  Writing this as a least squares problem produces
\begin{equation}
  \label{eq:PLSLMM}
  \tilde{\vec u}=\arg\min_{\vec u}\left\|
    \begin{bmatrix}
      \vec y-\vec X\vec\beta\\
      \vec 0
    \end{bmatrix} -
    \begin{bmatrix}
      \vec Z\Lambda_\theta\\
      \vec I_q
    \end{bmatrix}\vec u\right\|^2
\end{equation}
with a solution that satisfies
\begin{equation}
  \label{eq:LMMPLSsol}
  \left(\Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec
    I_q\right)\tilde{\vec u}
  =\Lambda_\theta\trans\vec Z\trans\left[\vec y-\vec X\vec\beta\right]
\end{equation}

To evaluate $\tilde{\vec u}$ we form the \emph{sparse Cholesky
  factor}, $\vec L_\theta$, which is a lower triangular $q\times q$
matrix that satisfies
\begin{equation}
  \label{eq:sparseCholesky}
  \vec L_\theta\vec L_\theta\trans=
  \Lambda_\theta\trans\vec Z\trans\vec Z\Lambda_\theta+\vec I_q .
\end{equation}
(In chapter~\ref{chap:computational} we will modify this definition slightly by incorporating a
``fill-reducing'' permutation matrix but that is a computational
detail and does not affect the results here.)  Obtaining the Cholesky
factor, $\vec L_\theta$, may not seem to be great progress because we
now need to solve
\begin{equation}
  \label{eq:LMMCholsol}
  \vec L_\theta\vec L_\theta\trans\tilde{\vec u}
  =\Lambda_\theta\trans\vec Z\trans\left[\vec y-\vec X\vec\beta\right]
\end{equation}
for $\tilde{\vec u}$.  However, this is the key step in computational
methods in the \package{lme4} package. The ability to evaluate $\vec
L_\theta$ rapidly for many different values of $\vec\theta$ is what
makes the computational methods in \package{lme4} feasible, even when
applied to very large data sets with complex structure.  Determining
$\tilde{\vec u}$ through (\ref{eq:LMMCholsol}) is a straightforward
process because $\vec L_\theta$ is lower triangular.

After evaluating $\vec L_\theta$ and using that to solve for
$\tilde{\vec u}$, which also produces $r^2_{\beta,\theta}$, the value
of the penalized residual sum of squares (PRSS) at $\tilde{\vec u}$, we can
evaluate the likelihood, which is a function of the parameters,
$\vec\beta$, $\vec\theta$ and $\sigma$, given the observed data, $\vec
y$. On the deviance scale, which is negative twice the (natural)
logarithm of the likelihood, this is
\begin{equation}
  \label{eq:LMMdeviance}
  d(\vec\theta,\vec\beta,\sigma|\vec y)
  =n\log(2\pi\sigma^2)+\log(|\vec L_\theta|^2)+\frac{r^2_{\beta,\theta}}{\sigma^2}.
\end{equation}
%(The notation $|\vec L_\theta|$ denotes the \emph{determinant} of
%$\vec L_\theta$. In the case of a triangular matrix, such as $\vec
%L_\theta$, this is simply the product of the diagonal elements.)
The maximum likelihood estimates of the parameters are those that
minimize the deviance, (\ref{eq:LMMdeviance}).

Equation (\ref{eq:LMMdeviance}) is a remarkably compact expression,
considering that the class of models to which it applies is very large
indeed.  However, we can do better than this if we notice that
$\vec\beta$ enters (\ref{eq:LMMdeviance}) only through the PRSS
$r^2_{\beta,\theta}$, and, for any value of
$\vec\theta$, minimizing this expression with respect to $\vec\beta$
is just another least squares problem.  Let
$\widehat{\vec\beta}_\theta$ be the value of $\vec\beta$ that
minimizes the PRSS simultaneously with respect to
$\vec\beta$ and $\vec u$ and let $r^2_\theta$ be the minimum PRSS.
Furthermore, if we set $\widehat{\sigma^2}_\theta=r^2_\theta/n$,
which is the value of $\sigma^2$ that minimizes the deviance for a
given value of $r^2_\theta$, then the \emph{profiled deviance}, which
is a function of $\vec\theta$ only, is
\begin{equation}
  \label{eq:LMMprofdeviance}
  \tilde{d}(\vec\theta|\vec y)
  =\log(|\vec L_\theta|^2)+n\left[1 +
    \log\left(\frac{2 \pi r^2_\theta}{n}\right)\right].
\end{equation}

\section{Introduction}
\label{sec:intro}

The \package{lme4} package provides \code{R} functions to fit and analyze
linear mixed models, generalized linear mixed models and nonlinear
mixed models.  These models are called \emph{mixed-effects models} or,
more simply, \emph{mixed models} because they incorporate both
\emph{fixed-effects} parameters, which apply to an entire population
or to certain well-defined and repeatable subsets of a population, and
\emph{random effects}, which apply to the particular experimental
units or observational units in the study.  Such models are also
called \emph{multilevel} models because the random effects represent
levels of variation in addition to the per-observation noise term that
is incorporated in common statistical models such as linear
regression models, generalized linear models and nonlinear regression
models.

We begin by describing common properties of these mixed models and the
general computational approach used in the \package{lme4} package. The
estimates of the parameters in a mixed model are determined as the
values that optimize an objective function --- either the likelihood
of the parameters given the observed data, for maximum likelihood
(ML) estimates, or a related objective function called the REML
criterion.  Because this objective function must be evaluated at many
different values of the model parameters during the optimization
process, we focus on the evaluation of the objective function and a
critical computation in this evalution --- determining the solution to a
penalized, weighted least squares (PWLS) problem.

The dimension of the solution of the PWLS problem can be very large,
perhaps in the millions.  Furthermore, such problems must be solved
repeatedly during the optimization process to determine parameter
estimates.  The whole approach would be infeasible were it not for the
fact that the matrices determining the PWLS problem are sparse and we
can use sparse matrix storage formats and sparse matrix computations
\citep{davis06:csparse_book}. In particular, the whole computational
approach hinges on the extraordinarily efficient methods for
determining the Cholesky decomposition of sparse, symmetric,
positive-definite matrices embodied in the CHOLMOD library of C
functions \citep{Cholmod}.

% The three types of mixed models -- linear, generalized linear and
% nonlinear -- share common characteristics in that the model is
% specified in whole or in part by a \emph{mixed model formula} that
% describes a \emph{linear predictor} and a variance-covariance
% structure for the random effects.  In the next section we describe
% the mixed model formula and the forms of these matrices.  The
% following section presents a general formulation of the Laplace
% approximation to the log-likelihood of a mixed model.

% In subsequent sections we describe computational methods for specific
% kinds of mixed models.  In particular, we should how a profiled
% log-likelihood for linear mixed models, and for some nonlinear mixed
% models, can be evaluated exactly.

In the next section we describe the general form of the mixed models
that can be represented in the \package{lme4} package and the 
computational approach embodied in the package.  In the following
section we describe a particular form of mixed model,
called a linear mixed model, and the computational details for those
models.  In the fourth section we describe computational methods for
generalized linear mixed models, nonlinear mixed models and
generalized nonlinear mixed models.


\section{Formulation of mixed models}
\label{sec:form-mixed-models}

A mixed-effects model incorporates two vector-valued random variables:
the $n$-dimensional response vector, $\bc Y$, and the $q$-dimensional
random effects vector, $\bc B$. We observe the value, $\vec y$, of $\bc Y$.
We do not observe the value of $\bc B$.

The random variable $\bc Y$ may be continuous or discrete.  That is,
the observed data, $\vec y$, may be on a continuous scale or they may
be on a discrete scale, such as binary responses or responses
representing a count. In our formulation, the random variable $\bc B$
is always continous.

We specify a mixed model by describing the unconditional distribution
of $\bc B$ and the conditional distribution $(\bc Y|\bc B=\vec b)$.

\subsection{The unconditional distribution of $\bc B$}
\label{sec:uncond-distr-B}

In our formulation, the unconditional distribution of $\bc B$ is
always a $q$-dimensional multivariate Gaussian (or ``normal'')
distribution with mean $\vec 0$ and with a parameterized covariance
matrix,
\begin{equation}
  \label{eq:2}
  \bc B\sim\mathcal{N}\left(\vec 0,\sigma^2\Lambda(\vec\theta)
  \Lambda\trans(\vec\theta)\right) .
\end{equation}

The scalar, $\sigma$, in (\ref{eq:2}), is called the \emph{common scale
  parameter}.  As we will see later, not all types of mixed models
incorporate this parameter. We will include $\sigma^2$ in the general
form of the unconditional distribution of $\bc B$ with the
understanding that, in some models, $\sigma\equiv 1$.

The $q\times q$ matrix $\Lambda(\vec\theta)$, which is a left factor
of the covariance matrix (when $\sigma=1$) or the relative covariance
matrix (when $\sigma\ne 1$), depends on an $m$-dimensional parameter
$\vec\theta$.  Typically $m\ll q$; in the examples we show below it is
always the case that $m<5$, even when $q$ is in the thousands.  The
fact that $m$ is very small is important because, as we shall see,
determining the parameter estimates in a mixed model can be expressed
as an optimization problem with respect to $\vec\theta$ only.

The parameter $\vec\theta$ may be, and typically is, subject to
constraints. For ease of computation, we require that the constraints
be expressed as ``box'' constraints of the form
$\theta_{iL}\le\theta_i\le\theta_{iU},i=1,\dots,m$ for constants
$\theta_{iL}$ and $\theta_{iU}, i=1,\dots,m$.  We shall write the set
of such constraints as $\vec\theta_L\le\vec\theta\le\vec\theta_R$.  The matrix
$\Lambda(\vec\theta)$ is required to be non-singular
(i.e.{} invertible) when $\vec\theta$ is not on the boundary.

\subsection{The conditional distribution,  $(\bc Y|\bc B=\vec b)$}
\label{sec:cond-distr-YB}

The conditional distribution, $(\bc Y|\bc B=\vec b)$, must satisfy:
\begin{enumerate}
\item The conditional mean, $\vec\mu_{\bc Y|\bc B}(\vec b) =
  \mathrm{E}[\bc Y|\bc B=\vec b]$, depends on $\vec b$ only through the
  value of the \emph{linear predictor}, $\vec Z\vec b+\vec X\vec\beta$,
  where $\vec\beta$ is the $p$-dimensional \emph{fixed-effects}
  parameter vector and the \emph{model matrices}, $\vec Z$ and $\vec X$,
  are fixed matrices of the appropriate dimension.  That is, the
  two model matrices must have the same number of rows and must have
  $q$ and $p$ columns, respectively.  The number of rows
  in $\vec Z$ and $\vec X$ is a multiple of $n$, the dimension of $\vec y$.
\item The scalar distributions, $(\mathcal{Y}_i|\bc B=\vec
  b),i=1,\dots,n$, all have the same form and are completely
  determined by the conditional mean, $\vec\mu_{\bc Y|\bc B}(\vec b)$
  and, at most, one additional parameter, $\sigma$, which is the
  common scale parameter.
\item The scalar distributions, $(\mathcal{Y}_i|\bc B=\vec
  b),i=1,\dots,n$, are independent.  That is, the components of
  $\bc Y$ are \emph{conditionally independent} given $\bc B$.
\end{enumerate}

An important special case of the conditional distribution is the
multivariate Gaussian distribution of the form
\begin{equation}
  \label{eq:1}
  (\bc Y|\bc B=\vec b)\sim\mathcal{N}(\vec Z\vec b+\vec
  X\vec\beta,\sigma^2\vec I_n)
\end{equation}
where $\vec I_n$ denotes the identity matrix of size $n$.
In this case the conditional mean, $\vec\mu_{\bc Y|\bc B}(\vec b)$, is
exactly the linear predictor, $\vec Z\vec b+\vec X\vec\beta$, a situation
we will later describe as being an ``identity link'' between the
conditional mean and the linear predictor.  Models with conditional
distribution (\ref{eq:1}) are called \emph{linear mixed models}.

\subsection{A change of variable to ``spherical'' random effects}
\label{sec:change-vari-spher}

Because the conditional distribution $(\bc Y|\bc B=\vec b)$ depends on
$\vec b$ only through the linear predictor, it is easy to express the
model in terms of a linear transformation of $\bc B$.  We define the
linear transformation from a $q$-dimensional ``spherical'' Gaussian
random variable, $\bc U$, to $\bc B$ as
\begin{equation}
  \label{eq:3}
  \bc B=\Lambda(\vec\theta)\bc U,\quad
  \bc U\sim\mathcal{N}(\vec 0,\sigma^2\vec I_q).
\end{equation}
(The term ``spherical'' refers to the fact that contours of constant
probability density for $\bc U$ are spheres centered at the mean ---
in this case, $\vec0$.)

When $\vec\theta$ is not on the boundary this is an invertible
transformation.  When $\vec\theta$ is on the boundary the
transformation can fail to be invertible.  However, we will only need
to be able to express $\bc B$ in terms of $\bc U$ and that
transformation is well-defined, even when $\vec\theta$ is on the
boundary.

The linear predictor, as a function of $\vec u$, is
\begin{equation}
  \label{eq:4}
  \vec\gamma(\vec u)=\vec Z\Lambda(\vec\theta)\vec u
  + \vec X\vec\beta.
\end{equation}
When we wish to emphasize the role of the model parameters,
$\vec\theta$ and $\vec\beta$, in the formulation of $\vec\gamma$, we will
write the linear predictor as $\vec\gamma(\vec u,\vec\theta,\vec\beta)$.

\subsection{The conditional density $(\bc U|\bc Y=\vec y)$}
\label{sec:cond-dens-bc}

Because we observe $\vec y$ and do not observe $\vec b$ or $\vec u$, the
conditional distribution of interest, for the purposes of statistical
inference, is $(\bc U|\bc Y=\vec y)$ (or, equivalently, $(\bc B|\bc
Y=\vec y)$).  This conditional distribution is always a continuous
distribution with conditional probability density $f_{\bc U|\bc
  Y}(\vec u|\vec y)$.

We can evaluate $f_{\bc U|\bc Y}(\vec u|\vec y)$ , up to a constant, as
the product of the unconditional density, $f_{\bc U}(\vec u)$, and the
conditional density (or the probability mass function, whichever is
appropriate), $f_{\bc Y|\bc U}(\vec y|\vec u)$.  We write this
unnormalized conditional density as
\begin{equation}
  \label{eq:5}
  h(\vec u|\vec y,\vec\theta,\vec\beta,\sigma) =
  f_{\bc Y|\bc U}(\vec y|\vec u,\vec\theta,\vec\beta,\sigma)
  f_{\bc U}(\vec u|\sigma) .
\end{equation}

We say that $h$ is the ``unnormalized'' conditional density because
all we know is that the conditional density is proportional to $h(\vec
u|\vec y,\vec\theta,\vec\beta,\sigma)$.  To obtain the conditional
density we must normalize $h$ by dividing by the value of the integral
\begin{equation}
  \label{eq:6}
  L(\vec\theta,\vec\beta,\sigma|\vec y) =
  \int_{\mathbb{R}^q}h(\vec u|\vec y,\vec\theta,\vec\beta,\sigma)\,d\vec u .
\end{equation}
We write the value of the integral (\ref{eq:6}) as
$L(\vec\theta,\vec\beta,\sigma|\vec y)$ because it is exactly the
\emph{likelihood} of the parameters $\vec\theta$, $\vec\beta$ and
$\sigma$, given the observed data $\vec y$.  The \emph{maximum
  likelihood (ML) estimates} of these parameters are the values that
maximize $L$.

\subsection{Determining the ML estimates}
\label{sec:DeterminingML}

The general problem of maximizing $L(\vec\theta,\vec\beta,\sigma|\vec y)$
with respect to $\vec\theta$, $\vec\beta$ and $\sigma$ can be formidable
because each evaluation of this function involves a potentially
high-dimensional integral and because the dimension of $\vec\beta$ can
be large.  However, this general optimization problem can be split
into manageable subproblems.  Given a value of $\vec\theta$ we can
determine the \emph{conditional mode}, $\tilde{\vec u}(\vec\theta)$, of
$\vec u$ and the \emph{conditional estimate},
$\tilde{\vec\beta}(\vec\theta)$ simultaneously using \emph{penalized,
  iteratively re-weighted least squares} (PIRLS).  The conditional
mode and the conditional estimate are defined as
\begin{equation}
  \label{eq:condmode}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\
    \tilde{\vec\beta}(\vec\theta)
  \end{bmatrix}=\arg\max_{\vec u,\vec\beta}h(\vec u|\vec
  y,\vec\theta,\vec\beta,\sigma) .
\end{equation}
(It may look as if we have missed the dependence on $\sigma$ on the
left-hand side but it turns out that the scale parameter does not
affect the location of the optimal values of quantities in the linear
predictor.)

As is common in such optimization problems, we re-express the conditional
density on the \emph{deviance scale}, which is negative twice the
logarithm of the density, where the optimization becomes
\begin{equation}
  \label{eq:condmode2}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\
    \tilde{\vec\beta}(\vec\theta)
  \end{bmatrix}=\arg\min_{\vec u,\vec\beta}-2\log\left(h(\vec u|\vec
  y,\vec\theta,\vec\beta,\sigma)\right) .
\end{equation}
It is this optimization problem that can be solved quite efficiently
using PIRLS.  In fact, for linear mixed models, which are described in
the next section, $\tilde{\vec u}(\vec\theta)$ and
$\tilde{\vec\beta}(\vec\theta)$ can be directly evaluated.

The second-order Taylor series expansion of $-2\log h$ at $\tilde{\vec
  u}(\vec\theta)$ and $\tilde{\vec\beta}(\vec\theta)$ provides the
Laplace approximation to the profiled deviance.  Optimizing this
function with respect to $\vec\theta$ provides the ML estimates of
$\vec\theta$, from which the ML estimates of $\vec\beta$ and $\sigma$
(if used) are derived.

\section{Methods for linear mixed models}
\label{sec:pwls-problem}

As indicated in the introduction, a critical step in our methods for
determining the maximum likelihood estimates of the parameters in a
mixed model is solving a penalized, weighted least squares (PWLS)
problem.  We will motivate the general form of the PWLS problem by
first considering computational methods for linear mixed models that
result in a penalized least squares (PLS) problem.

Recall from \S\ref{sec:cond-distr-YB} that, in a linear mixed model,
both the conditional distribution, $(\bc Y|\bc U=\vec u)$, and the
unconditional distribution, $\bc U$, are spherical Gaussian
distributions and that the conditional mean, $\vec\mu_{\bc Y|\bc U}(\vec
u)$, is the linear predictor, $\vec\gamma(\vec u)$.  Because all the
distributions determining the model are continuous distributions, we
consider their densities.  On the deviance scale these are
\begin{equation}
  \label{eq:7}
  \begin{aligned}
    -2\log(f_{\bc U}(\vec u))&=q\log(2\pi\sigma^2)+\frac{\|\vec u\|^2}{\sigma^2}\\
    -2\log(f_{\bc Y|\bc U}(\vec y|\vec u))&=n\log(2\pi\sigma^2)+
    \frac{\|\vec y-\vec Z\Lambda(\vec\theta)\vec u-\vec X\vec\beta\|^2}{\sigma^2}\\    
    -2\log(h(\vec u|\vec y,\vec\theta,\vec\beta,\sigma))
    &= (n+q)\log(2\pi\sigma^2)+
    \frac{\|\vec y-\vec\gamma(\vec u,\vec\theta,\vec\beta)\|^2+\|\vec
      u\|^2}{\sigma^2}\\
    &= (n+q)\log(2\pi\sigma^2)+
    \frac{d(\vec u|\vec y,\vec\theta,\vec\beta)}{\sigma^2}
  \end{aligned}
\end{equation}

In (\ref{eq:7}) the \emph{discrepancy} function, 
\begin{equation}
  \label{eq:9}
    d(\vec u|\vec y,\vec\theta,\vec\beta) = 
    \|\vec y-\vec\gamma(\vec u,\vec\theta,\vec\beta)\|^2+\|\vec u\|^2
\end{equation}
has the form of a penalized residual sum of squares in that the first
term, $\|\vec y-\vec\gamma(\vec u,\vec\theta,\vec\beta)\|^2$ is
the residual sum of squares for $\vec y$, $\vec u$, $\vec\theta$ and
$\vec\beta$ and the second term, $\|\vec u\|^2$, is a penalty on the
size of $\vec u$.  Notice that the discrepancy does not depend on the
common scale parameter, $\sigma$.  

\subsection{The canonical form of the discrepancy}
\label{sec:conditional-mode-bm}

Using a so-called ``pseudo data'' representation, we can write the
discrepancy as a residual sum of squares for a regression model that
is linear in both $\vec u$ and $\vec\beta$
\begin{equation}
  \label{eq:10}
  d(\vec u|\vec y,\vec\theta,\vec\beta) =\left\|
    \begin{bmatrix} \vec y\\\vec 0 \end{bmatrix} -
    \begin{bmatrix}
      \vec Z\Lambda(\vec\theta) & \vec X \\
      \vec I_q & \vec0
    \end{bmatrix}
    \begin{bmatrix}\vec u\\\vec\beta\end{bmatrix}
  \right\|^2 .
\end{equation}
The term ``pseudo data'' reflects the fact that we have added $q$
``pseudo observations'' to the observed response, $\vec y$, and to the
linear predictor, $\vec\gamma(\vec u,\vec\theta,\vec\beta)=\vec
Z\Lambda(\vec\theta)\vec u+\vec X\vec\beta$, in such a way that their
contribution to the overall residual sum of squares is exactly the
penalty term in the discrepancy.

In the form (\ref{eq:10}) we can see that the discrepancy is a
quadratic form in both $\vec u$ and $\vec\beta$.  Furthermore, because
we require that $\vec X$ has full column rank, the discrepancy is a
positive-definite quadratic form in $\vec u$ and $\vec\beta$ that is
minimized at $\tilde{\vec u}(\vec\theta)$ and
$\tilde{\vec\beta}(\vec\theta)$ satisfying
\begin{equation}
  \label{eq:13}
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec Z\Lambda(\theta)
    +\vec I_q&\Lambda\trans(\vec\theta)\vec Z\trans\vec X\\
    \vec X\trans\vec Z\Lambda(\theta) &\vec X\trans\vec X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\\tilde{\vec\beta}(\vec\theta)
  \end{bmatrix} =
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec y\\
    \vec X\trans\vec y
  \end{bmatrix}
\end{equation}

An effective way of determining the solution to a sparse, symmetric,
positive definite system of equations such as (\ref{eq:13}) is the
sparse Cholesky decomposition \citep{davis06:csparse_book}.  If $\vec A$
is a sparse, symmetric positive definite matrix then the sparse
Cholesky factor with fill-reducing permutation $\vec P$ is the
lower-triangular matrix $\vec L$ such that
\begin{equation}
  \label{eq:14}
  \vec L\vec L\trans=\vec P\vec A\vec P\trans .
\end{equation}
(Technically, the factor $\vec L$ is only determined up to changes in
the sign of the diagonal elements.  By convention we require the
diagonal elements to be positive.)

The fill-reducing permutation represented by the permutation matrix
$\vec P$, which is determined from the pattern of nonzeros in $\vec A$
but does not depend on particular values of those nonzeros, can have a
profound impact on the number of nonzeros in $\vec L$ and hence on the
speed with which $\vec L$ can be calculated from $\vec A$.  

In most applications of linear mixed models the matrix $\vec
Z\Lambda(\vec\theta)$ is sparse while $\vec X$ is dense or close to
it so the permutation matrix $\vec P$ can be restricted to the form
\begin{equation}
  \label{eq:15}
  \vec P=\begin{bmatrix}\vec P_{\vec Z}&\vec0\\
    \vec0&\vec P_{\vec X}\end{bmatrix}
\end{equation}
without loss of efficiency.  In fact, in most cases we can set $\vec
P_{\vec X}=\vec I_p$ without loss of efficiency.

Let us assume that the permutation matrix is required to be of the
form (\ref{eq:15}) so that we can write the Cholesky factorization for
the positive definite system (\ref{eq:13}) as
\begin{multline}
  \label{eq:16}
  \begin{bmatrix}
    \vec L_{\vec Z}&\vec0\\\vec L_{\vec{XZ}}&\vec L_{\vec X}
  \end{bmatrix}
  \begin{bmatrix}
    \vec L_{\vec Z}&\vec0\\\vec L_{\vec{XZ}}&\vec L_{\vec X}
  \end{bmatrix}\trans =\\
  \begin{bmatrix}\vec P_{\vec Z}&\vec0\\
    \vec0&\vec P_{\vec X}\end{bmatrix}
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec Z\Lambda(\theta)
    +\vec I_q&\Lambda\trans(\vec\theta)\vec Z\trans\vec X\\
    \vec X\trans\vec Z\Lambda(\theta) &\vec X\trans\vec X
  \end{bmatrix}
  \begin{bmatrix}\vec P_{\vec Z}&\vec0\\
    \vec0&\vec P_{\vec X}\end{bmatrix}\trans .
\end{multline}
The discrepancy can now be written in the canonical form
\begin{equation}
  \label{eq:17}
  d(\vec u|\vec y,\vec\theta,\vec\beta) =\tilde{d}(\vec y,\vec\theta) +
  \left\|
    \begin{bmatrix}
      \vec L_{\vec Z}\trans&\vec L_{\vec{XZ}}\trans\\
      \vec 0&\vec L_{\vec X}\trans
    \end{bmatrix}
    \begin{bmatrix}
      \vec P_{\vec Z}(\vec u-\tilde{\vec u})\\
      \vec P_{\vec X}(\vec\beta-\tilde{\vec\beta})
    \end{bmatrix}
  \right\|^2
\end{equation}
where
\begin{equation}
  \label{eq:18}
  \tilde{d}(\vec y,\vec\theta)=
  d(\tilde{\vec u}(\vec\theta)|\vec y,\vec\theta,\tilde{\vec\beta}(\vec\theta))
\end{equation}
is the minimum discrepancy, given $\vec\theta$.


\subsection{The profiled likelihood for linear mixed models}
\label{sec:prof-log-likel}

Substituting (\ref{eq:17}) into (\ref{eq:7}) provides the unnormalized
conditional density $h(\vec u|\vec y,\vec\theta,\vec\beta,\sigma)$ on the
deviance scale as
\begin{multline}
  \label{eq:32}
  -2\log(h(\vec u|\vec y,\vec\theta,\vec\beta,\sigma))\\=
  (n+q)\log(2\pi\sigma^2)+\frac{\tilde{d}(\vec y,\vec\theta) +
  \left\|
    \begin{bmatrix}
      \vec L_{\vec Z}\trans&\vec L_{\vec{XZ}}\trans\\
      \vec 0&\vec L_{\vec X}\trans
    \end{bmatrix}
    \begin{bmatrix}
      \vec P_{\vec Z}(\vec u-\tilde{\vec u})\\
      \vec P_{\vec X}(\vec\beta-\tilde{\vec\beta})
    \end{bmatrix}
  \right\|^2}{\sigma^2} .
\end{multline}
As shown in Appendix \ref{sec:integr-quadr-devi}, the integral of a
quadratic form on the deviance scale, such as (\ref{eq:32}), is easily
evaluated, providing the log-likelihood,
$\ell(\vec\theta,\vec\beta,\sigma|\vec y)$, as
\begin{multline}
  \label{eq:lmmdev}
  -2\ell(\vec\theta,\vec\beta,\sigma|\vec y)\\
  =-2\log\left(L(\vec\theta,\vec\beta,\sigma|\vec y)\right)\\ 
  =n\log(2\pi\sigma^2)+\log(|\vec L_{\vec Z}|^2)+\frac{\tilde{d}(\vec y,\vec\theta) +
  \left\|\vec L_{\vec X}\trans\vec P_{\vec X}(\vec\beta-\tilde{\vec\beta})\right\|^2}{\sigma^2},
\end{multline}
from which we can see that the conditional estimate of $\vec\beta$,
given $\vec\theta$, is $\tilde{\vec\beta}(\vec\theta)$ and the conditional
estimate of $\sigma$, given $\vec\theta$, is
\begin{equation}
  \label{eq:condsigma}
  \tilde{\sigma^2}(\vec\theta)= \frac{\tilde{d}(\vec\theta|\vec y)}{n} .
\end{equation}
Substituting these conditional estimates into (\ref{eq:lmmdev}) produces the
\emph{profiled likelihood}, $\tilde{L}(\vec\theta|\vec y)$, as
\begin{equation}
  \label{eq:19}
  -2\tilde{\ell}(\vec\theta|\vec y))=
  \log(|\vec L_{\vec Z}(\vec\theta)|^2)+
  n\left(1+\log\left(\frac{2\pi\tilde{d}(\vec y,\vec\theta)}{n}\right)\right) .
\end{equation}
The maximum likelihood estimate of $\vec\theta$ can then be expressed as
\begin{equation}
  \label{eq:29}
  \widehat{\vec\theta}_L=\arg\min_{\vec\theta}
  \left(-2\tilde{\ell}(\vec\theta|\vec y)\right) .
\end{equation}
from which the ML estimates of $\sigma^2$ and $\vec\beta$ are
evaluated as
\begin{align}
  \label{eq:30}
  \widehat{\sigma^2_L}&=
  \frac{\tilde{d}(\widehat{\vec\theta}_L,\vec y)}{n}\\
  \widehat{\vec\beta}_L&=\tilde{\vec\beta}(\widehat{\vec\theta}_L) .
\end{align}

The important thing to note about optimizing the profiled likelihood,
(\ref{eq:19}), is that it is a $m$-dimensional optimization problem
and typically $m$ is very small.

\subsection{The REML criterion}
\label{sec:reml-criterion}

In practice the so-called REML estimates of variance components are
often preferred to the maximum likelihood estimates.  (``REML'' can be
considered to be an acronym for ``restricted'' or ``residual'' maximum
likelihood, although neither term is completely accurate because these
estimates do not maximize a likelihood.) We can motivate the use of
the REML criterion by considering a linear regression model,
\begin{equation}
  \label{eq:20}
  \bc Y\sim\mathcal{N}(\vec X\vec\beta,\sigma^2\vec I_n),
\end{equation}
in which we typically estimate $\sigma^2$ by
\begin{equation}
  \label{eq:21}
  \widehat{\sigma^2_R}=\frac{\|\vec y-\vec X\widehat{\vec\beta}\|^2}{n-p}
\end{equation}
even though the maximum likelihood estimate of $\sigma^2$ is
\begin{equation}
  \label{eq:22}
  \widehat{\sigma^2_{L}}=\frac{\|\vec y-\vec
    X\widehat{\vec\beta}\|^2}{n} .
\end{equation}

The argument for preferring $\widehat{\sigma^2_R}$ to
$\widehat{\sigma^2_{L}}$ as an estimate of $\sigma^2$ is that the
numerator in both estimates is the sum of squared residuals at
$\widehat{\vec\beta}$ and, although the residual vector $\vec y-\vec
X\vec\beta$ is an $n$-dimensional vector, the residual at
$\widehat{\vec\theta}$ satisfies $p$ linearly independent constraints,
$\vec X\trans(\vec y-\vec X\widehat{\vec\beta})=\vec 0$. That is, the residual at
$\widehat{\vec\theta}$ is the projection of the observed response
vector, $\vec y$, into an $(n-p)$-dimensional linear subspace of the
$n$-dimensional response space.  The estimate $\widehat{\sigma^2_R}$
takes into account the fact that $\sigma^2$ is estimated from
residuals that have only $n-p$ \emph{degrees of freedom}.

The REML criterion for determining parameter estimates
$\widehat{\vec\theta}_R$ and $\widehat{\sigma_R^2}$ in a linear mixed
model has the property that these estimates would
specialize to $\widehat{\sigma^2_R}$ from (\ref{eq:21}) for a linear
regression model.  Although not usually derived in this way, the REML
criterion can be expressed as
\begin{equation}
  \label{eq:23}
  c_R(\vec\theta,\vec\sigma|\vec y)=-2\log
  \int_{\mathbb{R}^p}L(\vec u|\vec
  y,\vec\theta,\vec\beta,\sigma)\,d\vec\beta
\end{equation}
on the deviance scale.  The REML estimates $\widehat{\vec\theta}_R$ and
$\widehat{\sigma_R^2}$ minimize $c_R(\vec\theta,\vec\sigma|\vec y)$.

The profiled REML criterion, a function of $\vec\theta$ only, is
\begin{equation}
  \label{eq:24}
  \tilde{c}_R(\vec\theta|\vec y)=
  \log(|\vec L_{\vec Z}(\vec\theta)|^2|\vec L_{\vec X}(\vec\theta)|^2)+(n-p)
  \left(1+\log\left(\frac{2\pi\tilde{d}(\vec\theta|\vec y)}{n-p}\right)\right)
\end{equation}
and the REML estimate of $\vec\theta$ is
\begin{equation}
  \label{eq:31}
  \widehat{\vec\theta}_R =
  \arg\min_{\vec\theta}\tilde{c}_R(\vec\theta,\vec y) .
\end{equation}
The REML estimate of $\sigma^2$ is
$\widehat{\sigma^2_R}=\tilde{d}(\widehat{\vec\theta}_R|\vec y)/(n-p)$.

It is not entirely clear how one would define a ``REML estimate'' of
$\vec\beta$ because the REML criterion, $c_R(\vec\theta,\vec\sigma|\vec
y)$, defined in (\ref{eq:23}), does not depend on $\vec\beta$.
However, it is customary (and not unreasonable) to use
$\widehat{\vec\beta}_R=\tilde{\vec\beta}(\widehat{\vec\theta}_R)$ as the REML
estimate of $\vec\beta$.

Note that the profiled REML criterion can be evaluated from a sparse
Cholesky decomposition like that in (\ref{eq:16}) but without the
requirement that the permutation can be applied to the columns of $\vec
Z\Lambda(\vec\theta)$ separately from the columnns of $\vec X$.  That
is, we can use a general fill-reducing permutation rather than the
specific form (\ref{eq:15}) with separate permutations represented by
$\vec P_{\vec Z}$ and $\vec P_{\vec X}$. This can be useful in cases where
both $\vec Z$ and $\vec X$ are large and sparse.

\subsection{Summary for linear mixed models}
\label{sec:lmmsummary}

A linear mixed model is characterized by the conditional distribution
\begin{equation}
  \label{eq:lmmcond}
  (\bc Y|\bc U=\vec u)\sim\mathcal{N}(\vec\gamma(\vec
  u,\vec\theta,\vec\beta),\sigma^2\vec I_n)\text{ where }
  \vec\gamma(\vec u,\vec\theta,\vec\beta)=\vec Z\Lambda(\vec\theta)\vec
  u+\vec X\vec\beta
\end{equation}
and the unconditional distribution
$\bc U\sim\mathcal{N}(\vec 0,\sigma^2\vec I_q)$.  The discrepancy
function,
\begin{displaymath}
  d(\vec u|\vec y,\vec\theta,\vec\beta)=
  \left\|\vec y-\vec\gamma(\vec u,\vec\theta,\vec\beta)\right\|^2+\|\vec u\|^2,
\end{displaymath}
is minimized at the conditional mode, $\tilde{\vec u}(\vec\theta)$, and
the conditional estimate, $\tilde{\vec\beta}(\vec\theta)$, which are the
solutions to the sparse, positive-definite linear system
\begin{displaymath}
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec Z\Lambda(\theta)
    +\vec I_q&\Lambda\trans(\vec\theta)\vec Z\trans\vec X\\
    \vec X\trans\vec Z\Lambda(\theta) &\vec X\trans\vec X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\\tilde{\vec\beta}(\vec\theta)
  \end{bmatrix} =
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec y\\
    \vec X\trans\vec y 
  \end{bmatrix} .
\end{displaymath}
In the process of solving this system we create the sparse left
Cholesky factor, $L_{\vec Z}(\vec\theta)$, which is a lower triangular
sparse matrix satisfying
\begin{displaymath}
  \vec L_{\vec Z}(\vec\theta)\vec L_{\vec Z}(\vec\theta)\trans=\vec P_{\vec
    Z}\left(\Lambda\trans(\vec\theta)\vec Z\trans\vec
    Z\Lambda(\theta)+\vec I_q\right)\vec P_{\vec Z}\trans
\end{displaymath}
where $\vec P_{\vec Z}$ is a permutation matrix representing a
fill-reducing permutation formed from the pattern of nonzeros in $\vec
Z\Lambda(\vec\theta)$ for any $\vec\theta$ not on the boundary of the
parameter region.  (The values of the nonzeros depend on $\vec\theta$
but the pattern doesn't.)

The profiled log-likelihood, $\tilde{\ell}(\vec\theta|\vec y)$, is
\begin{displaymath}
    -2\tilde{\ell}(\vec\theta|\vec y)=
  \log(|\vec L_{\vec Z}(\vec\theta)|^2)+
  n\left(1+\log\left(\frac{2\pi\tilde{d}(\vec y,\vec\theta)}{n}\right)\right)
\end{displaymath}
where $\tilde{d}(\vec y,\vec\theta)=d(\tilde{\vec u}(\vec\theta)|\vec
y,\tilde{\vec\beta}(\vec\theta),\vec\theta)$.

\section{Generalizing the discrepancy function}
\label{sec:generalizations}

Because one of the factors influencing the choice of implementation
for linear mixed models is the extent to which the methods can also be
applied to other mixed models, we describe several other classes of
mixed models before discussing the implementation details for linear
mixed models.  At the core of our methods for determining the maximum
likelihood estimates (MLEs) of the parameters in the mixed model are
methods for minimizing the discrepancy function with respect to the
coefficients $\vec u$ and $\vec\beta$ in the linear predictor
$\vec\gamma(\vec u,\vec\theta,\vec\beta)$.

In this section we describe the general form of the discrepancy
function that we will use and a penalized iteratively reweighted least
squares (PIRLS) algorithm for determining the conditional modes
$\tilde{\vec u}(\vec\theta)$ and $\tilde{\vec\beta}(\vec\theta)$.  We then
describe several types of mixed models and the form of the discrepancy
function for each.

\subsection{A weighted residual sum of squares}
\label{sec:weighted}

As shown in \S\ref{sec:conditional-mode-bm}, the discrepancy function
for a linear mixed model has the form of a penalized residual sum of
squares from a linear model (\ref{eq:10}).  In this section we
generalize that definition to
\begin{equation}
  \label{eq:11}
  d(\vec u|\vec y,\vec\theta,\vec\beta) =\left\|\vec W^{1/2}(\vec\mu)
    \left[\vec y-\vec\mu_{\bc Y|\bc U}(\vec u,\vec\theta,\vec\beta)\right]\right\|^2+
  \|\vec 0-\vec u\|^2 .
\end{equation}
where $\vec W$ is an $n\times n$ diagonal matrix, called the
\emph{weights matrix}, with positive diagonal elements and $\vec
W^{1/2}$ is the diagonal matrix with the square roots of the weights
on the diagonal.  The $i$th weight is inversely proportional to
the conditional variances of $(\mathcal{Y}|\bc U=\vec u)$ and may
depend on the conditional mean, $\vec\mu_{\bc Y|\bc U}$.

We allow the conditional mean to be a nonlinear function of the linear
predictor, but with certain restrictions.  We require that the
mapping from $\vec u$ to $\vec\mu_{\bc Y|\bc U=\vec u}$ be expressed as
\begin{equation}
  \label{eq:uGammaEtaMu}
  \vec u\;\rightarrow\;\vec\gamma\;\rightarrow\;\vec\eta\;\rightarrow\;\vec\mu
\end{equation}
where $\vec\gamma=\vec Z\Lambda(\vec\theta)\vec u+\vec X\vec\theta$ is an
$ns$-dimensional vector ($s > 0$) while $\vec\eta$ and $\vec\mu$ are
$n$-dimensional vectors.

The map $\vec\eta\rightarrow\vec\mu$ has the property that $\mu_i$
depends only on $\eta_i$, $i=1,\dots,n$.  The map
$\vec\gamma\rightarrow\vec\eta$ has a similar property in that, if we
write $\vec\gamma$ as an $n\times s$ matrix $\vec\Gamma$ such that
\begin{equation}
  \label{eq:vecGamma}
  \vec\gamma=\vec{\vec\Gamma}
\end{equation}
(i.e.{} concatenating the columns of $\vec\Gamma$ produces $\vec\gamma$)
then $\eta_i$ depends only on the $i$th row of $\vec\Gamma$,
$i=1,\dots,n$.  Thus the Jacobian matrix
$\frac{d\vec\mu}{d\vec\eta\trans}$ is an $n\times n$ diagonal matrix and
the Jacobian matrix $\frac{d\vec\eta}{d\vec\gamma\trans}$ is the
horizontal concatenation of $s$ diagonal $n\times n$ matrices.

For historical reasons, the function that maps $\eta_i$ to $\mu_i$ is
called the \emph{inverse link} function and is written
$\mu=g^{-1}(\eta)$.  The \emph{link function}, naturally, is
$\eta=g(\mu)$.  When applied component-wise to vectors $\vec\mu$ or
$\vec\eta$ we write these as $\vec\eta=\vec g(\vec\mu)$ and $\vec\mu=\vec
g^{-1}(\vec\eta)$.

Recall that the conditional distribution, $(\mathcal{Y}_i|\bc U=\vec
u)$, is required to be independent of $(\mathcal{Y}_j|\bc U=\vec u)$
for $i,j=1,\dots,n,\,i\ne j$ and that all the component conditional
distributions must be of the same form and differ only according to
the value of the conditional mean.

Depending on the family of the conditional distributions, the allowable
values of the $\mu_i$ may be in a restricted range.  For example, if
the conditional distributions are Bernoulli then
$0\le\mu_i\le1,i=1,\dots,n$. If the conditional distributions are
Poisson then $0\le\mu_i,i=1,\dots,n$.  A characteristic of the link
function, $g$, is that it must map the restricted range to an
unrestricted range.  That is, a link function for the Bernoulli
distribution must map $[0,1]$ to $[-\infty,\infty]$ and must be invertible
within the range.

The mapping from $\vec\gamma$ to $\vec\eta$ is defined by a function
$m:\mathbb{R}^s\rightarrow\mathbb{R}$, called the
\emph{nonlinear model} function, such that
$\eta_i=m(\vec\gamma_i),i=1,\dots,n$ where $\vec\gamma_i$ is the $i$th
row of $\vec\Gamma$.  The vector-valued function is $\vec\eta=\vec m(\vec\gamma)$.

Determining the conditional modes, $\tilde{\vec u}(\vec y|\vec\theta)$,
and $\tilde{\vec\beta}(\vec y|\vec\theta)$, that jointly minimize the
discrepancy,
\begin{equation}
  \label{eq:12}
  \begin{bmatrix}
    \tilde{\vec u}(\vec y|\vec\theta)\\
    \tilde{\vec\beta}(\vec y|\vec\theta)
  \end{bmatrix}
  =\arg\min_{\vec u,\vec\beta}\left[(\vec y-\vec\mu)\trans\vec W(\vec
  y-\vec\mu)+\|\vec u\|^2\right]
\end{equation}
becomes a weighted, nonlinear least squares problem except that the
weights, $\vec W$, can depend on $\vec\mu$ and, hence, on $\vec u$ and
$\vec\beta$.  

In describing an algorithm for linear mixed models we called
$\tilde{\vec\beta}(\vec\theta)$ the \emph{conditional estimate}.  That
name reflects that fact that this is the maximum likelihood estimate
of $\vec\beta$ for that particular value of $\vec\theta$.  Once we have
determined the MLE, $\widehat(\vec\theta)_L$ of $\vec\theta$, we have a
``plug-in'' estimator,
$\widehat{\vec\beta}_L=\tilde{\vec\beta}(\vec\theta)$ for $\vec\beta$.

This property does not carry over exactly to other forms of mixed
models.  The values $\tilde{\vec u}(\vec\theta)$ and
$\tilde{\vec\beta}(\vec\theta)$ are conditional modes in the sense that
they are the coefficients in $\vec\gamma$ that jointly maximize the
unscaled conditional density $h(\vec u|\vec
y,\vec\theta,\vec\beta,\sigma)$.  Here we are using the adjective
``conditional'' more in the sense of conditioning on $\bc Y=\vec y$
than in the sense of conditioning on $\vec\theta$, although these
values are determined for a fixed value of $\vec\theta$.

\subsection{The PIRLS algorithm for $\tilde{\vec u}$ and $\tilde{\vec\beta}$}
\label{sec:pirls-algor-tild}

The penalized, iteratively reweighted, least squares (PIRLS) algorithm
to determine $\tilde{\vec u}(\vec\theta)$ and
$\tilde{\vec\beta}(\vec\theta)$ is a form of the Fisher scoring
algorithm.  We fix the weights matrix, $\vec W$, and use penalized,
weighted, nonlinear least squares to minimize the penalized, weighted
residual sum of squares conditional on these weights.  Then we update
the weights to those determined by the current value of $\vec\mu$ and
iterate.

To describe this algorithm in more detail we will use parenthesized
superscripts to denote the iteration number.  Thus $\vec u^{(0)}$ and
$\vec\beta^{(0)}$ are the initial values of these parameters, while
$\vec u^{(i)}$ and $\vec\beta^{(i)}$ are the values at the $i$th
iteration.  Similarly $\vec\gamma^{(i)}=\vec Z\Lambda(\vec\theta)\vec
u^{(i)}+\vec X\vec\beta^{(i)}$, $\vec\eta^{(i)}=\vec m(\vec\gamma^{(i)})$ and
$\vec\mu^{(i)}=\vec g^{-1}(\vec\eta^{(i)})$.

We use a penalized version of the Gauss-Newton algorithm
\citep[ch.~2]{bateswatts88:_nonlin} for which we define the weighted Jacobian matrices
\begin{align}
  \label{eq:Jacobian}
  \vec U^{(i)}&=\vec W^{1/2}\left.\frac{d\vec\mu}{d\vec u\trans}\right|_{\vec u=\vec
    u^{(i)},\vec\beta=\vec\beta^{(i)}}=\vec W^{1/2}
  \left.\frac{d\vec\mu}{d\vec\eta\trans}\right|_{\vec\eta^{(i)}}
  \left.\frac{d\vec\eta}{d\vec\gamma\trans}\right|_{\vec\gamma^{(i)}}
  \vec Z\Lambda(\vec\theta)\\
  \vec V^{(i)}&=\vec W^{1/2}\left.\frac{d\vec\mu}{d\vec\beta\trans}\right|_{\vec u=\vec
    u^{(i)},\vec\beta=\vec\beta^{(i)}}=\vec W^{1/2}
  \left.\frac{d\vec\mu}{d\vec\eta\trans}\right|_{\vec\eta^{(i)}}
  \left.\frac{d\vec\eta}{d\vec\gamma\trans}\right|_{\vec\gamma^{(i)}}
  \vec X
\end{align}
of dimension $n\times q$ and $n\times p$, respectively.
The increments at the $i$th iteration, $\vec\delta_{\vec u}^{(i)}$ and
$\vec\delta_{\vec\beta}^{(i)}$, are the solutions to
\begin{equation}
  \label{eq:PNLSinc}
  \begin{bmatrix}
    {\vec U^{(i)}}\trans\vec U^{(i)}+\vec I_q&{\vec U^{(i)}}\trans\vec V^{(i)}\\
    {\vec V^{(i)}}\trans\vec U^{(i)}&{\vec V^{(i)}}\trans\vec V^{(i)}
  \end{bmatrix}
  \begin{bmatrix}
    \vec\delta_{\vec u}^{(i)}\\
    \vec\delta_{\vec\beta}^{(i)}
  \end{bmatrix} =
  \begin{bmatrix}
    {\vec U^{(i)}}\trans\vec W^{1/2}(\vec y-\vec\mu^{(i)})-\vec u^{(i)}\\
    {\vec U^{(i)}}\trans\vec W^{1/2}(\vec y-\vec\mu^{(i)})
  \end{bmatrix}
\end{equation}
providing the updated parameter values 
\begin{equation}
  \label{eq:33}
  \begin{bmatrix}\vec u^{(i+1)}\\\vec\beta^{(i+1)}\end{bmatrix}=
  \begin{bmatrix}\vec u^{(i)}\\\vec\beta^{(i)}\end{bmatrix}+\lambda
  \begin{bmatrix}\vec\delta_{\vec u}^{(i)}\\\vec\delta_{\vec\beta}^{(i)}
  \end{bmatrix}
\end{equation}
where $\lambda>0$ is a step factor chosen to ensure that
\begin{equation}
  \label{eq:34}
  (\vec y-\vec\mu^{(i+1)})\trans\vec W(\vec
  y-\vec\mu^{(i+1)})+\|\vec u^{(i+1)}\|^2  <
  (\vec y-\vec\mu^{(i)})\trans\vec W(\vec
  y-\vec\mu^{(i)})+\|\vec u^{(i)}\|^2  .
\end{equation}

In the process of solving for the increments we form the sparse, lower
triangular, Cholesky factor, $\vec L^{(i)}$, satisfying
\begin{equation}
  \label{eq:35}
  \vec L^{(i)} {\vec L^{(i)}}\trans =
  \vec P_{\vec Z}\left({\vec U^{(i)}}\trans\vec U^{(i)}+
    \vec I_n\right)\vec P_{\vec Z}\trans .
\end{equation}
After each successful iteration, determining new values of the
coefficients, $\vec u^{(i+1)}$ and $\vec\beta^{(i+1)}$, that reduce the
penalized, weighted residual sum of squqres, we update the weights
matrix to $\vec W(\vec\mu^{(i+1)})$ and the weighted Jacobians,
$\vec U^{(i+1)}$ and $\vec V^{(i+1)}$, then iterate.  Convergence is
determined according to the orthogonality convergence
criterion~\citep[ch.~2]{bateswatts88:_nonlin}, suitably adjusted for
the weights matrix and the penalty.

\subsection{Weighted linear mixed models}
\label{sec:weightedLMM}

One of the simplest generalizations of linear mixed models is a
weighted linear mixed model where $s=1$, the link function, $g$, and
the nonlinear model function, $m$, are both the identity, the weights
matrix, $\vec W$, is constant and the conditional distribution family
is Gaussian.  That is, the conditional distribution can be written
\begin{equation}
  \label{eq:weightedLMM}
    (\bc Y|\bc U=\vec u)\sim\mathcal{N}(\vec\gamma(\vec
    u,\vec\theta,\vec\beta),\sigma^2\vec W^{-1})
\end{equation}
with discrepancy function
\begin{equation}
  \label{eq:wtddisc}
  d(\vec u|\vec y,\vec\theta,\vec\beta)=\left\|\vec W^{1/2}(\vec
    y-\vec Z\Lambda(\vec\theta)\vec u-\vec X\vec\theta)\right\|^2+\|\vec u\|^2 .
\end{equation}
The conditional mode, $\tilde{\vec u}(\vec\theta)$, and the conditional
estimate, $\tilde{\vec\beta}(\vec\theta)$, are the solutions to
\begin{equation}
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec W\vec Z\Lambda(\theta)
    +\vec I_q&\Lambda\trans(\vec\theta)\vec Z\trans\vec W\vec X\\
    \vec X\trans\vec W\vec Z\Lambda(\theta) &\vec X\trans\vec W\vec X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\\tilde{\vec\beta}(\vec\theta)
  \end{bmatrix} =
  \begin{bmatrix}
    \Lambda\trans(\vec\theta)\vec Z\trans\vec W\vec y\\
    \vec X\trans\vec W\vec y 
  \end{bmatrix} ,
\end{equation}
which can be solved directly, and the Cholesky factor, $\vec L_{\vec Z}(\vec\theta)$, satisfies
\begin{equation}
  \vec L_{\vec Z}(\vec\theta)\vec L_{\vec Z}(\vec\theta)\trans=\vec P_{\vec
    Z}\left(\Lambda\trans(\vec\theta)\vec Z\trans\vec W\vec
    Z\Lambda(\theta)+\vec I_q\right)\vec P_{\vec Z}\trans .
\end{equation}

The profiled log-likelihood, $\tilde{\ell}(\vec\theta|\vec y)$, is
\begin{equation}
  \label{eq:wtdprofilelik}
  -2\tilde{\ell}(\vec\theta|\vec y)=
  \log\left(\frac{|\vec L_{\vec Z}(\vec\theta)|^2}{|\vec W|}\right)+
  n\left(1+\log\left(\frac{2\pi\tilde{d}(\vec
        y,\vec\theta)}{n}\right)\right) .
\end{equation}

If the matrix $\vec W$ is fixed then we can ignore the term $|\vec W|$
in (\ref{eq:wtdprofilelik}) when determining the MLE,
$\widehat{\vec\theta}_L$.  However, in some models, we use a
parameterized weight matrix, $\vec W(\vec\phi)$, and wish to determine
the MLEs, $\widehat{\vec\phi}_L$ and $\widehat{\vec\theta}_L$
simultaneously.  In these cases we must include the term involving
$|\vec W(\vec\phi)|$ when evaluating the profiled log-likelihood.

Note that we must define the parameterization of $\vec W(\vec\phi)$ 
such that $\sigma^2$ and $\vec\phi$ are not a redundant
parameterization of $\sigma^2\vec W(\vec\phi)$.  For example, we could
require that the first diagonal element of $\vec W$ be unity.

\subsection{Nonlinear mixed models}
\label{sec:NLMMs}

In an unweighted, nonlinear mixed model the conditional distribution is
Gaussian, the link, $g$, is the identity and the weights matrix, $\vec
W=\vec I_n$.  That is,
\begin{equation}
  \label{eq:conddistNLMM}
  (\bc Y|\bc U=\vec u)\sim\mathcal{N}(\vec m(\vec\gamma),\sigma^2\vec I_n) 
\end{equation}
with discrepancy function
\begin{equation}
  \label{eq:discNLMM}
  d(\vec u|\vec y,\vec\theta,\vec\beta)=
  \|\vec y-\vec\mu\|^2 + \|\vec u\|^2 .
\end{equation}
For a given value of $\vec\theta$ we determine the conditional modes,
$\tilde{\vec u}(\vec\theta)$ and $\tilde{\vec\beta}(\vec\theta)$, as the
solution to the penalized nonlinear least squares problem
\begin{equation}
  \label{eq:NLMMpnls}
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\
    \tilde{\vec\beta}(\vec\theta)
  \end{bmatrix} = \arg\min_{\vec u,\vec\theta}d(\vec u|\vec y,\vec\theta,\vec\beta)
\end{equation}
and we write the minimum discrepancy, given $\vec y$ and $\vec\theta$,
as 
\begin{equation}
  \label{eq:25}
  \tilde{d}(\vec y,\vec\theta)=d(\tilde{\vec u}(\vec\theta)|\vec
  y,\vec\theta,\tilde{\vec\beta}(\vec\theta)).
\end{equation}

Let $\tilde{\vec L}_Z(\vec\theta)$ and $\tilde{\vec L}_X(\vec\theta)$ be
the Cholesky factors at $\vec\theta$, $\tilde{\vec\beta}(\vec\theta)$ and
$\tilde{\vec u}(\vec\theta)$.  Then the \emph{Laplace approximation} to
the log-likelihood is
\begin{equation}
  \label{eq:36}
  -2\ell_P(\vec\theta,\vec\beta,\sigma|\vec y)\approx
  n\log(2\pi\sigma^2)+\log(|\tilde{\vec L}_{\vec Z}|^2)+
  \frac{\tilde{d}(\vec y,\vec\theta) +
    \left\|\tilde{\vec L}_{\vec X}\trans(\vec\beta-\tilde{\vec\beta})\right\|^2}{\sigma^2},
\end{equation}
producing the approximate profiled log-likelihood,
$\tilde{\ell}_P(\vec\theta|\vec y)$,
\begin{equation}
  \label{eq:37}
  -2\tilde{\ell}_P(\vec\theta|\vec y)\approx
  \log(|\tilde{\vec L}_{\vec Z}|^2)+n\left(1+\log(2\pi
    \tilde{d}(\vec y,\vec\theta)/n) \right).
\end{equation}

\subsubsection{Nonlinear mixed model summary}
\label{sec:nonl-mixed-model}

In a nonlinear mixed model we determine the parameter estimate,
$\widehat{\vec\theta}_P$, from the Laplace approximation to the
log-likelihood as
\begin{equation}
  \label{eq:38}
  \widehat{\vec\theta}_P =
  \arg\max_{\vec\theta}\tilde{\ell}_P(\vec\theta|\vec y)
  =\arg\min_{\vec\theta}
  \log(|\tilde{\vec L}_{\vec Z}|^2)+
  n\left(1+\log(2\pi
    \tilde{d}(\vec y,\vec\theta)/n) \right).
\end{equation}
Each evaluation of $\tilde{\ell}_P(\vec\theta|\vec y)$ requires a
solving the penalized nonlinear least squares problem
(\ref{eq:NLMMpnls}) simultaneously with respect to both sets of
coefficients, $\vec u$ and $\vec\beta$, in the linear predictor,
$\vec\gamma$.

For a weighted nonlinear mixed model with fixed weights, $\vec W$, we
replace the unweighted discrepancy function $d(\vec u|\vec
y,\vec\theta,\vec\beta)$ with the weighted discrepancy function,
%% Finish this off

\section{Details of the implementation}
\label{sec:details}

\subsection{Implementation details for linear mixed models}
\label{sec:impl-line-mixed}

The crucial step in implementing algorithms for determining ML or REML
estimates of the parameters in a linear mixed model is evaluating the
factorization (\ref{eq:16}) for any $\vec\theta$ satisfying
$\vec\theta_L\le\vec\theta\le\vec\theta_U$.  We will assume that $\vec Z$
is sparse as is $\vec Z\Lambda(\vec\theta)$.  

When $\vec X$ is not sparse we will use the factorization (\ref{eq:16})
setting $\vec P_{\vec X}=\vec I_p$ and storing $\vec L_{\vec X\vec Z}$ and
$\vec L_{\vec X}$ as dense matrices.  The permutation matrix $\vec P_{\vec
  Z}$ is determined from the pattern of non-zeros in $\vec
Z\Lambda(\vec\theta)$ which is does not depend on $\vec\theta$, as
long as $\vec\theta$ is not on the boundary.  In fact, in most cases
the pattern of non-zeros in $\vec Z\Lambda(\vec\theta)$ is the same
as the pattern of non-zeros in $\vec Z$. For many models, in particular
models with scalar random effects (described later), the matrix
$\Lambda(\vec\theta)$ is diagonal.

Given a value of $\vec\theta$ we determine the Cholesky factor $\vec
L_{\vec Z}$ satisfying
\begin{equation}
  \label{eq:LZ}
  \vec L_{\vec Z}\vec L_{\vec Z}\trans=\vec P_{\vec Z}(
  \Lambda\trans(\vec\theta)\vec Z\trans\vec Z\Lambda(\theta)
    +\vec I_q)\vec P_{\vec Z}\trans .
\end{equation}
The CHOLMOD package allows for $\vec L_{\vec Z}$ to be calculated
directly from $\Lambda\trans(\vec\theta)\vec Z\trans$ or from
$\Lambda\trans(\vec\theta)\vec Z\trans\vec Z\Lambda(\theta)$.  The
choice in implementation is whether to store $\vec Z\trans$ and update
it to $\Lambda\trans(\vec\theta)\vec Z$ or to store $\vec Z\trans\vec
Z$ and use it to form $\Lambda\trans(\vec\theta)\vec Z\trans\vec
Z\Lambda(\theta)$ at each evaluation.  

In the \package{lme4} package we store $\vec Z\trans$ and use it to
form $\Lambda\trans(\vec\theta)\vec Z\trans$ from which $\vec L_{\vec
  Z}$ is evaluated.  There are two reasons for this choice.  First,
the calculations for the more general forms of mixed models cannot be
reduced to calculations involving $\vec Z\trans\vec Z$ and by expressing
these calculations in terms of $\Lambda(\vec\theta)\vec Z\trans$ for
linear mixed models we can reuse the code for the more general
models.  Second, the calculation of $\Lambda(\vec\theta)\trans\left(\vec
  Z\trans\vec Z\right)\Lambda(\vec\theta)$ from $\vec Z\trans\vec Z$ is
complicated compared to the calculation of
$\Lambda(\vec\theta)\trans\vec Z\trans$ from $\vec Z\trans$.

This choice is disadvantageous when $n\gg q$ because $\vec Z\trans$ is
much larger than $\vec Z\trans\vec Z$, even when they are stored as
sparse matrices.  Evaluation of $\vec L_{\vec Z}$ directly from $\vec
Z\trans$ requires more storage and more calculation that evaluating
$\vec L_{\vec Z}$ from $\vec Z\trans\vec Z$.

Next we evaluate $\vec L_{\vec X\vec Z}\trans$ as the solution to
\begin{equation}
  \label{eq:LXZ}
  \vec L_{\vec Z}\vec L_{\vec X\vec Z}\trans=\vec P_{\vec
    Z}\Lambda\trans(\vec\theta)\vec Z\trans\vec X .
\end{equation}
Again we have the choice of calculating and storing $\vec Z\trans\vec X$
or storing $\vec X$ and using it to reevaluate $\vec Z\trans\vec X$.  In
the \package{lme4} package we store $\vec X$, because the calculations
for the more general models cannot be expressed in terms of $\vec Z\trans\vec X$.

Finally $\vec L_{\vec X}$ is evaluated as the (dense) solution to
\begin{equation}
  \label{eq:LX}
  \vec L_{\vec X}\vec L_{\vec X}\trans=
  \vec X\trans\vec X-\vec L_{\vec X\vec Z}\vec L_{\vec X\vec Z} .
\end{equation}
from which $\tilde{\vec\beta}$ can be determined as the
solution to dense system
\begin{equation}
  \label{eq:tildebeta}
  \vec L_{\vec X}\vec L_{\vec X}\tilde{\vec\beta}=\vec X\trans\vec y
\end{equation}
and $\tilde{\vec u}$ as the solution to the sparse system
\begin{equation}
  \label{eq:tildeu}
  \vec L_{\vec Z}\vec L_{\vec Z}\tilde{u}=\Lambda\trans\vec Z\trans\vec y
\end{equation}

For many models, in particular models with scalar random effects,
which are described later, the matrix $\Lambda(\vec\theta)$ is
diagonal.  For such a model, if both $\vec Z$ and $\vec X$ are sparse
and we plan to use the REML criterion then we create and store
\begin{equation}
  \label{eq:8}
  \vec A=
  \begin{bmatrix}
    \vec Z\trans\vec Z & \vec Z\trans\vec X\\
    \vec X\trans\vec Z & \vec X\trans\vec X
  \end{bmatrix}\quad\text{and}\quad
  \vec c =\begin{bmatrix}\vec Z\trans\vec y\\\vec X\trans\vec y\end{bmatrix}
\end{equation}
and determine a fill-reducing permutation, $\vec P$, for $\vec A$.
Given a value of $\vec\theta$ we create the factorization
\begin{equation}
  \label{eq:26}
  \vec L(\vec\theta)\vec L(\vec\theta)\trans=\vec P\left(
    \begin{bmatrix}
      \Lambda(\vec\theta) & \vec0\\\vec0&\vec I_p
    \end{bmatrix} \vec A
    \begin{bmatrix}
      \Lambda(\vec\theta) & \vec0\\\vec0&\vec I_p
    \end{bmatrix}+
    \begin{bmatrix}\vec I_q&\vec0\\\vec0&\vec0\end{bmatrix}\right)
  \vec P\trans
\end{equation}
solve for $\tilde{\vec u}(\vec\theta)$ and
$\tilde{\vec\beta}(\vec\theta)$ in
\begin{equation}
  \label{eq:28}
  \vec L\vec L\trans\vec P
  \begin{bmatrix}
    \tilde{\vec u}(\vec\theta)\\\tilde{\vec\beta}(\vec\theta)
  \end{bmatrix}=
  \vec P
  \begin{bmatrix}\Lambda(\vec\theta) & \vec0\\\vec0&\vec I_p
  \end{bmatrix} \vec c
\end{equation}
then evaluate $\tilde{d}(\vec y|\vec\theta)$
and the profiled REML criterion as
\begin{equation}
  \label{eq:27}
  \tilde{d}_R(\vec\theta|\vec y)=\log(|\vec L(\vec\theta)|^2)+
  (n-p)\left(1+\log\left(\frac{2\pi\tilde{d}(\vec y|\vec\theta)}
      {n-p}\right)\right) .
\end{equation}
